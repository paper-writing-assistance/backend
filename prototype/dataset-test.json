[
    {
      "pdf_path": "./pdf/AI_VIT_O/2103.15808v1.pdf",
      "title": "CvT: Introducing Convolutions to Vision Transformers",
      "abstract": "We present in this paper a new architecture, named Con-\nvolutional vision Transformer (CvT), that improves Vision\nTransformer (ViT) in performance and efficiency by intro-\nducing convolutions into ViT to yield the best of both de-\nsigns. This is accomplished through two primary modifica-\ntions: a hierarchy of Transformers containing a new convo-\nlutional token embedding, and a convolutional Transformer\nblock leveraging a convolutional projection. These changes\nintroduce desirable properties of convolutional neural net-\nworks (CNNs) to the ViT architecture (i.e. shift, scale,\nand distortion invariance) while maintaining the merits of\nTransformers (i.e. dynamic attention, global context, and\nbetter generalization). We validate CvT by conducting ex-\ntensive experiments, showing that this approach achieves\nstate-of-the-art performance over other Vision Transform-\ners and ResNets on ImageNet-1k, with fewer parame-\nters and lower FLOPs. In addition, performance gains\nare maintained when pretrained on larger datasets (e.g.\nImageNet-22k) and fine-tuned to downstream tasks. Pre-\ntrained on ImageNet-22k, our CvT-W24 obtains a top-1 ac-\ncuracy of 87.7% on the ImageNet-1k val set. Finally, our\nresults show that the positional encoding, a crucial com-\nponent in existing Vision Transformers, can be safely re-\nmoved in our model, simplifying the design for higher res-\nolution vision tasks. Code will be released at https :",
      "result": {
        "summary": "CvT merges convolutions with Vision Transformers, enhancing efficiency and performance. It introduces a convolutional token embedding and a block in Transformers, achieving superior results on ImageNet-1k and maintaining benefits when scaled.",
        "keywords": ["CvT", "Vision Transformers", "convolutions", "ImageNet-1k", "ImageNet-22k", "architecture", "performance", "efficiency", "positional encoding"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_O/2111.06377v3.pdf",
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "abstract": "This paper shows that masked autoencoders (MAE) are\nscalable self-supervised learners for computer vision. Our\nMAE approach is simple: we mask random patches of the\ninput image and reconstruct the missing pixels. It is based\non two core designs. First, we develop an asymmetric\nencoder-decoder architecture, with an encoder that oper-\nates only on the visible subset of patches (without mask to-\nkens), along with a lightweight decoder that reconstructs\nthe original image from the latent representation and mask\ntokens. Second, we find that masking a high proportion\nof the input image, e.g., 75%, yields a nontrivial and\nmeaningful self-supervisory task. Coupling these two de-\nsigns enables us to train large models efficiently and ef-\nfectively: we accelerate training (by 3x or more) and im-\nprove accuracy. Our scalable approach allows for learning\nhigh-capacity models that generalize well: e.g., a vanilla\nViT-Huge model achieves the best accuracy (87.8%) among\nmethods that use only ImageNet-1K data. Transfer per-\nformance in downstream tasks outperforms supervised pre-\ntraining and shows promising scaling behavior.",
      "result":{  
        "summary": "MAE employs an asymmetric encoder-decoder for scalable vision learning, masking 75% of input to enhance self-supervision. This simplifies training, boosts accuracy, and excels in downstream tasks using only ImageNet-1K data.",
        "keywords": ["masked autoencoders", "MAE", "scalable", "self-supervised learning", "computer vision", "asymmetric encoder-decoder", "ImageNet-1K", "training efficiency", "transfer performance"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_O/2105.15168v3.pdf",
      "title": "MSG- Transformer: Exchanging Local Spatial Information by\nManipulating Messenger Tokens",
      "abstract": "Transformers have offered a new methodology of de-\nsigning neural networks for visual recognition. Compared\nto convolutional networks, Transformers enjoy the ability\nof referring to global features at each stage, yet the at-\ntention module brings higher computational overhead that\nobstructs the application of Transformers to process high-\nresolution visual data. This paper aims to alleviate the\nconflict between efficiency and flexibility, for which we pro-\npose a specialized token for each region that serves as a\nmessenger (MSG). Hence, by manipulating these MSG to-\nkens, one can flexibly exchange visual information across\nregions and the computational complexity is reduced. We\nthen integrate the MSG token into a multi-scale architec-\nture named MSG-Transformer. In standard image classi-\nfication and object detection, MSG-Transformer achieves\ncompetitive performance and the inference on both GPU\nand CPU is accelerated. Code is available at https :\n\\ \\ github · com/hustvl /MSG- Transformer.",
      "result": {
        "summary": "MSG-Transformer introduces specialized 'messenger' tokens to exchange visual information efficiently, reducing computational complexity. It shows competitive performance in image classification and object detection, enhancing GPU and CPU inference speeds.",
        "keywords": ["MSG-Transformer", "Transformers", "messenger tokens", "visual recognition", "computational efficiency", "image classification", "object detection"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_O/2010.11929v2.pdf",
      "title": "AN IMAGE IS WORTH 16x16 WORDS:\nTRANSFORMERS FOR IMAGE RECOGNITION AT SCALE",
      "abstract": "While the Transformer architecture has become the de-facto standard for natural\nlanguage processing tasks, its applications to computer vision remain limited. In\nvision, attention is either applied in conjunction with convolutional networks, or\nused to replace certain components of convolutional networks while keeping their\noverall structure in place. We show that this reliance on CNNs is not necessary\nand a pure transformer applied directly to sequences of image patches can perform\nvery well on image classification tasks. When pre-trained on large amounts of\ndata and transferred to multiple mid-sized or small image recognition benchmarks\n(ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent\nresults compared to state-of-the-art convolutional networks while requiring sub-\nstantially fewer computational resources to train. 1",
      "result": {
        "summary": "The paper demonstrates that Transformers, when trained on sequences of image patches, can surpass conventional CNNs in image classification tasks across multiple benchmarks, with fewer computational resources.",
        "keywords": ["Transformers", "image recognition", "Vision Transformer", "ViT", "CNN replacement", "image classification", "computational efficiency"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_O/2005.00928v2.pdf",
      "title": "Quantifying Attention Flow in Transformers",
      "abstract": "In the Transformer model, \"self-attention\"\ncombines information from attended embed-\ndings into the representation of the focal em-\nbedding in the next layer. Thus, across lay-\ners of the Transformer, information originating\nfrom different tokens gets increasingly mixed.\nThis makes attention weights unreliable as ex-\nplanations probes. In this paper, we consider\nthe problem of quantifying this flow of infor-\nmation through self-attention. We propose two\nmethods for approximating the attention to in-\nput tokens given attention weights, attention\nrollout and attentionflow, as post hoc methods\nwhen we use attention weights as the relative\nrelevance of the input tokens. We show that\nthese methods give complementary views on\nthe flow of information, and compared to raw\nattention, both yield higher correlations with\nimportance scores of input tokens obtained us-\ning an ablation method and input gradients.",
      "result": {
        "summary": "The paper addresses quantifying information flow in Transformers via 'self-attention', proposing two methods, attention rollout and attentionflow, to enhance the reliability of attention weights for explaining token relevance.",
        "keywords": ["Transformers", "self-attention", "attention flow", "quantifying information", "attention rollout", "attentionflow", "explanations probes"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_O/2106.02277v1.pdf",
      "title": "Glance-and-Gaze Vision Transformer",
      "abstract": "Recently, there emerges a series of vision Transformers, which show superior\nperformance with a more compact model size than conventional convolutional\nneural networks, thanks to the strong ability of Transformers to model long-range\ndependencies. However, the advantages of vision Transformers also come with a\nprice: Self-attention, the core part of Transformer, has a quadratic complexity to\nthe input sequence length. This leads to a dramatic increase of computation and\nmemory cost with the increase of sequence length, thus introducing difficulties\nwhen applying Transformers to the vision tasks that require dense predictions based\non high-resolution feature maps.",
      "result": {
        "summary": "The Glance-and-Gaze Vision Transformer addresses the high computational cost of traditional vision Transformers due to the quadratic complexity of self-attention, especially in tasks requiring high-resolution feature maps.",
        "keywords": ["Glance-and-Gaze Vision Transformer", "vision Transformers", "self-attention", "computational cost", "high-resolution feature maps", "long-range dependencies"]
      }
    }, {
      "pdf_path": "./pdf/AI_VIT_O/2102.12122v2.pdf",
      "title": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction\nwithout Convolutions",
      "abstract": "Although convolutional neural networks (CNNs) have\nachieved great success in computer vision, this work inves-\ntigates a simpler, convolution-free backbone network use-\nful for many dense prediction tasks. Unlike the recently-\nproposed Vision Transformer (ViT) that was designed for\nimage classification specifically, we introduce the Pyra-\nmid Vision Transformer (PVT), which overcomes the diffi-\nculties of porting Transformer to various dense prediction\ntasks. PVT has several merits compared to current state\nof the arts. (1) Different from ViT that typically yields low-\nresolution outputs and incurs high computational and mem-\nory costs, PVT not only can be trained on dense partitions\nof an image to achieve high output resolution, which is im-\nportant for dense prediction, but also uses a progressive\nshrinking pyramid to reduce the computations of large fea-\nture maps. (2) PVT inherits the advantages of both CNN\nand Transformer, making it a unified backbone for vari-",
      "result": {
        "summary": "Pyramid Vision Transformer (PVT) offers a convolution-free, efficient backbone for dense prediction tasks, addressing limitations of Vision Transformer with high-resolution outputs and reduced computational demands.",
        "keywords": ["Pyramid Vision Transformer", "PVT", "dense prediction", "convolution-free backbone", "high-resolution outputs", "Vision Transformer", "computational efficiency"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_O/1512.03385v1.pdf",
      "title": "Deep Residual Learning for Image Recognition",
      "abstract": "Deeper neural networks are more difficult to train. We\npresent a residual learning framework to ease the training\nof networks that are substantially deeper than those used\npreviously. We explicitly reformulate the layers as learn-\ning residual functions with reference to the layer inputs, in-\nstead of learning unreferenced functions. We provide com-\nprehensive empirical evidence showing that these residual\nnetworks are easier to optimize, and can gain accuracy from\nconsiderably increased depth. On the ImageNet dataset we\nevaluate residual nets with a depth of up to 152 layers-8x\ndeeper than VGG nets [41] but still having lower complex-\nity. An ensemble of these residual nets achieves 3.57% error\non the ImageNet test set. This result won the 1st place on the\nILSVRC 2015 classification task. We also present analysis\non CIFAR-10 with 100 and 1000 layers.",
      "result": {
        "summary": "The paper introduces a residual learning framework for easier training of very deep neural networks, showing that these networks optimize better and achieve higher accuracy, demonstrated on ImageNet with up to 152 layers.",
        "keywords": ["Deep Residual Learning", "image recognition", "neural networks", "residual networks", "ImageNet", "deep learning", "ILSVRC 2015"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_O/2103.14030v2.pdf",
      "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows",
      "abstract": "This paper presents a new vision Transformer, called\nSwin Transformer, that capably serves as a general-purpose\nbackbone for computer vision. Challenges in adapting\nTransformer, from language to vision arise from differences\nbetween the two domains, such as large variations in the\nscale of visual entities and the high resolution of pixels\nin images compared to words in text. To address these\ndifferences, we propose a hierarchical Transformer whose\nrepresentation is computed with Shifted windows. The\nshifted windowing scheme brings greater efficiency by lim-\niting self-attention computation to non-overlapping local\nwindows while also allowing for cross-window connection.\nThis hierarchical architecture has the flexibility to model\nat various scales and has linear computational complexity\nwith respect to image size. These qualities of Swin Trans-\nformer make it compatible with a broad range of vision\ntasks, including image classification (87.3 top-1 accuracy\non ImageNet-1K) and dense prediction tasks such as object\ndetection (58.7 box AP and 51.1 mask AP on COCO test-\ndev) and semantic segmentation (53.5 mIoU on ADE20K\nval). Its performance surpasses the previous state-of-the-\nart by a large margin of +2.7 box AP and +2.6 mask AP on\nCOCO, and +3.2 mIoU on ADE20K, demonstrating the po-\ntential of Transformer-based models as vision backbones.\nThe hierarchical design and the shifted window approach\nalso prove beneficial for all-MLP architectures. The code\nand models are publicly available at https : / /github.\ncom/mi crosoft / Swin-Transformer.",
      "result": {
        "summary": "Swin Transformer introduces a hierarchical, shifted window approach for efficient vision Transformers, enhancing performance in image classification, object detection, and segmentation, proving superior to previous models.",
        "keywords": ["Swin Transformer", "vision Transformer", "hierarchical", "shifted windows", "image classification", "object detection", "semantic segmentation", "computational efficiency", "ImageNet-1K", "COCO", "ADE20K"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_O/2111.07624v1.pdf",
      "title": "Attention Mechanisms in Computer Vision:\nA Survey",
      "abstract": "Abstract-Humans can naturally and effectively find salient regions in complex scenes. Motivated by this observation, attention\nmechanisms were introduced into computer vision with the aim of imitating this aspect of the human visual system. Such an attention\nmechanism can be regarded as a dynamic weight adjustment process based on features of the input image. Attention mechanisms have\nachieved great success in many visual tasks, including image classification, object detection, semantic segmentation, video\nunderstanding, image generation, 3D vision, multi-modal tasks and self-supervised learning. In this survey, we provide a comprehensive\nreview of various attention mechanisms in computer vision and categorize them according to approach, such as channel attention, spatial\nattention, temporal attention and branch attention; a related repository https://hub.com/MengfasoGuokAreernermew-Aters is\ndedicated to collecting related work. We also suggest future directions for attention mechanism research.",
      "result": {
        "summary": "This survey paper explores attention mechanisms in computer vision, categorizing them by type and highlighting their impact on various visual tasks. It suggests future research directions and provides a repository for related works.",
        "keywords": ["attention mechanisms", "computer vision", "image classification", "object detection", "semantic segmentation", "video understanding", "3D vision", "multi-modal tasks", "self-supervised learning"]
      }       
    }, {
      "pdf_path": "./pdf/AI_VIT_O/2104.12533v5.pdf",
      "title": "Visformer: The Vision-friendly Transformer",
      "abstract": "Abstract-The past few years have witnessed the rapid devel-\nopment of applying the Transformer module to vision problems.\nWhile some researchers have demonstrated that Transformer-\nbased models enjoy a favorable ability of fitting data, there are\nstill growing number of evidences showing that these models\nsuffer over-fitting especially when the training data is limited.\nThis paper offers an empirical study by performing step-by-\nstep operations to gradually transit a Transformer-based model\nto a convolution-based model. The results we obtain during\nthe transition process deliver useful messages for improving\nvisual recognition. Based on these observations, we propose a\nnew architecture named Visformer, which is abbreviated from\nthe 'Vision-friendly Transformer'. With the same computational\ncomplexity, Visformer outperforms both the Transformer-based\nand convolution-based models in terms of ImageNet classification\nand object detection performance, and the advantage becomes\nmore significant when the model complexity is lower or the\ntraining set is smaller. The code is available at https://github.\ncom/danczs/Visformer.",
      "result": {
        "summary": "Visformer merges Transformer and convolutional approaches, tackling over-fitting in vision tasks with limited data, and surpassing traditional models in ImageNet classification and object detection.",
        "keywords": ["Visformer", "Vision-friendly Transformer", "Transformer", "convolutional models", "over-fitting", "ImageNet classification", "object detection"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_O/2204.02557v2.pdf",
      "title": "MixFormer: Mixing Features across Windows and Dimensions",
      "abstract": "While local-window self-attention performs notably in\nvision tasks, it suffers from limited receptive field and weak\nmodeling capability issues. This is mainly because it per-\nforms self-attention within non-overlapped windows and\nshares weights on the channel dimension. We propose Mix-\nFormer to find a solution. First, we combine local-window\nself-attention with depth-wise convolution in a parallel de-\nsign, modeling cross-window connections to enlarge the re-\nceptive fields. Second, we propose bi-directional interac-\ntions across branches to provide complementary clues in\nthe channel and spatial dimensions. These two designs are\nintegrated to achieve efficient feature mixing among win-\ndows and dimensions. Our MixFormer provides compet-\nitive results on image classification with EfficientNet and\nshows better results than RegNet and Swin Transformer.\nPerformance in downstream tasks outperforms its alterna-\ntives by significant margins with less computational costs\nin 5 dense prediction tasks on MS COCO, ADE20k, and\nLVIS. Code is available at https : / / github · com /\nPaddl ePaddl e/Paddl eClas.",
      "result": {
        "summary": "MixFormer enhances local-window self-attention in vision tasks by integrating depth-wise convolution and bi-directional interactions, improving receptive fields and feature mixing, excelling in image classification and dense prediction tasks.",
        "keywords": ["MixFormer", "feature mixing", "local-window self-attention", "depth-wise convolution", "bi-directional interactions", "image classification", "dense prediction tasks", "EfficientNet", "RegNet", "Swin Transformer"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_O/2103.11886v4.pdf",
      "title": "Deep ViT: Towards Deeper Vision Transformer",
      "abstract": "Vision transformers (ViTs) have been successfully ap-\nplied in image classification tasks recently. In this paper,\nwe show that, unlike convolution neural networks (CNNs)\nthat can be improved by stacking more convolutional lay-\ners, the performance of ViTs saturate fast when scaled to\nbe deeper. More specifically, we empirically observe that\nsuch scaling difficulty is caused by the attention collapse\nissue: as the transformer goes deeper, the attention maps\ngradually become similar and even much the same after\ncertain layers. In other words, the feature maps tend to\nbe identical in the top layers of deep ViT models. This\nfact demonstrates that in deeper layers of ViTs, the self-\nattention mechanism fails to learn effective concepts for\nrepresentation learning and hinders the model from get-\nting expected performance gain. Based on above obser-\nvation, we propose a simple yet effective method, named\nRe-attention, to re-generate the attention maps to increase\ntheir diversity at different layers with negligible computa-\ntion and memory cost. The proposed method makes it feasi-\nble to train deeper ViT models with consistent performance\nimprovements via minor modification to existing ViT mod-\nels. Notably, when training a deep ViT model with 32 trans-\nformer blocks, the Top-1 classification accuracy can be im-\nproved by 1.6% on ImageNet. Code is publicly available at\nhttps : //github · com/ zhoudaquan/ dvit_repo.",
      "result": {
        "summary": "Deep ViT addresses the 'attention collapse' in deep Vision Transformers by introducing 'Re-attention' to diversify attention maps, enabling deeper ViT models to achieve performance gains, evidenced by improved ImageNet accuracy.",
        "keywords": ["Deep ViT", "Vision Transformers", "attention collapse", "Re-attention", "deeper layers", "image classification", "performance improvement", "ImageNet"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_O/2304.02643v1.pdf",
      "title": "Segment Anything",
      "abstract": "We introduce the Segment Anything (SA) project: a new\ntask, model, and datasetfor image segmentation. Using our\nefficient model in a data collection loop, we built the largest\nsegmentation dataset to date (by far), with over 1 billion\nmasks on 11M licensed and privacy respecting images. The\nmodel is designed and trained to be promptable, SO it can\ntransfer zero-shot to new image distributions and tasks. We\nevaluate its capabilities on numerous tasks and find that\nits zero-shot performance is impressive - often competitive\nwith or even superior to prior fully supervised results. We\nare releasing the Segment Anything Model (SAM) and cor-\nresponding dataset (SA-1B) of 1B masks and 11M images at\nhttps://segment-anything.com to foster research into foun-\ndation models for computer vision.",
      "result": {
        "summary": "The Segment Anything project introduces a promptable model and the largest dataset for image segmentation, featuring over 1 billion masks. The model excels in zero-shot tasks, matching or surpassing supervised methods.",
        "keywords": ["Segment Anything", "image segmentation", "largest dataset", "zero-shot performance", "promptable model", "foundation models", "computer vision"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_O/2012.12877v2.pdf",
      "title": "Training data-efficient image transformers\n& distillation through attention",
      "abstract": "Recently, neural networks purely based on attention were shown to ad-\ndress image understanding tasks such as image classification. These high-\nperforming vision transformers are pre-trained with hundreds of millions\nof images using a large infrastructure, thereby limiting their adoption.",
      "result": {
        "summary": "The paper explores attention-based vision transformers for image classification, noting their high performance but also the extensive pre-training and infrastructure requirements limiting broader use.",
        "keywords": ["image transformers", "attention mechanisms", "image classification", "data efficiency", "infrastructure requirements"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_O/2010.04159v4.pdf",
      "title": "DEFORMABLE DETR: DEFORMABLE TRANSFORMERS\nFOR END-TO-END OBJECT DETECTION",
      "abstract": "DETR has been recently proposed to eliminate the need for many hand-designed\ncomponents in object detection while demonstrating good performance. However,\nit suffers from slow convergence and limited feature spatial resolution, due to the\nlimitation of Transformer attention modules in processing image feature maps. To\nmitigate these issues, we proposed Deformable DETR, whose attention modules\nonly attend to a small set of key sampling points around a reference. Deformable\nDETR can achieve better performance than DETR (especially on small objects)\nwith 10x less training epochs. Extensive experiments on the COCO benchmark\ndemonstrate the effectiveness of our approach. Code is released at https : / /\ngithub · com/ fundamentalvision /Deformable-DETR.",
      "result": {
        "summary": "Deformable DETR enhances DETR for object detection by focusing attention on key points, improving performance and training efficiency, especially for small objects, validated on the COCO benchmark.",
        "keywords": ["Deformable DETR", "DETR", "object detection", "transformer attention", "COCO benchmark", "efficient training", "feature spatial resolution"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_O/1911.03584v2.pdf",
      "title": "ON THE RELATIONSHIP BETWEEN SELF-ATTENTION\nAND CONVOLUTIONAL LAYERS",
      "abstract": "Recent trends of incorporating attention mechanisms in vision have led re-\nsearchers to reconsider the supremacy of convolutional layers as a primary build-\ning block. Beyond helping CNNs to handle long-range dependencies, Ramachan-\ndran et al. (2019) showed that attention can completely replace convolution and\nachieve state-of-the-art performance on vision tasks. This raises the question: do\nlearned attention layers operate similarly to convolutional layers? This work pro-\nvides evidence that attention layers can perform convolution and, indeed, they of-\nten learn to do so in practice. Specifically, we prove that a multi-head self-attention\nlayer with sufficient number of heads is at least as expressive as any convolutional\nlayer. Our numerical experiments then show that self-attention layers attend to\npixel-grid patterns similarly to CNN layers, corroborating our analysis. Our code\nis publicly available1",
      "result": {
        "summary": "This study reveals that self-attention layers can mimic convolutional layers, proving multi-head self-attention's capacity to match CNNs' expressiveness and performance in vision tasks.",
        "keywords": ["self-attention", "convolutional layers", "vision tasks", "multi-head self-attention", "CNN", "expressiveness"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_O/2005.14165v4.pdf",
      "title": "Language Models are Few-Shot Learners",
      "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training\non a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic\nin architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of\nthousands of examples. By contrast, humans can generally perform a new language task from only\na few examples or from simple instructions - something which current NLP systems still largely\nstruggle to do. Here we show that scaling up language models greatly improves task-agnostic,\nfew-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-\ntuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion\nparameters, 10x more than any previous non-sparse language model, and test its performance in\nthe few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning,\nwith tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3\nachieves strong performance on many NLP datasets, including translation, question-answering, and\ncloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as\nunscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same\ntime, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some\ndatasets where GPT-3 faces methodological issues related to training on large web corpora. Finally,\nwe find that GPT-3 can generate samples of news articles which human evaluators have difficulty\ndistinguishing from articles written by humans. We discuss broader societal impacts of this finding\nand of GPT-3 in general.",
      "result": {
        "summary": "GPT-3, a language model with 175 billion parameters, enhances few-shot learning in NLP, performing competitively without fine-tuning across diverse tasks, though some challenges remain.",
        "keywords": ["GPT-3", "few-shot learning", "language models", "NLP tasks", "no fine-tuning", "task-agnostic"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_O/1805.00932v1.pdf",
      "title": "Exploring the Limits of\nWeakly Supervised Pretraining",
      "abstract": "Abstract. State-of-the-art visual perception models for a wide range\nof tasks rely on supervised pretraining. ImageNet classification is the de\nfacto pretraining task for these models. Yet, ImageNet is now nearly ten\nyears old and is by modern standards \"small\" Even SO, relatively little is\nknown about the behavior of pretraining with datasets that are multiple\norders of magnitude larger. The reasons are obvious: such datasets are\ndifficult to collect and annotate. In this paper, we present a unique study\nof transfer learning with large convolutional networks trained to predict\nhashtags on billions of social media images. Our experiments demon-\nstrate that training for large-scale hashtag prediction leads to excellent\nresults. We show improvements on several image classification and object\ndetection tasks, and report the highest ImageNet-1k single-crop, top-1\naccuracy to date: 85.4% (97.6% top-5). We also perform extensive ex-\nperiments that provide novel empirical data on the relationship between\nlarge-scale pretraining and transfer learning performance.",
      "result": {
        "summary": "This study explores weakly supervised pretraining on large-scale datasets using social media hashtags, showing significant improvements in image classification and object detection tasks, achieving record ImageNet accuracy.",
        "keywords": ["weakly supervised pretraining", "large-scale datasets", "hashtag prediction", "image classification", "object detection", "ImageNet", "transfer learning"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_O/2105.15203v3.pdf",
      "title": "SegFormer: Simple and Efficient Design for Semantic\nSegmentation with Transformers",
      "abstract": "We present SegFormer, a simple, efficient yet powerful semantic segmentation\nframework which unifies Transformers with lightweight multilayer perceptron\n(MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises\na novel hierarchically structured Transformer encoder which outputs multiscale\nfeatures. It does not need positional encoding, thereby avoiding the interpolation of\npositional codes which leads to decreased performance when the testing resolution\ndiffers from training. 2) SegFormer avoids complex decoders. The proposed\nMLP decoder aggregates information from different layers, and thus combining\nboth local attention and global attention to render powerful representations. We\nshow that this simple and lightweight design is the key to efficient segmentation\non Transformers. We scale our approach up to obtain a series of models from\nSegFormer-B0 to SegFormer-B5, reaching significantly better performance and\nefficiency than previous counterparts. For example, SegFormer-B4 achieves 50.3%\nmIoU on ADE20K with 64M parameters, being 5x smaller and 2.2% better than\nthe previous best method. Our best model, SegFormer-B5, achieves 84.0% mloU on\nCityscapes validation set and shows excellent zero-shot robustness on Cityscapes-C.\nCode will be released at: gi thub.com/NVlabs /SegF ormer.",
      "result": {
        "summary": "SegFormer integrates Transformers with MLP decoders for semantic segmentation, offering multi-scale features without positional encoding and a simple yet effective MLP decoder for improved segmentation efficiency and performance.",
        "keywords": ["SegFormer", "semantic segmentation", "Transformers", "MLP decoder", "multi-scale features", "positional encoding", "efficient segmentation"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_O/2107.00652v3.pdf",
      "title": "CSWin Transformer: A General Vision Transformer Backbone with\nCross-Shaped Windows",
      "abstract": "We present CSWin Transformer, an efficient and effec-\ntive Transformer-based backbone for general-purpose vision\ntasks. A challenging issue in Transformer design is that\nglobal self-attention is very expensive to compute whereas\nlocal self-attention often limits the field of interactions of\neach token. To address this issue, we develop the Cross-\nShaped Window self-attention mechanism for computing\nself-attention in the horizontal and vertical stripes in parallel\nthat form a cross-shaped window, with each stripe obtained\nby splitting the inputfeature into stripes of equal width. We\nprovide a mathematical analysis of the effect of the stripe\nwidth and vary the stripe width for different layers of the\nTransformer network which achieves strong modeling capa-\nbility while limiting the computation cost. We also introduce\nLocally-enhanced Positional Encoding (LePE), which han-\ndles the local positional information better than existing\nencoding schemes. LePE naturally supports arbitrary input\nresolutions, and is thus especially effective and friendly for\ndownstream tasks. Incorporated with these designs and a hi-\nerarchical structure, CSWin Transformer demonstrates com-\npetitive performance on common vision tasks. Specifically,\nit achieves 85.4% Top-1 accuracy on ImageNet-1K without\nany extra training data or label, 53.9 box AP and 46.4 mask\nAP on the COCO detection task, and 52.2 mIOU on the\nADE20K semantic segmentation task, surpassing previous\nstate-of-the-art Swin Transformer backbone by +1.2, +2.0,\n+1.4, and +2.0 respectively under the similar FLOPs setting.\nBy further pretraining on the larger dataset ImageNet-21K,\nwe achieve 87.5% Top-1 accuracy on ImageNet-1K and high\nsegmentation performance on ADE20K with 55.7 mIoU.",
      "result": {
        "summary": "CSWin Transformer introduces a Cross-Shaped Window self-attention and Locally-enhanced Positional Encoding, enhancing vision tasks efficiency and accuracy, surpassing previous models like Swin Transformer.",
        "keywords": ["CSWin Transformer", "Cross-Shaped Window", "self-attention", "Locally-enhanced Positional Encoding", "vision tasks", "ImageNet-1K", "COCO", "ADE20K"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_O/1904.09925v5.pdf",
      "title": "Attention Augmented Convolutional Networks",
      "abstract": "Convolutional networks have been the paradigm of\nchoice in many computer vision applications. The convolu-\ntion operation however has a significant weakness in that it\nonly operates on a local neighborhood, thus missing global\ninformation. Self-attention, on the other hand, has emerged\nas a recent advance to capture long range interactions, but\nhas mostly been applied to sequence modeling and gener-\native modeling tasks. In this paper, we consider the use of\nself-attentionfor discriminative visual tasks as an alterna-\ntive to convolutions. We introduce a novel two-dimensional\nrelative self-attention mechanism that proves competitive\nin replacing convolutions as a stand-alone computational\nprimitive for image classification. We find in control exper-\niments that the best results are obtained when combining\nboth convolutions and self-attention. We therefore propose\nto augment convolutional operators with this self-attention\nmechanism by concatenating convolutional feature maps\nwith a set offeature maps produced via self-attention. Ex-\ntensive experiments show that Attention Augmentation leads\nto consistent improvements in image classification on Im-\nageNet and object detection on COCO across many dif-\nferent models and scales, including ResNets and a state-\nof-the art mobile constrained network, while keeping the\nnumber of parameters similar. In particular, our method\nachieves a 1.3% top-1 accuracy improvement on ImageNet\nclassification over a ResNet50 baseline and outperforms\nother attention mechanisms for images such as Squeeze-\nand-Excitation [17]. It also achieves an improvement of\n1.4 mAP in COCO Object Detection on top of a RetinaNet\nbaseline.",
      "result":  {
        "summary": "This paper introduces an Attention Augmented Convolutional Network that blends self-attention with convolutions, enhancing global information processing in vision tasks, leading to performance gains in ImageNet classification and COCO detection.",
        "keywords": ["Attention Augmented Convolutional Networks", "self-attention", "convolutions", "image classification", "object detection", "ImageNet", "COCO", "global information"]
      }
    }, {
      "pdf_path": "./pdf/AI_VIT_O/2101.11986v3.pdf",
      "title": "Tokens-to- Token ViT: Training Vision Transformers from Scratch on ImageNet",
      "abstract": "Transformers, which are popular for language modeling,\nhave been explored for solving vision tasks recently, e.g.,\nthe Vision Transformer (ViT) for image classification. The\nViT model splits each image into a sequence of tokens with\nfixed length and then applies multiple Transformer layers\nto model their global relation for classification. However,\nViT achieves inferior performance to CNNs when trained\nfrom scratch on a midsize dataset like ImageNet. We find\nit is because: 1) the simple tokenization of input images\nfails to model the important local structure such as edges\nand lines among neighboring pixels, leading to low train-\ning sample efficiency; 2) the redundant attention backbone\ndesign of ViT leads to limited feature richness for fixed com-\nputation budgets and limited training samples. To overcome\nsuch limitations, we propose a new Tokens-To-Token Vi-\nsion Transformer (T2T- ViT), which incorporates 1) a layer-\nwise Tokens-to-Token (T2T) transformation to progressively\nstructurize the image to tokens by recursively aggregating\nneighboring Tokens into one Token (Tokens-to-Token), such\nthat local structure represented by surrounding tokens can\nbe modeled and tokens length can be reduced; 2) an ef-\nficient backbone with a deep-narrow structure for vision\ntransformer motivated by CNN architecture design after\nempirical study. Notably, T2T-ViT reduces the parameter\ncount and MACs of vanilla ViT by half, while achieving\nmore than 3.0% improvement when trained from scratch on\nImageNet. It also outperforms ResNets and achieves com-\nparable performance with MobileNets by directly training\non ImageNet. For example, T2T-ViT with comparable size\nto ResNet50 (21.5M parameters) can achieve 83.3% topl\naccuracy in image resolution 384x384 on ImageNet. 1",
      "result": {
        "summary": "Tokens-to-Token ViT improves the Vision Transformer by incorporating a Tokens-to-Token transformation to better model local structures and a deep-narrow backbone, enhancing ImageNet training efficiency and performance.",
        "keywords": ["Tokens-to-Token ViT", "Vision Transformer", "T2T-ViT", "image classification", "ImageNet", "local structure modeling", "transformer efficiency"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_O/2012.15840v3.pdf",
      "title": "Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective\nwith Transformers",
      "abstract": "Most recent semantic segmentation methods adopt\na fully-convolutional network (FCN) with an encoder-\ndecoder architecture. The encoder progressively reduces\nthe spatial resolution and learns more abstract/semantic\nvisual concepts with larger receptive fields. Since context\nmodeling is critical for segmentation, the latest efforts have\nbeen focused on increasing the receptive field, through ei-\nther dilated/atrous convolutions or inserting attention mod-\nules. However, the encoder-decoder based FCN architec-\nture remains unchanged. In this paper, we aim to provide\nan alternative perspective by treating semantic segmenta-\ntion as a sequence-to-sequence prediction task. Specifically,\nwe deploy a pure transformer (i.e., without convolution and\nresolution reduction) to encode an image as a sequence of\npatches. With the global context modeled in every layer of\nthe transformer, this encoder can be combined with a simple\ndecoder to provide a powerful segmentation model, termed\nSEgmentation TRansformer (SETR). Extensive experiments\nshow that SETR achieves new state of the art on ADE20K\n(50.28% mIoU), Pascal Context (55.83% mIoU) and com-\npetitive results on Cityscapes. Particularly, we achieve the\nfirst position in the highly competitive ADE20K test server\nleaderboard on the day of submission.",
      "result": {
        "summary": "SETR redefines semantic segmentation as a sequence-to-sequence task using a pure transformer, avoiding convolutions and achieving state-of-the-art results on ADE20K and Pascal Context, and competitive outcomes on Cityscapes.",
        "keywords": ["SETR", "semantic segmentation", "sequence-to-sequence", "transformer", "global context", "ADE20K", "Pascal Context", "Cityscapes"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_O/2104.06399v2.pdf",
      "title": "Co-Scale Conv-Attentional Image Transformers",
      "abstract": "In this paper, we present Co-scale conv-attentional image\nTransformers (CoaT), a Transformer-based image classifier\nequipped with co-scale and conv-attentional mechanisms.\nFirst, the co-scale mechanism maintains the integrity of\nTransformers' encoder branches at individual scales, while\nallowing representations learned at different scales to ef-\nfectively communicate with each other; we design a series\nof serial and parallel blocks to realize the co-scale mecha-\nnism. Second, we devise a conv-attentional mechanism by\nrealizing a relative position embedding formulation in the\nfactorized attention module with an efficient convolution-like\nimplementation. CoaT empowers image Transformers with\nenriched multi-scale and contextual modeling capabilities.\nOn ImageNet, relatively small CoaT models attain superior\nclassification results compared with similar-sized convolu-\ntional neural networks and image/vision Transformers. The\neffectiveness of CoaT's backbone is also illustrated on ob-\nject detection and instance segmentation, demonstrating its\napplicability to downstream computer vision tasks.",
      "result": {
        "summary": "CoaT introduces co-scale and conv-attentional mechanisms to enhance image Transformers, enabling multi-scale interaction and efficient contextual modeling, showing superior results on ImageNet and various vision tasks.",
        "keywords": ["Co-scale conv-attentional image Transformers", "CoaT", "image classifier", "multi-scale interaction", "contextual modeling", "ImageNet", "object detection", "instance segmentation"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_O/2005.12872v3.pdf",
      "title": "End-to-End Object Detection with Transformers",
      "abstract": "Abstract. We present a new method that views object detection as a\ndirect set prediction problem. Our approach streamlines the detection\npipeline, effectively removing the need for many hand-designed compo-\nnents like a non-maximum suppression procedure or anchor generation\nthat explicitly encode our prior knowledge about the task. The main\ningredients of the new framework, called DEtection TRansformer or\nDETR, are a set-based global loss that forces unique predictions via bi-\npartite matching, and a transformer encoder-decoder architecture. Given\na fixed small set of learned object queries, DETR reasons about the re-\nlations of the objects and the global image context to directly output\nthe final set of predictions in parallel. The new model is conceptually\nsimple and does not require a specialized library, unlike many other\nmodern detectors. DETR demonstrates accuracy and run-time perfor-\nmance on par with the well-established and highly-optimized Faster R-\nCNN baseline on the challenging COCO object detection dataset. More-\nover, DETR can be easily generalized to produce panoptic segmentation\nin a unified manner. We show that it significantly outperforms com-\npetitive baselines. Training code and pretrained models are available at\nhttps: / /github. com/facebookresearch/ detr.",
      "result": {
        "summary": "DETR transforms object detection into a direct set prediction problem, eliminating many traditional components like non-maximum suppression, and uses a transformer architecture to output predictions in parallel, showing strong performance on COCO.",
        "keywords": ["DETR", "object detection", "transformers", "set prediction", "COCO dataset", "panoptic segmentation", "encoder-decoder architecture"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_O/1809.10853v3.pdf",
      "title": "ADAPTIVE INPUT REPRESENTATIONS\nFOR NEURAL LANGUAGE MODELING",
      "abstract": "We introduce adaptive input representations for neural language modeling which\nextend the adaptive softmax of Grave et al. (2017) to input representations of vari-\nable capacity. There are several choices on how to factorize the input and output\nlayers, and whether to model words, characters or sub-word units. We perform a\nsystematic comparison of popular choices for a self-attentional architecture. Our\nexperiments show that models equipped with adaptive embeddings are more than\ntwice as fast to train than the popular character input CNN while having a lower\nnumber of parameters. On the WIKITEXT-103 benchmark we achieve 18.7 per-\nplexity, an improvement of 10.5 perplexity compared to the previously best pub-\nlished result and on the BILLION WORD benchmark, we achieve 23.02 perplexity.1",
      "result": {
        "summary": "The paper introduces adaptive input representations for neural language modeling, enhancing training speed and reducing parameters, achieving significant improvements in perplexity on WIKITEXT-103 and BILLION WORD benchmarks.",
        "keywords": ["adaptive input representations", "neural language modeling", "adaptive embeddings", "self-attentional architecture", "WIKITEXT-103", "BILLION WORD", "perplexity improvement"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_O/2103.00112v3.pdf",
      "title": "Transformer in Transformer",
      "abstract": "Transformer is a new kind of neural architecture which encodes the input data as\npowerful features via the attention mechanism. Basically, the visual transformers\nfirst divide the input images into several local patches and then calculate both\nrepresentations and their relationship. Since natural images are of high complexity\nwith abundant detail and color information, the granularity of the patch dividing is\nnot fine enough for excavating features of objects in different scales and locations.\nIn this paper, we point out that the attention inside these local patches are also\nessential for building visual transformers with high performance and we explore\na new architecture, namely, Transformer iN Transformer (TNT). Specifically, we\nregard the local patches (e.g., 16x16) as \"visual sentences\" and present to further\ndivide them into smaller patches (e.g., 4x4) as \"visual words\". The attention of\neach word will be calculated with other words in the given visual sentence with\nnegligible computational costs. Features of both words and sentences will be ag-\ngregated to enhance the representation ability. Experiments on several benchmarks\ndemonstrate the effectiveness of the proposed TNT architecture, e.g., we achieve an\n81.5% top-1 accuracy on the ImageNet, which is about 1.7% higher than that of the\nstate-of-the-art visual transformer with similar computational cost. The PyTorch\ncode is available at https : // github · com/huawei -noah/ CV-Backbones, and\nthe MindSpore code is available at https : / /gitee · com/mindspore/models/\ntree/master /research/ cv /TNT.",
      "result": {
        "summary": "The Transformer in Transformer (TNT) model enhances visual transformers by refining the granularity of patch division, applying attention within local patches, and aggregating features from 'visual words' and 'sentences', achieving superior ImageNet accuracy.",
        "keywords": ["Transformer in Transformer", "TNT", "visual transformers", "local patches", "attention mechanism", "ImageNet", "feature aggregation"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_O/2201.03545v2.pdf",
      "title": "A ConvNet for the 2020s",
      "abstract": "The \"Roaring 20s\" of visual recognition began with the\nintroduction of Vision Transformers (ViTs), which quickly\nsuperseded ConvNets as the state-of-the-art image classifica-\ntion model. A vanilla ViT, on the other hand, faces difficulties\nwhen applied to general computer vision tasks such as object\ndetection and semantic segmentation. It is the hierarchical\nTransformers (e.g., Swin Transformers) that reintroduced sev-\neral ConvNet priors, making Transformers practically viable\nas a generic vision backbone and demonstrating remarkable\nperformance on a wide variety of vision tasks. However,\nthe effectiveness of such hybrid approaches is still largely\ncredited to the intrinsic superiority of Transformers, rather\nthan the inherent inductive biases of convolutions. In this\nwork, we reexamine the design spaces and test the limits of\nwhat a pure ConvNet can achieve. We gradually \"modernize\"\na standard ResNet toward the design of a vision Transformer,\nand discover several key components that contribute to the\nperformance difference along the way. The outcome of this\nexploration is a family of pure ConvNet models dubbed Con-\nvNeXt. Constructed entirely from standard ConvNet modules,\nConvNeXts compete favorably with Transformers in terms of\naccuracy and scalability, achieving 87.8% ImageNet top-1\naccuracy and outperforming Swin Transformers on COCO\ndetection and ADE20K segmentation, while maintaining the\nsimplicity and efficiency of standard ConvNets.",
      "result": {
        "summary": "The study modernizes ConvNets, creating ConvNeXt models that rival Transformers in accuracy and scalability on vision tasks like ImageNet classification and COCO detection, retaining ConvNet efficiency.",
        "keywords": ["ConvNet", "2020s", "Vision Transformers", "ConvNeXt", "ImageNet", "COCO detection", "ADE20K segmentation", "hierarchical Transformers"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_O/2107.10224v4.pdf",
      "title": "CYCLEMLP: A MLP-LIKE ARCHITECTURE FOR\nDENSE PREDICTION",
      "abstract": "This paper presents a simple MLP-like architecture, CycleMLP, which is a versa-\ntile backbone for visual recognition and dense predictions. As compared to mod-\nern MLP architectures, e.g. , MLP-Mixer (Tolstikhin et al., 2021), ResMLP (Tou-\nvron et al., 2021a), and gMLP (Liu et al., 2021a), whose architectures are cor-\nrelated to image size and thus are infeasible in object detection and segmen-\ntation, CycleMLP has two advantages compared to modern approaches. (1) It\ncan cope with various image sizes. (2) It achieves linear computational com-\nplexity to image size by using local windows. In contrast, previous MLPs have\nO(N2) computations due to fully spatial connections. We build a family of\nmodels which surpass existing MLPs and even state-of-the-art Transformer-based\nmodels, e.g. Swin Transformer (Liu et al., 2021b), while using fewer parame-\nters and FLOPs. We expand the MLP-like models' applicability, making them\na versatile backbone for dense prediction tasks. CycleMLP achieves competi-\ntive results on object detection, instance segmentation, and semantic segmenta-\ntion. In particular, CycleMLP-Tiny outperforms Swin-Tiny by 1.3% mloU on\nADE20K dataset with fewer FLOPs. Moreover, CycleMLP also shows excel-\nlent zero-shot robustness on ImageNet-C dataset. Code is available at https :",
      "result": {
        "summary": "CycleMLP offers a versatile, MLP-like architecture that handles different image sizes with linear computational complexity, outperforming existing MLPs and Transformer-based models on dense prediction tasks with fewer parameters and FLOPs.",
        "keywords": ["CycleMLP", "MLP-like architecture", "visual recognition", "dense prediction", "object detection", "instance segmentation", "semantic segmentation", "ADE20K", "ImageNet-C", "computational complexity"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_X/2110.14168v2.pdf",
      "title": "Training Verifiers to Solve Math Word Problems",
      "abstract": "State-of-the-art language models can match human performance on\nmany tasks, but they still struggle to robustly perform multi-step mathe-\nmatical reasoning. To diagnose the failures of current models and support\nresearch, we introduce GSM8K, a dataset of 8.5K high quality linguisti-\ncally diverse grade school math word problems. We find that even the\nlargest transformer models fail to achieve high test performance, despite\nthe conceptual simplicity of this problem distribution. To increase per-\nformance, we propose training verifiers to judge the correctness of model\ncompletions. At test time, we generate many candidate solutions and\nselect the one ranked highest by the verifier. We demonstrate that ver-\nification significantly improves performance on GSM8K, and we provide\nstrong empirical evidence that verification scales more effectively with\nincreased data than a finetuning baseline.",
      "result": {
        "summary": "Introducing the GSM8K dataset for diagnosing multi-step math problem-solving in language models, this paper proposes using verifiers to enhance solution accuracy by ranking model-generated answers, showing improved performance and scalability.",
        "keywords": ["GSM8K", "math word problems", "language models", "transformer models", "verifiers", "solution verification", "model performance"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_X/1708.05031v2.pdf",
      "title": "Neural Collaborative Filtering",
      "abstract": "In recent years, deep neural networks have yielded immense\nsuccess on speech recognition, computer vision and natural\nlanguage processing. However, the exploration of deep neu-\nral networks on recommender systems has received relatively\nless scrutiny. In this work, we strive to develop techniques\nbased on neural networks to tackle the key problem in rec-\nommendation collaborative filtering on the basis of\nimplicit feedback.",
      "result": {
        "summary": "This work explores deep neural networks for recommender systems, particularly focusing on collaborative filtering with implicit feedback, an area previously less examined compared to other applications.",
        "keywords": ["neural collaborative filtering", "deep neural networks", "recommender systems", "implicit feedback", "collaborative filtering"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_X/2402.03620v1.pdf",
      "title": "SELF-DISCOVER: Large Language Models Self-Compose Reasoning Structures",
      "abstract": "We introduce SELF-DISCOVER, a general frame-\nwork for LLMs to self-discover the task-intrinsic\nreasoning structures to tackle complex reasoning\nproblems that are challenging for typical prompt-\ning methods. Core to the framework is a self-\ndiscovery process where LLMs select multiple\natomic reasoning modules such as critical think-\ning and step-by-step thinking, and compose them\ninto an explicit reasoning structure for LLMs to\nfollow during decoding. SELF-DISCOVER sub-\nstantially improves GPT-4 and PaLM 2's per-\nformance on challenging reasoning benchmarks\nsuch as BigBench-Hard, grounded agent reason-\ning, and MATH, by as much as 32% compared\nto Chain of Thought (CoT). Furthermore, SELF-\nDISCOVER outperforms inference-intensive meth-\nods such as CoT-Self-Consistency by more than\n20%, while requiring 10-40x fewer inference com-\npute. Finally, we show that the self-discovered\nreasoning structures are universally applicable\nacross model families: from PaLM 2-L to GPT-4,\nand from GPT-4 to Llama2, and share commonal-\nities with human reasoning patterns.",
      "result": {
        "summary": "SELF-DISCOVER enhances LLMs by enabling them to self-compose reasoning structures, improving performance by up to 32% on complex benchmarks. It outperforms other methods with significantly less compute.",
        "keywords": ["SELF-DISCOVER", "Large Language Models", "reasoning structures", "performance improvement", "complex reasoning"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_X/2109.00122v3.pdf",
      "title": "FINQA: A Dataset of Numerical Reasoning over Financial Data",
      "abstract": "The sheer volume of financial statements\nmakes it difficult for humans to access and an-\nalyze a business's financials. Robust numeri-\ncal reasoning likewise faces unique challenges\nin this domain. In this work, we focus on\nanswering deep questions over financial data,\naiming to automate the analysis of a large cor-\npus of financial documents. In contrast to ex-\nisting tasks on general domain, the finance do-\nmain includes complex numerical reasoning\nand understanding of heterogeneous represen-\ntations. To facilitate analytical progress, we\npropose a new large-scale dataset, FINQA,\nwith Question-Answering pairs over Financial\nreports, written by financial experts. We also\nannotate the gold reasoning programs to en-\nsure full explainability. We further introduce\nbaselines and conduct comprehensive experi-\nments in our dataset. The results demonstrate\nthat popular, large, pre-trained models fall far\nshort of expert humans in acquiring finance\nknowledge and in complex multi-step numer-\nical reasoning on that knowledge. Our dataset\nthe first of its kind - should therefore en-\nable significant, new community research into\ncomplex application domains. The dataset and\ncode are publicly available1.",
      "result": {
        "summary": "FINQA introduces a dataset for numerical reasoning over financial data, featuring expert-curated Q&A pairs and gold reasoning programs. It highlights gaps in current models' performance on financial reasoning.",
        "keywords": ["FINQA", "financial data", "numerical reasoning", "dataset", "question-answering"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_X/1502.05477v5.pdf",
      "title": "Trust Region Policy Optimization",
      "abstract": "We describe an iterative procedure for optimizing\npolicies, with guaranteed monotonic improve-\nment. By making several approximations to the\ntheoretically-justified procedure, we develop a\npractical algorithm, called Trust Region Policy\nOptimization (TRPO). This algorithm is similar\nto natural policy gradient methods and is effec-\ntive for optimizing large nonlinear policies such\nas neural networks. Our experiments demon-\nstrate its robust performance on a wide variety\nof tasks: learning simulated robotic swimming,\nhopping, and walking gaits; and playing Atari\ngames using images of the screen as input. De-\nspite its approximations that deviate from the\ntheory, TRPO tends to give monotonic improve-\nment, with little tuning of hyperparameters.",
      "result": {
        "summary": "TRPO optimizes policies with guaranteed improvement, effective for nonlinear policies like neural networks. It shows robust performance across tasks including robotics and Atari games.",
        "keywords": ["Trust Region Policy Optimization", "TRPO", "policy optimization", "neural networks", "robotics", "Atari games"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_X/2309.05653v3.pdf",
      "title": "Preprint. Work in Progress",
      "abstract": "We introduce MAmmo TH, a series of open-source large language models (LLMs)\nspecifically tailored for general math problem-solving. The MAmmoTH models are\ntrained on MathInstruct, our meticulously curated instruction tuning dataset.\nMathInstruct is compiled from 13 math datasets with intermediate rationales,\nsix of which have rationales newly curated by us. It presents a unique hybrid\nof chain-of-thought (CoT) and program-of-thought (PoT) rationales, and also en-\nsures extensive coverage of diverse fields in math. The hybrid of CoT and PoT not\nonly unleashes the potential of tool use but also allows different thought processes\nfor different math problems. As a result, the MAmmo TH series substantially outper-\nform existing open-source models on nine mathematical reasoning datasets across\nall scales with an average accuracy gain between 16% and 32%. Remarkably,\nour MAmmoTH-7B model reaches 33% on MATH (a competition-level dataset),\nwhich exceeds the best open-source 7B model (WizardMath) by 23%, and the\nMAmmoTH-34B model achieves 44% accuracy on MATH, even surpassing GPT-\n4's CoT result. Our work underscores the importance of diverse problem coverage\nand the use of hybrid rationales in developing superior math generalist models.",
      "result": {
        "summary": "MAmmo TH, a series of LLMs for math problem-solving, trained on MathInstruct, excels with a hybrid of CoT and PoT rationales, achieving significant accuracy gains across various datasets.",
        "keywords": ["MAmmo TH", "math problem-solving", "LLMs", "MathInstruct", "hybrid rationales", "accuracy gains"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_X/0705.2011v1.pdf",
      "title": "Multi-Dimensional Recurrent Neural Networks",
      "abstract": "Recurrent neural networks (RNNs) have proved effective at one dimensional\nsequence learning tasks, such as speech and online handwriting recognition. Some\nof the properties that make RNNs suitable for such tasks, for example robustness\nto input warping, and the ability to access contextual information, are also desir-\nable in multidimensional domains. However, there has SO far been no direct way\nof applying RNNs to data with more than one spatio-temporal dimension. This pa-\nper introduces multi-dimensional recurrent neural networks (MDRNNs), thereby\nextending the potential applicability of RNNs to vision, video processing, medi-\ncal imaging and many other areas, while avoiding the scaling problems that have\nplagued other multi-dimensional models. Experimental results are provided for\ntwo image segmentation tasks.",
      "result": {
        "summary": "MDRNNs extend RNNs to multi-dimensional data, enhancing their application in fields like vision and medical imaging, and showing promising results in image segmentation tasks.",
        "keywords": ["Multi-Dimensional Recurrent Neural Networks", "MDRNNs", "RNNs", "image segmentation", "multi-dimensional data"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_X/2102.09672v1.pdf",
      "title": "Improved Denoising Diffusion Probabilistic Models",
      "abstract": "Denoising diffusion probabilistic models (DDPM)\nare a class of generative models which have re-\ncently been shown to produce excellent sam-\nples. We show that with a few simple modifi-\ncations, DDPMs can also achieve competitive log-\nlikelihoods while maintaining high sample quality.\nAdditionally, we find that learning variances of\nthe reverse diffusion process allows sampling with\nan order of magnitude fewer forward passes with\na negligible difference in sample quality, which\nis important for the practical deployment of these\nmodels. We additionally use precision and re-\ncall to compare how well DDPMs and GANs\ncover the target distribution. Finally, we show\nthat the sample quality and likelihood of these\nmodels scale smoothly with model capacity and\ntraining compute, making them easily scalable.\nWe release our code at https : / / github · com/",
      "result": {
        "summary": "Simple modifications to Denoising Diffusion Probabilistic Models (DDPMs) enhance their performance, allowing fewer forward passes while maintaining sample quality and demonstrating scalability.",
        "keywords": ["Denoising Diffusion Probabilistic Models", "DDPMs", "generative models", "sample quality", "scalability"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_X/1311.2524v5.pdf",
      "title": "Rich feature hierarchies for accurate object detection and semantic segmentation\nTech report (v5)",
      "abstract": "Object detection performance, as measured on the\ncanonical PASCAL VOC dataset, has plateaued in the last\nfew years. The best-performing methods are complex en-\nsemble systems that typically combine multiple low-level\nimage features with high-level context. In this paper, we\npropose a simple and scalable detection algorithm that im-\nproves mean average precision (mAP) by more than 30%\nrelative to the previous best result on VOC 2012-achieving\na mAP of 53.3%. Our approach combines two key insights:\n(1) one can apply high-capacity convolutional neural net-\nworks (CNNs) to bottom-up region proposals in order to\nlocalize and segment objects and (2) when labeled training\ndata is scarce, supervised pre-training for an auxiliary task,\nfollowed by domain-specific fine-tuning, yields a significant\nperformance boost. Since we combine region proposals\nwith CNNs, we call our method R-CNN: Regions with CNN\nfeatures. We also compare R-CNN to OverFeat, a recently\nproposed sliding-window detector based on a similar CNN\narchitecture. We find that R-CNN outperforms OverFeat\nby a large margin on the 200-class ILSVRC2013 detection\ndataset. Source code for the complete system is available at\nhttp : / / www · CS berkeley . edu/ ~rbg/ rcnn.",
      "result": {
        "summary": "R-CNN combines region proposals with CNNs for object detection, significantly boosting mAP on VOC 2012 by over 30%. It outperforms traditional models and offers a method scalable for limited data.",
        "keywords": ["R-CNN", "object detection", "semantic segmentation", "CNN", "PASCAL VOC", "performance improvement"]
      }
      
    }, {
      "pdf_path": "./pdf/AI_VIT_X/1602.01783v2.pdf",
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "abstract": "We propose a conceptually simple and\nlightweight framework for deep reinforce-\nment learning that uses asynchronous gradient\ndescent for optimization of deep neural network\ncontrollers. We present asynchronous variants of\nfour standard reinforcement learning algorithms\nand show that parallel actor-learners have a\nstabilizing effect on training allowing all four\nmethods to successfully train neural network\ncontrollers. The best performing method, an\nasynchronous variant of actor-critic, surpasses\nthe current state-of-the-art on the Atari domain\nwhile training for half the time on a single\nmulti-core CPU instead of a GPU. Furthermore,\nwe show that asynchronous actor-critic succeeds\non a wide variety of continuous motor control\nproblems as well as on a new task of navigating\nrandom 3D mazes using a visual input.",
      "result": {
        "summary": "A new deep RL framework using asynchronous gradient descent stabilizes training and improves efficiency, outperforming current models in Atari and extending to continuous control and 3D navigation.",
        "keywords": ["deep reinforcement learning", "asynchronous gradient descent", "actor-critic", "Atari", "continuous control", "3D navigation"]
      }
    }, {
      "pdf_path": "./pdf/AI_VIT_X/1312.6229v4.pdf",
      "title": "OverFeat:\nIntegrated Recognition, Localization and Detection\nusing Convolutional Networks",
      "abstract": "We present an integrated framework for using Convolutional Networks for classi-\nfication, localization and detection. We show how a multiscale and sliding window\napproach can be efficiently implemented within a ConvNet. We also introduce a\nnovel deep learning approach to localization by learning to predict object bound-\naries. Bounding boxes are then accumulated rather than suppressed in order to\nincrease detection confidence. We show that different tasks can be learned simul-\ntaneously using a single shared network. This integrated framework is the winner\nof the localization task of the ImageNet Large Scale Visual Recognition Challenge\n2013 (ILSVRC2013) and obtained very competitive results for the detection and\nclassifications tasks. In post-competition work, we establish a new state of the art\nfor the detection task. Finally, we release a feature extractor from our best model\ncalled OverFeat.",
      "result": {
        "summary": "OverFeat is a ConvNet framework for classification, localization, and detection, achieving top results in ILSVRC2013. It uses a multiscale, sliding window approach and a unique method for increasing detection confidence.",
        "keywords": ["OverFeat", "Convolutional Networks", "classification", "localization", "detection", "ILSVRC2013"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_X/1606.05908v3.pdf",
      "title": "Tutorial on Variational Autoencoders",
      "abstract": "In just three years, Variational Autoencoders (VAEs) have emerged\nas one of the most popular approaches to unsupervised learning of\ncomplicated distributions. VAEs are appealing because they are built\non top of standard function approximators (neural networks), and\ncan be trained with stochastic gradient descent. VAEs have already\nshown promise in generating many kinds of complicated data, in-\ncluding handwritten digits [1, 2], faces [1, 3, 4], house numbers [5, 6],\nCIFAR images [6], physical models of scenes [4], segmentation [7], and\npredicting the future from static images [8]. This tutorial introduces the\nintuitions behind VAEs, explains the mathematics behind them, and\ndescribes some empirical behavior. No prior knowledge of variational\nBayesian methods is assumed.",
      "result": {
        "summary": "Variational Autoencoders (VAEs) offer a popular method for unsupervised learning of complex distributions using neural networks and stochastic gradient descent, effectively generating diverse data types.",
        "keywords": ["Variational Autoencoders", "VAEs", "unsupervised learning", "neural networks", "data generation"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_X/2302.07577v2.pdf",
      "title": "Efficient Teacher: Semi-Supervised Object Detection for YOLOv5",
      "abstract": "Semi-Supervised Object Detection (SSOD) has been suc-\ncessful in improving the performance of both R-CNN se-\nries and anchor-free detectors. However, one-stage anchor-\nbased detectors lack the structure to generate high-quality\nor flexible pseudo labels, leading to serious inconsistency\nproblems in SSOD. In this paper, we propose the Efficient\nTeacher framework for scalable and effective one-stage\nanchor-based SSOD training, consisting of Dense Detec-\ntor, Pseudo Label Assigner, and Epoch Adaptor. Dense\nDetector is a baseline model that extends RetinaNet with\ndense sampling techniques inspired by YOLOv5. The Ef-\nficient Teacher framework introduces a novel pseudo la-\nbel assignment mechanism, named Pseudo Label Assigner,\nwhich makes more refined use of pseudo labels from Dense\nDetector. Epoch Adaptor is a method that enables a stable\nand efficient end-to-end semi-supervised training schedule\nfor Dense Detector. The Pseudo Label Assigner prevents\nthe occurrence of bias caused by a large number of low-\nquality pseudo labels that may interfere with the Dense De-\ntector during the student-teacher mutual learning mecha-\nnism, and the Epoch Adaptor utilizes domain and distribu-\ntion adaptation to allow Dense Detector to learn globally\ndistributed consistent features, making the training inde-\npendent of the proportion of labeled data. Our experiments\nshow that the Efficient Teacher framework achieves state-\nof-the-art results on VOC, COCO-standard, and COCO-\nadditional using fewer FLOPs than previous methods. To\nthe best of our knowledge, this is the first attempt to apply\nSemi-Supervised Object Detection to YOLOv5.",
      "result": {
        "summary": "Efficient Teacher is a new framework for semi-supervised object detection using YOLOv5, introducing mechanisms for quality pseudo label assignment and stable training, achieving top results on major datasets.",
        "keywords": ["Efficient Teacher", "Semi-Supervised Object Detection", "YOLOv5", "pseudo labels", "anchor-based detectors"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_X/2305.01937v1.pdf",
      "title": "Can Large Language Models Be an Alternative to Human Evaluation?",
      "abstract": "Human evaluation is indispensable and in-\nevitable for assessing the quality of texts gen-\nerated by machine learning models or writ-\nten by humans. However, human evaluation\nis very difficult to reproduce and its quality\nis notoriously unstable, hindering fair compar-\nisons among different natural language pro-\ncessing (NLP) models and algorithms. Re-\ncently, large language models (LLMs) have\ndemonstrated exceptional performance on un-\nseen tasks when only the task instructions are\nprovided. In this paper, we explore if such an\nability of the LLMs can be used as an alter-\nnative to human evaluation. We present the\nLLMs with the exact same instructions, sam-\nples to be evaluated, and questions used to con-\nduct human evaluation, and then ask the LLMs\nto generate responses to those questions; we\ndub this LLM evaluation. We use human evalu-\nation and LLM evaluation to evaluate the texts\nin two NLP tasks: open-ended story genera-\ntion and adversarial attacks. We show that the\nresult of LLM evaluation is consistent with the\nresults obtained by expert human evaluation:\nthe texts rated higher by human experts are\nalso rated higher by the LLMs. We also find\nthat the results of LLM evaluation are stable\nover different formatting of the task instruc-\ntions and the sampling algorithm used to gen-\nerate the answer. We are the first to show the\npotential of using LLMs to assess the quality\nof texts and discuss the limitations and ethical\nconsiderations of LLM evaluation.",
      "result": {
        "summary": "This paper explores using large language models (LLMs) for evaluating text quality as an alternative to human evaluation, finding LLM assessments align with expert human ratings across NLP tasks.",
        "keywords": ["Large Language Models", "human evaluation", "text quality assessment", "NLP tasks", "LLM evaluation"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_X/silver14.pdf",
      "title": "Deterministic Policy Gradient Algorithms",
      "abstract": "In this paper we consider deterministic policy\ngradient algorithms for reinforcement learning\nwith continuous actions. The deterministic pol-\nicy gradient has a particularly appealing form: it\nis the expected gradient of the action-value func-\ntion. This simple form means that the deter-\nministic policy gradient can be estimated much\nmore efficiently than the usual stochastic pol-\nicy gradient. To ensure adequate exploration,\nwe introduce an off-policy actor-critic algorithm\nthat learns a deterministic target policy from an\nexploratory behaviour policy. We demonstrate\nthat deterministic policy gradient algorithms can\nsignificantly outperform their stochastic counter-\nparts in high-dimensional action spaces.",
      "result": {
        "summary": "This paper discusses deterministic policy gradient algorithms in reinforcement learning for continuous actions, showing they can outperform stochastic versions in high-dimensional spaces.",
        "keywords": ["deterministic policy gradient", "reinforcement learning", "continuous actions", "high-dimensional action spaces"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_X/1811.08883v1.pdf",
      "title": "Rethinking ImageNet Pre-training",
      "abstract": "We report competitive results on object detection and in-\nstance segmentation on the COCO dataset using standard\nmodels trained from random initialization. The results\nare no worse than their ImageNet pre-training counterparts\neven when using the hyper-parameters of the baseline sys-\ntem (Mask R-CNN) that were optimized for fine-tuning pre-\ntrained models, with the sole exception of increasing the\nnumber of training iterations SO the randomly initialized\nmodels may converge. Training from random initialization\nis surprisingly robust; our results hold even when: (i) us-\ning only 10% of the training data, (ii) for deeper and wider\nmodels, and (iii) for multiple tasks and metrics. Experi-\nments show that ImageNet pre-training speeds up conver-\ngence early in training, but does not necessarily provide\nregularization or improve final target task accuracy. To\npush the envelope we demonstrate 50.9 AP on COCO ob-\nject detection without using any external data-a result on\npar with the top COCO 2017 competition results that used\nImageNet pre-training. These observations challenge the\nconventional wisdom of ImageNet pre-training for depen-\ndent tasks and we expect these discoveries will encourage\npeople to rethink the current de facto paradigm of 'pre-\ntraining and fine-tuning' in computer vision.",
      "result": {
        "summary": "This study challenges the necessity of ImageNet pre-training for improving object detection and segmentation on COCO, showing comparable results with models trained from scratch.",
        "keywords": ["ImageNet pre-training", "random initialization", "object detection", "instance segmentation", "COCO dataset"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_X/2311.08355v2.pdf",
      "title": "Mustango: Toward Controllable Text-to-Music Generation",
      "abstract": "The quality of the text-to-music models has\nreached new heights due to recent advance-\nments in diffusion models. The controllabil-\nity of various musical aspects, however, has\nbarely been explored. In this paper, we pro-\npose Mustango: a music-domain-knowledge-\ninspired text-to-music system based on dif-\nfusion. Mustango aims to control the gen-\nerated music, not only with general text\ncaptions, but with more rich captions that\ncan include specific instructions related to\nchords, beats, tempo, and key. At the core\nof Mustango is MuNet, a Music-Domain-\nKnowledge-Informed UNet guidance module\nthat steers the generated music to include the\nmusic-specific conditions, which we predict\nfrom the text prompt, as well as the general\ntext embedding, during the reverse diffusion\nprocess. To overcome the limited availability\nof open datasets of music with text captions,\nwe propose a novel data augmentation method\nthat includes altering the harmonic, rhythmic,\nand dynamic aspects of music audio and using\nstate-of-the-art Music Information Retrieval\nmethods to extract the music features which\nwill then be appended to the existing descrip-\ntions in text format. We release the resulting\nMusicBench dataset which contains over 52K\ninstances and includes music-theory -based de-\nscriptions in the caption text. Through exten-\nsive experiments, we show that the quality of\nthe music generated by Mustango is state-of-\nthe-art, and the controllability through music-\nspecific text prompts greatly outperforms other\nmodels such as Musi cGen and AudioLDM2.",
      "result": {
        "summary": "Mustango, a text-to-music system using diffusion models, enhances control over musical aspects via detailed captions for chords, tempo, and more, significantly outperforming others in generating controlled music.",
        "keywords": ["text-to-music generation", "Mustango", "diffusion models", "music controllability", "MuNet"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_X/2301.12661v1.pdf",
      "title": "Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion\nModels",
      "abstract": "Large-scale multimodal generative modeling has\ncreated milestones in text-to-image and text-to-\nvideo generation. Its application to audio still\nlags behind for two main reasons: the lack of\nlarge-scale datasets with high-quality text-audio\npairs, and the complexity of modeling long con-\ntinuous audio data. In this work, we propose\nMake-An-Audio with a prompt-enhanced diffu-\nsion model that addresses these gaps by 1) in-\ntroducing pseudo prompt enhancement with a\ndistill-then-reprogram approach, it alleviates data\nscarcity with orders of magnitude concept compo-\nsitions by using language-free audios; 2) lever-\naging spectrogram autoencoder to predict the\nself-supervised audio representation instead of\nwaveforms. Together with robust contrastive\nlanguage-audio pretraining (CLAP) representa-\ntions, Make-An-Audio achieves state-of-the-art\nresults in both objective and subjective bench-\nmark evaluation. Moreover, we present its control-\nlability and generalization for X-to-Audio with\n\"No Modality Left Behind\", for the first time\nunlocking the ability to generate high-definition,\nhigh-fidelity audios given a user-defined modality\ninput. Audio samples are available at https :\n/ / Text-to-Audio github io",
      "result": {
        "summary": "Make-An-Audio introduces a prompt-enhanced diffusion model for text-to-audio, using pseudo prompts and a spectrogram autoencoder, achieving top results in audio generation with enhanced control.",
        "keywords": ["text-to-audio generation", "Make-An-Audio", "diffusion models", "audio modeling", "spectrogram autoencoder"]
      }
      
    }, {
      "pdf_path": "./pdf/AI_VIT_X/1804.07461v3.pdf",
      "title": "GLUE: A MULTI-TASK BENCHMARK AND ANALYSIS\nPLATFORM FOR NATURAL LANGUAGE UNDERSTAND-\nING",
      "abstract": "For natural language understanding (NLU) technology to be maximally useful, it\nmust be able to process language in a way that is not exclusive to a single task,\ngenre, or dataset. In pursuit of this objective, we introduce the General Language\nUnderstanding Evaluation (GLUE) benchmark, a collection of tools for evaluat-\ning the performance of models across a diverse set of existing NLU tasks. By\nincluding tasks with limited training data, GLUE is designed to favor and encour-\nage models that share general linguistic knowledge across tasks. GLUE also in-\ncludes a hand-crafted diagnostic test suite that enables detailed linguistic analysis\nof models. We evaluate baselines based on current methods for transfer and rep-\nresentation learning and find that multi-task training on all tasks performs better\nthan training a separate model per task. However, the low absolute performance\nof our best model indicates the need for improved general NLU systems.",
      "result": {
        "summary": "GLUE benchmark assesses NLU models across multiple tasks to promote general linguistic knowledge sharing, revealing the need for improved general NLU systems.",
        "keywords": ["GLUE benchmark", "natural language understanding", "NLU tasks", "linguistic knowledge sharing", "model evaluation"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_X/2211.12588v4.pdf",
      "title": "Program of Thoughts Prompting: Disentangling Computa-\ntion from Reasoning for Numerical Reasoning Tasks",
      "abstract": "Recently, there has been significant progress in teaching language models to perform step-by-\nstep reasoning to solve complex numerical reasoning tasks. Chain-of-thoughts prompting\n(CoT) is the state-of-art method for many of these tasks. CoT uses language models to\nproduce text describing reasoning, and computation, and finally the answer to a question.\nHere we propose 'Program of Thoughts' (PoT), which uses language models (mainly Codex)\nto generate text and programming language statements, and finally an answer. In PoT,\nthe computation can be delegated to a program interpreter, which is used to execute the\ngenerated program, thus decoupling complex computation from reasoning and language\nunderstanding. We evaluate PoT on five math word problem datasets and three financial-\nQA datasets in both few-shot and zero-shot settings. We find that PoT has an average\nperformance gain over CoT of around 12% across all datasets. By combining PoT with\nself-consistency decoding, we can achieve extremely strong performance on all the math\ndatasets and financial datasets. All of our data and code will be released.",
      "result": {
        "summary": "Program of Thoughts (PoT) enhances numerical reasoning in language models by separating computation and reasoning, showing a 12% performance gain over CoT across multiple datasets.",
        "keywords": ["Program of Thoughts", "PoT", "numerical reasoning", "Chain-of-thoughts", "language models", "performance gain"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_X/2309.03409v3.pdf",
      "title": "LARGE LANGUAGE MODELS AS OPTIMIZERS",
      "abstract": "Optimization is ubiquitous. While derivative-based algorithms have been powerful\ntools for various problems, the absence of gradient imposes challenges on many\nreal-world applications. In this work, we propose Optimization by PROmpting\n(OPRO), a simple and effective approach to leverage large language models (LLMs)\nas optimizers, where the optimization task is described in natural language. In\neach optimization step, the LLM generates new solutions from the prompt that\ncontains previously generated solutions with their values, then the new solutions are\nevaluated and added to the prompt for the next optimization step. We first showcase\nOPRO on linear regression and traveling salesman problems, then move on to our\nmain application in prompt optimization, where the goal is to find instructions\nthat maximize the task accuracy. With a variety of LLMs, we demonstrate that\nthe best prompts optimized by OPRO outperform human-designed prompts by\nup to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks. Code at",
      "result": {
        "summary": "OPRO leverages LLMs as optimizers for tasks described in natural language, improving solution quality iteratively. It notably enhances prompt optimization, outperforming human-designed prompts.",
        "keywords": ["OPRO", "large language models", "optimization", "prompt optimization", "natural language"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_X/1506.02640v5.pdf",
      "title": "You Only Look Once:\nUnified, Real-Time Object Detection",
      "abstract": "We present YOLO, a new approach to object detection.\nPrior work on object detection repurposes classifiers to per-\nform detection. Instead, we frame object detection as a re-\ngression problem to spatially separated bounding boxes and\nassociated class probabilities. A single neural network pre-\ndicts bounding boxes and class probabilities directly from\nfull images in one evaluation. Since the whole detection\npipeline is a single network, it can be optimized end-to-end\ndirectly on detection performance.",
      "result": {
        "summary": "YOLO introduces a novel approach to object detection by treating it as a regression problem, using a single neural network to predict bounding boxes and class probabilities in real-time.",
        "keywords": ["YOLO", "object detection", "real-time", "neural network", "regression"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_X/1507.01526v3.pdf",
      "title": "GRID LONG SHORT-TERM MEMORY",
      "abstract": "This paper introduces Grid Long Short-Term Memory, a network of LSTM cells\narranged in a multidimensional grid that can be applied to vectors, sequences or\nhigher dimensional data such as images. The network differs from existing deep\nLSTM architectures in that the cells are connected between network layers as\nwell as along the spatiotemporal dimensions of the data. The network provides\na unified way of using LSTM for both deep and sequential computation. We ap-\nply the model to algorithmic tasks such as 15-digit integer addition and sequence\nmemorization, where itis able to significantly outperform the standard LSTM. We\nthen give results for two empirical tasks. We find that 2D Grid LSTM achieves\n1.47 bits per character on the Wikipedia character prediction benchmark, which is\nstate-of-the-art among neural approaches. In addition, we use the Grid LSTM to\ndefine a novel two-dimensional translation model, the Reencoder, and show that it\noutperforms a phrase-based reference system on a Chinese-to-English translation\ntask.",
      "result": {
        "summary": "Grid LSTM introduces a novel LSTM architecture for handling vectors, sequences, and higher-dimensional data, excelling in algorithmic and empirical tasks with state-of-the-art results in character prediction and translation.",
        "keywords": ["Grid LSTM", "multidimensional data", "deep learning", "sequence computation", "translation model"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_X/1509.02971v6.pdf",
      "title": "CONTINUOUS CONTROL WITH DEEP REINFORCEMENT\nLEARNING",
      "abstract": "We adapt the ideas underlying the success of Deep Q-Learning to the continuous\naction domain. We present an actor-critic, model-free algorithm based on the de-\nterministic policy gradient that can operate over continuous action spaces. Using\nthe same learning algorithm, network architecture and hyper-parameters, our al-\ngorithm robustly solves more than 20 simulated physics tasks, including classic\nproblems such as cartpole swing-up, dexterous manipulation, legged locomotion\nand car driving. Our algorithm is able to find policies whose performance is com-\npetitive with those found by a planning algorithm with full access to the dynamics\nof the domain and its derivatives. We further demonstrate that for many of the\ntasks the algorithm can learn policies \"end-to-end\": directly from raw pixel in-\nputs.",
      "result": {
        "summary": "This paper adapts Deep Q-Learning to continuous actions using an actor-critic algorithm, achieving robust solutions for over 20 physics-based tasks, and learning directly from raw pixels.",
        "keywords": ["continuous control", "deep reinforcement learning", "actor-critic", "policy gradient", "physics tasks"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_X/NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf",
      "title": "ImageNet Classification with Deep Convolutional\nNeural Networks",
      "abstract": "We trained a large, deep convolutional neural network to classify the 1.2 million\nhigh-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif-\nferent classes. On the test data, we achieved top-1 and top-5 error rates of 37.5%\nand 17.0% which is considerably better than the previous state-of-the-art. The\nneural network, which has 60 million parameters and 650,000 neurons, consists\nof five convolutional layers, some of which are followed by max-pooling layers,\nand three fully-connected layers with a final 1000-way softmax. To make train-\ning faster, we used non-saturating neurons and a very efficient GPU implemen-\ntation of the convolution operation. To reduce overfitting in the fully-connected\nlayers we employed a recently-developed regularization method called \"dropout\"\nthat proved to be very effective. We also entered a variant of this model in the\nILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%,\ncompared to 26.2% achieved by the second-best entry.",
      "result": {
        "summary": "A deep convolutional neural network with 60 million parameters classifies ImageNet images into 1000 classes, achieving top error rates significantly better than prior methods, aided by dropout and efficient GPU use.",
        "keywords": ["deep convolutional neural network", "ImageNet classification", "dropout", "GPU efficiency", "error rates"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_X/1705.03551v2.pdf",
      "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset\nfor Reading Comprehension",
      "abstract": "We present TriviaQA, a challenging\nreading comprehension dataset contain-\ning over 650K question-answer-evidence\ntriples. TriviaQA includes 95K question-\nanswer pairs authored by trivia enthusi-\nasts and independently gathered evidence\ndocuments, S1X per question on average,\nthat provide high quality distant super-\nvision for answering the questions. We\nshow that, in comparison to other recently\nintroduced large-scale datasets, TriviaQA\n(1) has relatively complex, compositional\nquestions, (2) has considerable syntactic\nand lexical variability between questions\nand corresponding answer-evidence sen-\ntences, and (3) requires more cross sen-\ntence reasoning to find answers. We also\npresent two baseline algorithms: a feature-\nbased classifier and a state-of-the-art neu-\nral network, that performs well on SQuAD\nreading comprehension. Neither approach\ncomes close to human performance (23%\nand 40% VS. 80%), suggesting that Trivi-\naQA is a challenging testbed that is worth\nsignificant future study. 1",
      "result": {
        "summary": "TriviaQA is a large-scale reading comprehension dataset with 650K+ QA pairs requiring complex reasoning. It poses a significant challenge as baseline models fall short compared to human performance.",
        "keywords": ["TriviaQA", "reading comprehension", "dataset", "question-answer pairs", "distant supervision"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_X/2105.05233v4.pdf",
      "title": "Diffusion Models Beat GANs on Image Synthesis",
      "abstract": "We show that diffusion models can achieve image sample quality superior to the\ncurrent state-of-the-art generative models. We achieve this on unconditional im-\nage synthesis by finding a better architecture through a series of ablations. For\nconditional image synthesis, we further improve sample quality with classifier guid-\nance: a simple, compute-efficient method for trading off diversity for fidelity using\ngradients from a classifier. We achieve an FID of 2.97 on ImageNet 128 x 128,\n4.59 on ImageNet 256x256, and 7.72 on ImageNet 512x512, and we match\nBigGAN-deep even with as few as 25 forward passes per sample, all while main-\ntaining better coverage of the distribution. Finally, we find that classifier guidance\ncombines well with upsampling diffusion models, further improving FID to 3.94\non ImageNet 256x256 and 3.85 on ImageNet 512x512. We release our code at\nhttps : //github · com/ openai / guided-diffusion.",
      "result": {
        "summary": "Diffusion models surpass GANs in image synthesis quality through architecture improvements and classifier guidance, achieving top FID scores on ImageNet across various resolutions.",
        "keywords": ["diffusion models", "image synthesis", "GANs", "classifier guidance", "FID scores", "ImageNet"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_X/1707.06347v2.pdf",
      "title": "Proximal Policy Optimization Algorithms",
      "abstract": "We propose a new family of policy gradient methods for reinforcement learning, which al-\nternate between sampling data through interaction with the environment, and optimizing a\n\"surrogate\" objective function using stochastic gradient ascent. Whereas standard policy gra-\ndient methods perform one gradient update per data sample, we propose a novel objective\nfunction that enables multiple epochs of minibatch updates. The new methods, which we call\nproximal policy optimization (PPO), have some of the benefits of trust region policy optimiza-\ntion (TRPO), but they are much simpler to implement, more general, and have better sample\ncomplexity (empirically). Our experiments test PPO on a collection of benchmark tasks, includ-\ning simulated robotic locomotion and Atari game playing, and we show that PPO outperforms\nother online policy gradient methods, and overall strikes a favorable balance between sample\ncomplexity, simplicity, and wall-time.",
      "result": {
        "summary": "Proximal Policy Optimization (PPO) introduces a new family of policy gradient methods that optimize a surrogate objective, allowing multiple updates per sample and achieving better performance on benchmarks.",
        "keywords": ["Proximal Policy Optimization", "PPO", "policy gradient methods", "reinforcement learning", "sample complexity"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_X/1504.08083v2.pdf",
      "title": "Fast R-CNN",
      "abstract": "This paper proposes a Fast Region-based Convolutional\nNetwork method (Fast R-CNN) for object detection. Fast\nR-CNN builds on previous work to efficiently classify ob-\nject proposals using deep convolutional networks. Com-\npared to previous work, Fast R-CNN employs several in-\nnovations to improve training and testing speed while also\nincreasing detection accuracy. Fast R-CNN trains the very\ndeep VGG16 network 9x faster than R-CNN, is 213x faster\nat test-time, and achieves a higher mAP on PASCAL VOC\n2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x\nfaster, tests 10x faster, and is more accurate. Fast R-CNN\nis implemented in Python and C++ (using Caffe) and is\navailable under the open-source MIT License at https ··",
      "result": {
        "summary": "Fast R-CNN improves upon previous object detection methods, offering faster training and testing speeds with higher accuracy, significantly outperforming R-CNN and SPPnet on benchmarks.",
        "keywords": ["Fast R-CNN", "object detection", "deep convolutional networks", "speed improvement", "accuracy enhancement"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_X/1506.01497v3.pdf",
      "title": "Faster R-CNN: Towards Real-Time Object\nDetection with Region Proposal Networks",
      "abstract": "Abstract-State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations.\nAdvances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region\nproposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image\nconvolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional\nnetwork that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to\ngenerate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN\ninto a single network by sharing their convolutional features-using the recently popular terminology of neural networks with\n\"attention\" mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3],\nour detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection\naccuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO\n2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been\nmade publicly available.",
      "result": {
        "summary": "Faster R-CNN integrates a Region Proposal Network (RPN) with Fast R-CNN, significantly speeding up object detection by generating efficient region proposals and sharing convolutional features.",
        "keywords": ["Faster R-CNN", "Region Proposal Network", "object detection", "real-time", "RPN", "Fast R-CNN"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_X/1606.05250v3.pdf",
      "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text",
      "abstract": "We present the Stanford Question Answer-\ning Dataset (SQuAD), a new reading compre-\nhension dataset consisting of 100,000+ ques-\ntions posed by crowdworkers on a set of\nWikipedia articles, where the answer to each\nquestion is a segment of text from the cor-\nresponding reading passage. We analyze the\ndataset to understand the types of reason-\ning required to answer the questions, lean-\ning heavily on dependency and constituency\ntrees. We build a strong logistic regression\nmodel, which achieves an F1 score of 51.0%,\na significant improvement over a simple base-\nline (20%). However, human performance\n(86.8%) is much higher, indicating that the\ndataset presents a good challenge problem for\nfuture research. The dataset is freely available\nat https: / / stanford-qa.com.",
      "result": {
        "summary": "SQuAD is a large-scale reading comprehension dataset from Stanford with 100,000+ questions on Wikipedia articles, presenting a significant challenge for AI with human performance far exceeding current models.",
        "keywords": ["SQuAD", "machine comprehension", "reading comprehension dataset", "Stanford", "Wikipedia"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_X/1511.06581v3.pdf",
      "title": "Dueling Network Architectures for Deep Reinforcement Learning",
      "abstract": "In recent years there have been many successes\nof using deep representations in reinforcement\nlearning. Still, many of these applications use\nconventional architectures, such as convolutional\nnetworks, LSTMs, or auto-encoders. In this pa-\nper, we present a new neural network architec-\nture for model-free reinforcement learning. Our\ndueling network represents two separate estima-\ntors: one for the state value function and one for\nthe state-dependent action advantage function.\nThe main benefit of this factoring is to general-\nize learning across actions without imposing any\nchange to the underlying reinforcement learning\nalgorithm. Our results show that this architec-\nture leads to better policy evaluation in the pres-\nence of many similar-valued actions. Moreover,\nthe dueling architecture enables our RL agent to\noutperform the state-of-the-art on the Atari 2600\ndomain.",
      "result": {
        "summary": "Dueling network architecture introduces two separate estimators for state values and action advantages in reinforcement learning, improving policy evaluation and outperforming current models in Atari 2600 games.",
        "keywords": ["dueling network", "deep reinforcement learning", "architecture", "policy evaluation", "Atari 2600"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_X/1511.06391v4.pdf",
      "title": "ORDER MATTERS: SEQUENCE TO SEQUENCE FOR SETS",
      "abstract": "Sequences have become first class citizens in supervised learning thanks to the\nresurgence of recurrent neural networks. Many complex tasks that require map-\nping from or to a sequence of observations can now be formulated with the\nsequence-to-sequence (seq2seq) framework which employs the chain rule to ef-\nficiently represent the joint probability of sequences. In many cases, however,\nvariable sized inputs and/or outputs might not be naturally expressed as sequences.\nFor instance, it is not clear how to input a set of numbers into a model where the\ntask is to sort them; similarly, we do not know how to organize outputs when\nthey correspond to random variables and the task is to model their unknown joint\nprobability. In this paper, we first show using various examples that the order in\nwhich we organize input and/or output data matters significantly when learning an\nunderlying model. We then discuss an extension of the seq2seq framework that\ngoes beyond sequences and handles input sets in a principled way. In addition,\nwe propose a loss which, by searching over possible orders during training, deals\nwith the lack of structure of output sets. We show empirical evidence of our claims\nregarding ordering, and on the modifications to the seq2seq framework on bench-\nmark language modeling and parsing tasks, as well as two artificial tasks - sorting\nnumbers and estimating the joint probability of unknown graphical models.",
      "result": {
        "summary": "This paper extends the sequence-to-sequence framework to handle sets, addressing challenges with variable sized inputs and outputs not naturally expressed as sequences, and introduces a loss to manage output set structures.",
        "keywords": ["sequence-to-sequence", "sets", "seq2seq framework", "variable sized inputs", "output structure"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_X/1506.04834v3.pdf",
      "title": "Tree-Structured Composition in Neural Networks\nwithout Tree-Structured Architectures",
      "abstract": "Tree-structured neural networks encode a particular tree geometry for a sentence\nin the network design. However, these models have at best only slightly out-\nperformed simpler sequence-based models. We hypothesize that neural sequence\nmodels like LSTMs are in fact able to discover and implicitly use recursive com-\npositional structure, at least for tasks with clear cues to that structure in the data.\nWe demonstrate this possibility using an artificial data task for which recursive\ncompositional structure is crucial, and find an LSTM-based sequence model can\nindeed learn to exploit the underlying tree structure. However, its performance\nconsistently lags behind that of tree models, even on large training sets, suggest-\ning that tree-structured models are more effective at exploiting recursive structure.",
      "result": {
        "summary": "This study explores how neural sequence models like LSTMs can implicitly learn recursive compositional structures, though they still perform less effectively than dedicated tree-structured models.",
        "keywords": ["neural networks", "tree-structured", "LSTMs", "recursive composition", "sequence models"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_X/1502.02367v4.pdf",
      "title": "Gated Feedback Recurrent Neural Networks",
      "abstract": "In this work, we propose a novel recurrent neu-\nral network (RNN) architecture. The proposed\nRNN, gated-feedback RNN (GF-RNN), extends\nthe existing approach of stacking multiple recur-\nrent layers by allowing and controlling signals\nflowing from upper recurrent layers to lower lay-\ners using a global gating unit for each pair of\nlayers. The recurrent signals exchanged between\nlayers are gated adaptively based on the previous\nhidden states and the current input. We evalu-\nated the proposed GF-RNN with different types\nof recurrent units, such as tanh, long short-term\nmemory and gated recurrent units, on the tasks\nof character-level language modeling and Python\nprogram evaluation. Our empirical evaluation of\ndifferent RNN units, revealed that in both tasks,\nthe GF-RNN outperforms the conventional ap-\nproaches to build deep stacked RNNs. We sug-\ngest that the improvement arises because the GF-\nRNN can adaptively assign different layers to dif-\nferent timescales and layer-to-layer interactions\n(including the top-down ones which are not usu-\nally present in a stacked RNN) by learning to gate\nthese interactions.",
      "result": {
        "summary": "Gated-feedback RNN (GF-RNN) introduces a novel architecture that controls signal flow between recurrent layers, improving performance on language modeling and code evaluation by adaptively gating layer interactions.",
        "keywords": ["GF-RNN", "recurrent neural network", "gated feedback", "language modeling", "Python code evaluation"]
      }
    }, {
      "pdf_path": "./pdf/AI_VIT_X/BF00992698.pdf",
      "title": "Q-Learning",
      "abstract": "Abstract. Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian\ndomains. It amounts to an incremental method for dynamic programming which imposes limited computational\ndemands. It works by successively improving its evaluations of the quality of particular actions at particular states. This paper presents and proves in detail a convergence theorem for Q-learning based on that outlined in Watkins\n(1989). We show that Q-learning converges to the optimum action-values with probability 1 so long as all actions\nare repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions\nto the cases of non-discounted, but absorbing, Markov environments, and where many Q values can be changed\neach iteration, rather than just one.",
      "result": {
        "summary": "Q-learning is a method where agents learn optimal actions in Markovian domains. This paper proves Q-learning's convergence to optimal action-values under certain conditions, and discusses its extensions.",
        "keywords": ["Q-learning", "Markovian domains", "optimal action-values", "convergence", "dynamic programming"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_X/1409.4842v1.pdf",
      "title": "Going deeper with convolutions",
      "abstract": "We propose a deep convolutional neural network architecture codenamed Incep-\ntion, which was responsible for setting the new state of the art for classification\nand detection in the ImageNet Large-Scale Visual Recognition Challenge 2014\n(ILSVRC14). The main hallmark of this architecture is the improved utilization\nof the computing resources inside the network. This was achieved by a carefully\ncrafted design that allows for increasing the depth and width of the network while\nkeeping the computational budget constant. To optimize quality, the architectural\ndecisions were based on the Hebbian principle and the intuition of multi-scale\nprocessing. One particular incarnation used in our submission for ILSVRC14 is\ncalled GoogLeNet, a 22 layers deep network, the quality of which is assessed in\nthe context of classification and detection.",
      "result": {
        "summary": "Inception, a deep convolutional neural network, sets a new standard in the ImageNet challenge by optimizing resource use, increasing network depth and width without raising computational costs, based on Hebbian principles and multi-scale processing.",
        "keywords": ["Inception", "GoogLeNet", "deep convolutional neural network", "ImageNet", "network architecture"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_X/1711.09020v3.pdf",
      "title": "StarGAN: Unified Generative Adversarial Networks\nfor Multi-Domain Image-to-Image Translation",
      "abstract": "Recent studies have shown remarkable success in image-\nto-image translation for two domains. However, existing\napproaches have limited scalability and robustness in han-\ndling more than two domains, since different models should\nbe built independently for every pair of image domains. To\naddress this limitation, we propose StarGAN, a novel and\nscalable approach that can perform image-to-image trans-\nlations for multiple domains using only a single model.\nSuch a unified model architecture ofStarGAN allows simul-\ntaneous training of multiple datasets with different domains\nwithin a single network. This leads to StarGAN's superior\nquality of translated images compared to existing models as\nwell as the novel capability offlexibly translating an input\nimage to any desired target domain. We empirically demon-\nstrate the effectiveness of our approach on a facial attribute\ntransfer and a facial expression synthesis tasks.",
      "result": {
        "summary": "StarGAN introduces a scalable unified GAN architecture for multi-domain image-to-image translation, efficiently handling multiple domains with a single model and improving image quality across diverse tasks.",
        "keywords": ["StarGAN", "image-to-image translation", "GAN", "multi-domain", "unified model architecture"]
      }
      
    }, {
      "pdf_path": "./pdf/AI_VIT_X/1503.00075v3.pdf",
      "title": "Improved Semantic Representations From\nTree-Structured Long Short- Term Memory Networks",
      "abstract": "Because of their superior ability to pre-\nserve sequence information over time,\nLong Short-Term Memory (LSTM) net-\nworks, a type of recurrent neural net-\nwork with a more complex computational\nunit, have obtained strong results on a va-\nriety of sequence modeling tasks. The\nonly underlying LSTM structure that has\nbeen explored SO far is a linear chain.\nHowever, natural language exhibits syn-\ntactic properties that would naturally com-\nbine words to phrases. We introduce the\nTree-LSTM, a generalization of LSTMs to\ntree-structured network topologies. Tree-\nLSTMs outperform all existing systems\nand strong LSTM baselines on two tasks:\npredicting the semantic relatedness of two\nsentences (SemEval 2014, Task 1) and\nsentiment classification (Stanford Senti-\nment Treebank).",
      "result": {
        "summary": "Tree-LSTM extends traditional LSTMs to tree-structured networks, better capturing language's syntactic properties, resulting in superior performance on semantic relatedness and sentiment classification tasks.",
        "keywords": ["Tree-LSTM", "semantic representations", "LSTM networks", "syntactic properties", "sequence modeling"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_X/glove.pdf",
      "title": "Glo Ve: Global Vectors for Word Representation",
      "abstract": "Recent methods for learning vector space\nrepresentations of words have succeeded\nin capturing fine-grained semantic and\nsyntactic regularities using vector arith-\nmetic, but the origin of these regularities\nhas remained opaque. We analyze and\nmake explicit the model properties needed\nfor such regularities to emerge in word\nvectors. The result is a new global log-\nbilinear regression model that combines\nthe advantages of the two major model\nfamilies in the literature: global matrix\nfactorization and local context window\nmethods. Our model efficiently leverages\nstatistical information by training only on\nthe nonzero elements in a word-word CO-\noccurrence matrix, rather than on the en-\ntire sparse matrix or on individual context\nwindows in a large corpus. The model pro-\nduces a vector space with meaningful sub-\nstructure, as evidenced by its performance\nof 75% on a recent word analogy task. It\nalso outperforms related models on simi-\nlarity tasks and named entity recognition.",
      "result": {
        "summary": "GloVe introduces a global log-bilinear regression model that merges matrix factorization and context window approaches to produce word vectors, capturing semantic regularities and outperforming on various NLP tasks.",
        "keywords": ["GloVe", "word representation", "vector space", "semantic regularities", "log-bilinear regression"]
      }    
    }, {
      "pdf_path": "./pdf/AI_VIT_X/2302.04166v2.pdf",
      "title": "GPTScore: Evaluate as You Desire",
      "abstract": "Generative Artificial Intelligence (AI) has enabled\nthe development of sophisticated models that are\ncapable of producing high-caliber text, images,\nand other outputs through the utilization of large\npre-trained models. Nevertheless, assessing the\nquality of the generation is an even more ardu-\nous task than the generation itself, and this is-\nsue has not been given adequate consideration\nrecently. This paper proposes a novel evalua-\ntion framework, GPTSCORE, which utilizes the\nemergent abilities (e.g., zero-shot instruction) of\ngenerative pre-trained models to score generated\ntexts. There are 19 pre-trained models explored\nin this paper, ranging in size from 80M (e.g.,\nFLAN-T5-small) to 175B (e.g., GPT3). Exper-\nimental results on four text generation tasks, 22\nevaluation aspects, and corresponding 37 datasets\ndemonstrate that this approach can effectively\nallow us to achieve what one desires to evalu-\nate for texts simply by natural language instruc-\ntions. This nature helps us overcome several\nlong-standing challenges in text evaluation-how\nto achieve customized, multi-faceted evaluation\nwithout the need for annotated samples. We make\nour code publicly available. 1",
      "result": {
        "summary": "GPTScore introduces an evaluation framework for generative AI outputs using pre-trained models to score texts across multiple dimensions, effectively addressing the challenge of custom, multifaceted text evaluation without annotated samples.",
        "keywords": ["GPTScore", "generative AI", "evaluation framework", "pre-trained models", "text generation"]
      }
    }, {
      "pdf_path": "./pdf/AI_VIT_X/2109.06835v1.pdf",
      "title": "The Perils of Using Mechanical Turk\nto Evaluate Open-Ended Text Generation",
      "abstract": "Recent text generation research has increas-\ningly focused on open-ended domains such as\nstory and poetry generation. Because mod-\nels built for such tasks are difficult to evaluate\nautomatically, most researchers in the space\njustify their modeling choices by collecting\ncrowdsourced human judgments of text qual-\nity (e.g., Likert scores of coherence or gram-\nmaticality) from Amazon Mechanical Turk\n(AMT). In this paper, we first conduct a sur-\nvey of 45 open-ended text generation papers\nand find that the vast majority of them fail to\nreport crucial details about their AMT tasks,\nhindering reproducibility. We then run a se-\nries of story evaluation experiments with both\nAMT workers and English teachers and dis-\ncover that even with strict qualification fil-\nters, AMT workers (unlike teachers) fail to\ndistinguish between model-generated text and\nhuman-generated references. We show that\nAMT worker judgments improve when they\nare shown model-generated output alongside\nhuman-generated references, which enables\nthe workers to better calibrate their ratings.\nFinally, interviews with the English teachers\nprovide deeper insights into the challenges of\nthe evaluation process, particularly when rat-\ning model-generated text.",
      "result": {}
    }, {
      "pdf_path": "./pdf/AI_VIT_X/2204.08292v1.pdf",
      "title": "StepGame: A New Benchmark for\nRobust Multi-Hop Spatial Reasoning in Texts",
      "abstract": "Inferring spatial relations in natural language is a crucial abil-\nity an intelligent system should possess. The bAbI dataset\ntries to capture tasks relevant to this domain (task 17 and 19).\nHowever, these tasks have several limitations. Most impor-\ntantly, they are limited to fixed expressions, they are limited\nin the number of reasoning steps required to solve them, and\nthey fail to test the robustness of models to input that contains\nirrelevant or redundant information. In this paper, we present\na new Question-Answering dataset called StepGame for ro-\nbust multi-hop spatial reasoning in texts. Our experiments\ndemonstrate that state-of-the-art models on the bAbI dataset\nstruggle on the StepGame dataset. Moreover, we propose a\nTensor-Product based Memory-Augmented Neural Network\n(TP-MANN) specialized for spatial reasoning tasks. Experi-\nmental results on both datasets show that our model outper-\nforms all the baselines with superior generalization and ro-\nbustness performance.",
      "result": {}
    }
  ]