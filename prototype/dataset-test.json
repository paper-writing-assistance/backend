[
  {
    "pdf_path": "./pdf/AI_VIT_O/2103.15808v1.pdf",
    "title": "CvT: Introducing Convolutions to Vision Transformers",
    "abstract": "We present in this paper a new architecture, named Con-\nvolutional vision Transformer (CvT), that improves Vision\nTransformer (ViT) in performance and efficiency by intro-\nducing convolutions into ViT to yield the best of both de-\nsigns. This is accomplished through two primary modifica-\ntions: a hierarchy of Transformers containing a new convo-\nlutional token embedding, and a convolutional Transformer\nblock leveraging a convolutional projection. These changes\nintroduce desirable properties of convolutional neural net-\nworks (CNNs) to the ViT architecture (i.e. shift, scale,\nand distortion invariance) while maintaining the merits of\nTransformers (i.e. dynamic attention, global context, and\nbetter generalization). We validate CvT by conducting ex-\ntensive experiments, showing that this approach achieves\nstate-of-the-art performance over other Vision Transform-\ners and ResNets on ImageNet-1k, with fewer parame-\nters and lower FLOPs. In addition, performance gains\nare maintained when pretrained on larger datasets (e.g.\nImageNet-22k) and fine-tuned to downstream tasks. Pre-\ntrained on ImageNet-22k, our CvT-W24 obtains a top-1 ac-\ncuracy of 87.7% on the ImageNet-1k val set. Finally, our\nresults show that the positional encoding, a crucial com-\nponent in existing Vision Transformers, can be safely re-\nmoved in our model, simplifying the design for higher res-\nolution vision tasks. Code will be released at https :",
    "result": {
      "summary": "CvT merges convolutions with Vision Transformers, enhancing efficiency and performance. It introduces a convolutional token embedding and a block in Transformers, achieving superior results on ImageNet-1k and maintaining benefits when scaled.",
      "keywords": ["CvT", "Vision Transformers", "convolutions", "ImageNet-1k", "ImageNet-22k", "architecture", "performance", "efficiency", "positional encoding"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/2111.06377v3.pdf",
    "title": "Masked Autoencoders Are Scalable Vision Learners",
    "abstract": "This paper shows that masked autoencoders (MAE) are\nscalable self-supervised learners for computer vision. Our\nMAE approach is simple: we mask random patches of the\ninput image and reconstruct the missing pixels. It is based\non two core designs. First, we develop an asymmetric\nencoder-decoder architecture, with an encoder that oper-\nates only on the visible subset of patches (without mask to-\nkens), along with a lightweight decoder that reconstructs\nthe original image from the latent representation and mask\ntokens. Second, we find that masking a high proportion\nof the input image, e.g., 75%, yields a nontrivial and\nmeaningful self-supervisory task. Coupling these two de-\nsigns enables us to train large models efficiently and ef-\nfectively: we accelerate training (by 3x or more) and im-\nprove accuracy. Our scalable approach allows for learning\nhigh-capacity models that generalize well: e.g., a vanilla\nViT-Huge model achieves the best accuracy (87.8%) among\nmethods that use only ImageNet-1K data. Transfer per-\nformance in downstream tasks outperforms supervised pre-\ntraining and shows promising scaling behavior.",
    "result":{  
      "summary": "MAE employs an asymmetric encoder-decoder for scalable vision learning, masking 75% of input to enhance self-supervision. This simplifies training, boosts accuracy, and excels in downstream tasks using only ImageNet-1K data.",
      "keywords": ["masked autoencoders", "MAE", "scalable", "self-supervised learning", "computer vision", "asymmetric encoder-decoder", "ImageNet-1K", "training efficiency", "transfer performance"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/2105.15168v3.pdf",
    "title": "MSG- Transformer: Exchanging Local Spatial Information by\nManipulating Messenger Tokens",
    "abstract": "Transformers have offered a new methodology of de-\nsigning neural networks for visual recognition. Compared\nto convolutional networks, Transformers enjoy the ability\nof referring to global features at each stage, yet the at-\ntention module brings higher computational overhead that\nobstructs the application of Transformers to process high-\nresolution visual data. This paper aims to alleviate the\nconflict between efficiency and flexibility, for which we pro-\npose a specialized token for each region that serves as a\nmessenger (MSG). Hence, by manipulating these MSG to-\nkens, one can flexibly exchange visual information across\nregions and the computational complexity is reduced. We\nthen integrate the MSG token into a multi-scale architec-\nture named MSG-Transformer. In standard image classi-\nfication and object detection, MSG-Transformer achieves\ncompetitive performance and the inference on both GPU\nand CPU is accelerated. Code is available at https :\n\\ \\ github · com/hustvl /MSG- Transformer.",
    "result": {
      "summary": "MSG-Transformer introduces specialized 'messenger' tokens to exchange visual information efficiently, reducing computational complexity. It shows competitive performance in image classification and object detection, enhancing GPU and CPU inference speeds.",
      "keywords": ["MSG-Transformer", "Transformers", "messenger tokens", "visual recognition", "computational efficiency", "image classification", "object detection"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/2010.11929v2.pdf",
    "title": "AN IMAGE IS WORTH 16x16 WORDS:\nTRANSFORMERS FOR IMAGE RECOGNITION AT SCALE",
    "abstract": "While the Transformer architecture has become the de-facto standard for natural\nlanguage processing tasks, its applications to computer vision remain limited. In\nvision, attention is either applied in conjunction with convolutional networks, or\nused to replace certain components of convolutional networks while keeping their\noverall structure in place. We show that this reliance on CNNs is not necessary\nand a pure transformer applied directly to sequences of image patches can perform\nvery well on image classification tasks. When pre-trained on large amounts of\ndata and transferred to multiple mid-sized or small image recognition benchmarks\n(ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent\nresults compared to state-of-the-art convolutional networks while requiring sub-\nstantially fewer computational resources to train. 1",
    "result": {
      "summary": "The paper demonstrates that Transformers, when trained on sequences of image patches, can surpass conventional CNNs in image classification tasks across multiple benchmarks, with fewer computational resources.",
      "keywords": ["Transformers", "image recognition", "Vision Transformer", "ViT", "CNN replacement", "image classification", "computational efficiency"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/2005.00928v2.pdf",
    "title": "Quantifying Attention Flow in Transformers",
    "abstract": "In the Transformer model, \"self-attention\"\ncombines information from attended embed-\ndings into the representation of the focal em-\nbedding in the next layer. Thus, across lay-\ners of the Transformer, information originating\nfrom different tokens gets increasingly mixed.\nThis makes attention weights unreliable as ex-\nplanations probes. In this paper, we consider\nthe problem of quantifying this flow of infor-\nmation through self-attention. We propose two\nmethods for approximating the attention to in-\nput tokens given attention weights, attention\nrollout and attentionflow, as post hoc methods\nwhen we use attention weights as the relative\nrelevance of the input tokens. We show that\nthese methods give complementary views on\nthe flow of information, and compared to raw\nattention, both yield higher correlations with\nimportance scores of input tokens obtained us-\ning an ablation method and input gradients.",
    "result": {
      "summary": "The paper addresses quantifying information flow in Transformers via 'self-attention', proposing two methods, attention rollout and attentionflow, to enhance the reliability of attention weights for explaining token relevance.",
      "keywords": ["Transformers", "self-attention", "attention flow", "quantifying information", "attention rollout", "attentionflow", "explanations probes"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/2106.02277v1.pdf",
    "title": "Glance-and-Gaze Vision Transformer",
    "abstract": "Recently, there emerges a series of vision Transformers, which show superior\nperformance with a more compact model size than conventional convolutional\nneural networks, thanks to the strong ability of Transformers to model long-range\ndependencies. However, the advantages of vision Transformers also come with a\nprice: Self-attention, the core part of Transformer, has a quadratic complexity to\nthe input sequence length. This leads to a dramatic increase of computation and\nmemory cost with the increase of sequence length, thus introducing difficulties\nwhen applying Transformers to the vision tasks that require dense predictions based\non high-resolution feature maps.",
    "result": {
      "summary": "The Glance-and-Gaze Vision Transformer addresses the high computational cost of traditional vision Transformers due to the quadratic complexity of self-attention, especially in tasks requiring high-resolution feature maps.",
      "keywords": ["Glance-and-Gaze Vision Transformer", "vision Transformers", "self-attention", "computational cost", "high-resolution feature maps", "long-range dependencies"]
    }
  }, {
    "pdf_path": "./pdf/AI_VIT_O/2102.12122v2.pdf",
    "title": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction\nwithout Convolutions",
    "abstract": "Although convolutional neural networks (CNNs) have\nachieved great success in computer vision, this work inves-\ntigates a simpler, convolution-free backbone network use-\nful for many dense prediction tasks. Unlike the recently-\nproposed Vision Transformer (ViT) that was designed for\nimage classification specifically, we introduce the Pyra-\nmid Vision Transformer (PVT), which overcomes the diffi-\nculties of porting Transformer to various dense prediction\ntasks. PVT has several merits compared to current state\nof the arts. (1) Different from ViT that typically yields low-\nresolution outputs and incurs high computational and mem-\nory costs, PVT not only can be trained on dense partitions\nof an image to achieve high output resolution, which is im-\nportant for dense prediction, but also uses a progressive\nshrinking pyramid to reduce the computations of large fea-\nture maps. (2) PVT inherits the advantages of both CNN\nand Transformer, making it a unified backbone for vari-",
    "result": {
      "summary": "Pyramid Vision Transformer (PVT) offers a convolution-free, efficient backbone for dense prediction tasks, addressing limitations of Vision Transformer with high-resolution outputs and reduced computational demands.",
      "keywords": ["Pyramid Vision Transformer", "PVT", "dense prediction", "convolution-free backbone", "high-resolution outputs", "Vision Transformer", "computational efficiency"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/1512.03385v1.pdf",
    "title": "Deep Residual Learning for Image Recognition",
    "abstract": "Deeper neural networks are more difficult to train. We\npresent a residual learning framework to ease the training\nof networks that are substantially deeper than those used\npreviously. We explicitly reformulate the layers as learn-\ning residual functions with reference to the layer inputs, in-\nstead of learning unreferenced functions. We provide com-\nprehensive empirical evidence showing that these residual\nnetworks are easier to optimize, and can gain accuracy from\nconsiderably increased depth. On the ImageNet dataset we\nevaluate residual nets with a depth of up to 152 layers-8x\ndeeper than VGG nets [41] but still having lower complex-\nity. An ensemble of these residual nets achieves 3.57% error\non the ImageNet test set. This result won the 1st place on the\nILSVRC 2015 classification task. We also present analysis\non CIFAR-10 with 100 and 1000 layers.",
    "result": {
      "summary": "The paper introduces a residual learning framework for easier training of very deep neural networks, showing that these networks optimize better and achieve higher accuracy, demonstrated on ImageNet with up to 152 layers.",
      "keywords": ["Deep Residual Learning", "image recognition", "neural networks", "residual networks", "ImageNet", "deep learning", "ILSVRC 2015"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/2103.14030v2.pdf",
    "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows",
    "abstract": "This paper presents a new vision Transformer, called\nSwin Transformer, that capably serves as a general-purpose\nbackbone for computer vision. Challenges in adapting\nTransformer, from language to vision arise from differences\nbetween the two domains, such as large variations in the\nscale of visual entities and the high resolution of pixels\nin images compared to words in text. To address these\ndifferences, we propose a hierarchical Transformer whose\nrepresentation is computed with Shifted windows. The\nshifted windowing scheme brings greater efficiency by lim-\niting self-attention computation to non-overlapping local\nwindows while also allowing for cross-window connection.\nThis hierarchical architecture has the flexibility to model\nat various scales and has linear computational complexity\nwith respect to image size. These qualities of Swin Trans-\nformer make it compatible with a broad range of vision\ntasks, including image classification (87.3 top-1 accuracy\non ImageNet-1K) and dense prediction tasks such as object\ndetection (58.7 box AP and 51.1 mask AP on COCO test-\ndev) and semantic segmentation (53.5 mIoU on ADE20K\nval). Its performance surpasses the previous state-of-the-\nart by a large margin of +2.7 box AP and +2.6 mask AP on\nCOCO, and +3.2 mIoU on ADE20K, demonstrating the po-\ntential of Transformer-based models as vision backbones.\nThe hierarchical design and the shifted window approach\nalso prove beneficial for all-MLP architectures. The code\nand models are publicly available at https : / /github.\ncom/mi crosoft / Swin-Transformer.",
    "result": {
      "summary": "Swin Transformer introduces a hierarchical, shifted window approach for efficient vision Transformers, enhancing performance in image classification, object detection, and segmentation, proving superior to previous models.",
      "keywords": ["Swin Transformer", "vision Transformer", "hierarchical", "shifted windows", "image classification", "object detection", "semantic segmentation", "computational efficiency", "ImageNet-1K", "COCO", "ADE20K"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/2111.07624v1.pdf",
    "title": "Attention Mechanisms in Computer Vision:\nA Survey",
    "abstract": "Abstract-Humans can naturally and effectively find salient regions in complex scenes. Motivated by this observation, attention\nmechanisms were introduced into computer vision with the aim of imitating this aspect of the human visual system. Such an attention\nmechanism can be regarded as a dynamic weight adjustment process based on features of the input image. Attention mechanisms have\nachieved great success in many visual tasks, including image classification, object detection, semantic segmentation, video\nunderstanding, image generation, 3D vision, multi-modal tasks and self-supervised learning. In this survey, we provide a comprehensive\nreview of various attention mechanisms in computer vision and categorize them according to approach, such as channel attention, spatial\nattention, temporal attention and branch attention; a related repository https://hub.com/MengfasoGuokAreernermew-Aters is\ndedicated to collecting related work. We also suggest future directions for attention mechanism research.",
    "result": {
      "summary": "This survey paper explores attention mechanisms in computer vision, categorizing them by type and highlighting their impact on various visual tasks. It suggests future research directions and provides a repository for related works.",
      "keywords": ["attention mechanisms", "computer vision", "image classification", "object detection", "semantic segmentation", "video understanding", "3D vision", "multi-modal tasks", "self-supervised learning"]
    }       
  }, {
    "pdf_path": "./pdf/AI_VIT_O/2104.12533v5.pdf",
    "title": "Visformer: The Vision-friendly Transformer",
    "abstract": "Abstract-The past few years have witnessed the rapid devel-\nopment of applying the Transformer module to vision problems.\nWhile some researchers have demonstrated that Transformer-\nbased models enjoy a favorable ability of fitting data, there are\nstill growing number of evidences showing that these models\nsuffer over-fitting especially when the training data is limited.\nThis paper offers an empirical study by performing step-by-\nstep operations to gradually transit a Transformer-based model\nto a convolution-based model. The results we obtain during\nthe transition process deliver useful messages for improving\nvisual recognition. Based on these observations, we propose a\nnew architecture named Visformer, which is abbreviated from\nthe 'Vision-friendly Transformer'. With the same computational\ncomplexity, Visformer outperforms both the Transformer-based\nand convolution-based models in terms of ImageNet classification\nand object detection performance, and the advantage becomes\nmore significant when the model complexity is lower or the\ntraining set is smaller. The code is available at https://github.\ncom/danczs/Visformer.",
    "result": {
      "summary": "Visformer merges Transformer and convolutional approaches, tackling over-fitting in vision tasks with limited data, and surpassing traditional models in ImageNet classification and object detection.",
      "keywords": ["Visformer", "Vision-friendly Transformer", "Transformer", "convolutional models", "over-fitting", "ImageNet classification", "object detection"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/2204.02557v2.pdf",
    "title": "MixFormer: Mixing Features across Windows and Dimensions",
    "abstract": "While local-window self-attention performs notably in\nvision tasks, it suffers from limited receptive field and weak\nmodeling capability issues. This is mainly because it per-\nforms self-attention within non-overlapped windows and\nshares weights on the channel dimension. We propose Mix-\nFormer to find a solution. First, we combine local-window\nself-attention with depth-wise convolution in a parallel de-\nsign, modeling cross-window connections to enlarge the re-\nceptive fields. Second, we propose bi-directional interac-\ntions across branches to provide complementary clues in\nthe channel and spatial dimensions. These two designs are\nintegrated to achieve efficient feature mixing among win-\ndows and dimensions. Our MixFormer provides compet-\nitive results on image classification with EfficientNet and\nshows better results than RegNet and Swin Transformer.\nPerformance in downstream tasks outperforms its alterna-\ntives by significant margins with less computational costs\nin 5 dense prediction tasks on MS COCO, ADE20k, and\nLVIS. Code is available at https : / / github · com /\nPaddl ePaddl e/Paddl eClas.",
    "result": {
      "summary": "MixFormer enhances local-window self-attention in vision tasks by integrating depth-wise convolution and bi-directional interactions, improving receptive fields and feature mixing, excelling in image classification and dense prediction tasks.",
      "keywords": ["MixFormer", "feature mixing", "local-window self-attention", "depth-wise convolution", "bi-directional interactions", "image classification", "dense prediction tasks", "EfficientNet", "RegNet", "Swin Transformer"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/2103.11886v4.pdf",
    "title": "Deep ViT: Towards Deeper Vision Transformer",
    "abstract": "Vision transformers (ViTs) have been successfully ap-\nplied in image classification tasks recently. In this paper,\nwe show that, unlike convolution neural networks (CNNs)\nthat can be improved by stacking more convolutional lay-\ners, the performance of ViTs saturate fast when scaled to\nbe deeper. More specifically, we empirically observe that\nsuch scaling difficulty is caused by the attention collapse\nissue: as the transformer goes deeper, the attention maps\ngradually become similar and even much the same after\ncertain layers. In other words, the feature maps tend to\nbe identical in the top layers of deep ViT models. This\nfact demonstrates that in deeper layers of ViTs, the self-\nattention mechanism fails to learn effective concepts for\nrepresentation learning and hinders the model from get-\nting expected performance gain. Based on above obser-\nvation, we propose a simple yet effective method, named\nRe-attention, to re-generate the attention maps to increase\ntheir diversity at different layers with negligible computa-\ntion and memory cost. The proposed method makes it feasi-\nble to train deeper ViT models with consistent performance\nimprovements via minor modification to existing ViT mod-\nels. Notably, when training a deep ViT model with 32 trans-\nformer blocks, the Top-1 classification accuracy can be im-\nproved by 1.6% on ImageNet. Code is publicly available at\nhttps : //github · com/ zhoudaquan/ dvit_repo.",
    "result": {
      "summary": "Deep ViT addresses the 'attention collapse' in deep Vision Transformers by introducing 'Re-attention' to diversify attention maps, enabling deeper ViT models to achieve performance gains, evidenced by improved ImageNet accuracy.",
      "keywords": ["Deep ViT", "Vision Transformers", "attention collapse", "Re-attention", "deeper layers", "image classification", "performance improvement", "ImageNet"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/2304.02643v1.pdf",
    "title": "Segment Anything",
    "abstract": "We introduce the Segment Anything (SA) project: a new\ntask, model, and datasetfor image segmentation. Using our\nefficient model in a data collection loop, we built the largest\nsegmentation dataset to date (by far), with over 1 billion\nmasks on 11M licensed and privacy respecting images. The\nmodel is designed and trained to be promptable, SO it can\ntransfer zero-shot to new image distributions and tasks. We\nevaluate its capabilities on numerous tasks and find that\nits zero-shot performance is impressive - often competitive\nwith or even superior to prior fully supervised results. We\nare releasing the Segment Anything Model (SAM) and cor-\nresponding dataset (SA-1B) of 1B masks and 11M images at\nhttps://segment-anything.com to foster research into foun-\ndation models for computer vision.",
    "result": {
      "summary": "The Segment Anything project introduces a promptable model and the largest dataset for image segmentation, featuring over 1 billion masks. The model excels in zero-shot tasks, matching or surpassing supervised methods.",
      "keywords": ["Segment Anything", "image segmentation", "largest dataset", "zero-shot performance", "promptable model", "foundation models", "computer vision"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/2012.12877v2.pdf",
    "title": "Training data-efficient image transformers\n& distillation through attention",
    "abstract": "Recently, neural networks purely based on attention were shown to ad-\ndress image understanding tasks such as image classification. These high-\nperforming vision transformers are pre-trained with hundreds of millions\nof images using a large infrastructure, thereby limiting their adoption.",
    "result": {
      "summary": "The paper explores attention-based vision transformers for image classification, noting their high performance but also the extensive pre-training and infrastructure requirements limiting broader use.",
      "keywords": ["image transformers", "attention mechanisms", "image classification", "data efficiency", "infrastructure requirements"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/2010.04159v4.pdf",
    "title": "DEFORMABLE DETR: DEFORMABLE TRANSFORMERS\nFOR END-TO-END OBJECT DETECTION",
    "abstract": "DETR has been recently proposed to eliminate the need for many hand-designed\ncomponents in object detection while demonstrating good performance. However,\nit suffers from slow convergence and limited feature spatial resolution, due to the\nlimitation of Transformer attention modules in processing image feature maps. To\nmitigate these issues, we proposed Deformable DETR, whose attention modules\nonly attend to a small set of key sampling points around a reference. Deformable\nDETR can achieve better performance than DETR (especially on small objects)\nwith 10x less training epochs. Extensive experiments on the COCO benchmark\ndemonstrate the effectiveness of our approach. Code is released at https : / /\ngithub · com/ fundamentalvision /Deformable-DETR.",
    "result": {
      "summary": "Deformable DETR enhances DETR for object detection by focusing attention on key points, improving performance and training efficiency, especially for small objects, validated on the COCO benchmark.",
      "keywords": ["Deformable DETR", "DETR", "object detection", "transformer attention", "COCO benchmark", "efficient training", "feature spatial resolution"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/1911.03584v2.pdf",
    "title": "ON THE RELATIONSHIP BETWEEN SELF-ATTENTION\nAND CONVOLUTIONAL LAYERS",
    "abstract": "Recent trends of incorporating attention mechanisms in vision have led re-\nsearchers to reconsider the supremacy of convolutional layers as a primary build-\ning block. Beyond helping CNNs to handle long-range dependencies, Ramachan-\ndran et al. (2019) showed that attention can completely replace convolution and\nachieve state-of-the-art performance on vision tasks. This raises the question: do\nlearned attention layers operate similarly to convolutional layers? This work pro-\nvides evidence that attention layers can perform convolution and, indeed, they of-\nten learn to do so in practice. Specifically, we prove that a multi-head self-attention\nlayer with sufficient number of heads is at least as expressive as any convolutional\nlayer. Our numerical experiments then show that self-attention layers attend to\npixel-grid patterns similarly to CNN layers, corroborating our analysis. Our code\nis publicly available1",
    "result": {
      "summary": "This study reveals that self-attention layers can mimic convolutional layers, proving multi-head self-attention's capacity to match CNNs' expressiveness and performance in vision tasks.",
      "keywords": ["self-attention", "convolutional layers", "vision tasks", "multi-head self-attention", "CNN", "expressiveness"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/2005.14165v4.pdf",
    "title": "Language Models are Few-Shot Learners",
    "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training\non a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic\nin architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of\nthousands of examples. By contrast, humans can generally perform a new language task from only\na few examples or from simple instructions - something which current NLP systems still largely\nstruggle to do. Here we show that scaling up language models greatly improves task-agnostic,\nfew-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-\ntuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion\nparameters, 10x more than any previous non-sparse language model, and test its performance in\nthe few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning,\nwith tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3\nachieves strong performance on many NLP datasets, including translation, question-answering, and\ncloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as\nunscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same\ntime, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some\ndatasets where GPT-3 faces methodological issues related to training on large web corpora. Finally,\nwe find that GPT-3 can generate samples of news articles which human evaluators have difficulty\ndistinguishing from articles written by humans. We discuss broader societal impacts of this finding\nand of GPT-3 in general.",
    "result": {
      "summary": "GPT-3, a language model with 175 billion parameters, enhances few-shot learning in NLP, performing competitively without fine-tuning across diverse tasks, though some challenges remain.",
      "keywords": ["GPT-3", "few-shot learning", "language models", "NLP tasks", "no fine-tuning", "task-agnostic"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/1805.00932v1.pdf",
    "title": "Exploring the Limits of\nWeakly Supervised Pretraining",
    "abstract": "Abstract. State-of-the-art visual perception models for a wide range\nof tasks rely on supervised pretraining. ImageNet classification is the de\nfacto pretraining task for these models. Yet, ImageNet is now nearly ten\nyears old and is by modern standards \"small\" Even SO, relatively little is\nknown about the behavior of pretraining with datasets that are multiple\norders of magnitude larger. The reasons are obvious: such datasets are\ndifficult to collect and annotate. In this paper, we present a unique study\nof transfer learning with large convolutional networks trained to predict\nhashtags on billions of social media images. Our experiments demon-\nstrate that training for large-scale hashtag prediction leads to excellent\nresults. We show improvements on several image classification and object\ndetection tasks, and report the highest ImageNet-1k single-crop, top-1\naccuracy to date: 85.4% (97.6% top-5). We also perform extensive ex-\nperiments that provide novel empirical data on the relationship between\nlarge-scale pretraining and transfer learning performance.",
    "result": {
      "summary": "This study explores weakly supervised pretraining on large-scale datasets using social media hashtags, showing significant improvements in image classification and object detection tasks, achieving record ImageNet accuracy.",
      "keywords": ["weakly supervised pretraining", "large-scale datasets", "hashtag prediction", "image classification", "object detection", "ImageNet", "transfer learning"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/2105.15203v3.pdf",
    "title": "SegFormer: Simple and Efficient Design for Semantic\nSegmentation with Transformers",
    "abstract": "We present SegFormer, a simple, efficient yet powerful semantic segmentation\nframework which unifies Transformers with lightweight multilayer perceptron\n(MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises\na novel hierarchically structured Transformer encoder which outputs multiscale\nfeatures. It does not need positional encoding, thereby avoiding the interpolation of\npositional codes which leads to decreased performance when the testing resolution\ndiffers from training. 2) SegFormer avoids complex decoders. The proposed\nMLP decoder aggregates information from different layers, and thus combining\nboth local attention and global attention to render powerful representations. We\nshow that this simple and lightweight design is the key to efficient segmentation\non Transformers. We scale our approach up to obtain a series of models from\nSegFormer-B0 to SegFormer-B5, reaching significantly better performance and\nefficiency than previous counterparts. For example, SegFormer-B4 achieves 50.3%\nmIoU on ADE20K with 64M parameters, being 5x smaller and 2.2% better than\nthe previous best method. Our best model, SegFormer-B5, achieves 84.0% mloU on\nCityscapes validation set and shows excellent zero-shot robustness on Cityscapes-C.\nCode will be released at: gi thub.com/NVlabs /SegF ormer.",
    "result": {
      "summary": "SegFormer integrates Transformers with MLP decoders for semantic segmentation, offering multi-scale features without positional encoding and a simple yet effective MLP decoder for improved segmentation efficiency and performance.",
      "keywords": ["SegFormer", "semantic segmentation", "Transformers", "MLP decoder", "multi-scale features", "positional encoding", "efficient segmentation"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/2107.00652v3.pdf",
    "title": "CSWin Transformer: A General Vision Transformer Backbone with\nCross-Shaped Windows",
    "abstract": "We present CSWin Transformer, an efficient and effec-\ntive Transformer-based backbone for general-purpose vision\ntasks. A challenging issue in Transformer design is that\nglobal self-attention is very expensive to compute whereas\nlocal self-attention often limits the field of interactions of\neach token. To address this issue, we develop the Cross-\nShaped Window self-attention mechanism for computing\nself-attention in the horizontal and vertical stripes in parallel\nthat form a cross-shaped window, with each stripe obtained\nby splitting the inputfeature into stripes of equal width. We\nprovide a mathematical analysis of the effect of the stripe\nwidth and vary the stripe width for different layers of the\nTransformer network which achieves strong modeling capa-\nbility while limiting the computation cost. We also introduce\nLocally-enhanced Positional Encoding (LePE), which han-\ndles the local positional information better than existing\nencoding schemes. LePE naturally supports arbitrary input\nresolutions, and is thus especially effective and friendly for\ndownstream tasks. Incorporated with these designs and a hi-\nerarchical structure, CSWin Transformer demonstrates com-\npetitive performance on common vision tasks. Specifically,\nit achieves 85.4% Top-1 accuracy on ImageNet-1K without\nany extra training data or label, 53.9 box AP and 46.4 mask\nAP on the COCO detection task, and 52.2 mIOU on the\nADE20K semantic segmentation task, surpassing previous\nstate-of-the-art Swin Transformer backbone by +1.2, +2.0,\n+1.4, and +2.0 respectively under the similar FLOPs setting.\nBy further pretraining on the larger dataset ImageNet-21K,\nwe achieve 87.5% Top-1 accuracy on ImageNet-1K and high\nsegmentation performance on ADE20K with 55.7 mIoU.",
    "result": {
      "summary": "CSWin Transformer introduces a Cross-Shaped Window self-attention and Locally-enhanced Positional Encoding, enhancing vision tasks efficiency and accuracy, surpassing previous models like Swin Transformer.",
      "keywords": ["CSWin Transformer", "Cross-Shaped Window", "self-attention", "Locally-enhanced Positional Encoding", "vision tasks", "ImageNet-1K", "COCO", "ADE20K"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/1904.09925v5.pdf",
    "title": "Attention Augmented Convolutional Networks",
    "abstract": "Convolutional networks have been the paradigm of\nchoice in many computer vision applications. The convolu-\ntion operation however has a significant weakness in that it\nonly operates on a local neighborhood, thus missing global\ninformation. Self-attention, on the other hand, has emerged\nas a recent advance to capture long range interactions, but\nhas mostly been applied to sequence modeling and gener-\native modeling tasks. In this paper, we consider the use of\nself-attentionfor discriminative visual tasks as an alterna-\ntive to convolutions. We introduce a novel two-dimensional\nrelative self-attention mechanism that proves competitive\nin replacing convolutions as a stand-alone computational\nprimitive for image classification. We find in control exper-\niments that the best results are obtained when combining\nboth convolutions and self-attention. We therefore propose\nto augment convolutional operators with this self-attention\nmechanism by concatenating convolutional feature maps\nwith a set offeature maps produced via self-attention. Ex-\ntensive experiments show that Attention Augmentation leads\nto consistent improvements in image classification on Im-\nageNet and object detection on COCO across many dif-\nferent models and scales, including ResNets and a state-\nof-the art mobile constrained network, while keeping the\nnumber of parameters similar. In particular, our method\nachieves a 1.3% top-1 accuracy improvement on ImageNet\nclassification over a ResNet50 baseline and outperforms\nother attention mechanisms for images such as Squeeze-\nand-Excitation [17]. It also achieves an improvement of\n1.4 mAP in COCO Object Detection on top of a RetinaNet\nbaseline.",
    "result":  {
      "summary": "This paper introduces an Attention Augmented Convolutional Network that blends self-attention with convolutions, enhancing global information processing in vision tasks, leading to performance gains in ImageNet classification and COCO detection.",
      "keywords": ["Attention Augmented Convolutional Networks", "self-attention", "convolutions", "image classification", "object detection", "ImageNet", "COCO", "global information"]
    }
  }, {
    "pdf_path": "./pdf/AI_VIT_O/2101.11986v3.pdf",
    "title": "Tokens-to- Token ViT: Training Vision Transformers from Scratch on ImageNet",
    "abstract": "Transformers, which are popular for language modeling,\nhave been explored for solving vision tasks recently, e.g.,\nthe Vision Transformer (ViT) for image classification. The\nViT model splits each image into a sequence of tokens with\nfixed length and then applies multiple Transformer layers\nto model their global relation for classification. However,\nViT achieves inferior performance to CNNs when trained\nfrom scratch on a midsize dataset like ImageNet. We find\nit is because: 1) the simple tokenization of input images\nfails to model the important local structure such as edges\nand lines among neighboring pixels, leading to low train-\ning sample efficiency; 2) the redundant attention backbone\ndesign of ViT leads to limited feature richness for fixed com-\nputation budgets and limited training samples. To overcome\nsuch limitations, we propose a new Tokens-To-Token Vi-\nsion Transformer (T2T- ViT), which incorporates 1) a layer-\nwise Tokens-to-Token (T2T) transformation to progressively\nstructurize the image to tokens by recursively aggregating\nneighboring Tokens into one Token (Tokens-to-Token), such\nthat local structure represented by surrounding tokens can\nbe modeled and tokens length can be reduced; 2) an ef-\nficient backbone with a deep-narrow structure for vision\ntransformer motivated by CNN architecture design after\nempirical study. Notably, T2T-ViT reduces the parameter\ncount and MACs of vanilla ViT by half, while achieving\nmore than 3.0% improvement when trained from scratch on\nImageNet. It also outperforms ResNets and achieves com-\nparable performance with MobileNets by directly training\non ImageNet. For example, T2T-ViT with comparable size\nto ResNet50 (21.5M parameters) can achieve 83.3% topl\naccuracy in image resolution 384x384 on ImageNet. 1",
    "result": {
      "summary": "Tokens-to-Token ViT improves the Vision Transformer by incorporating a Tokens-to-Token transformation to better model local structures and a deep-narrow backbone, enhancing ImageNet training efficiency and performance.",
      "keywords": ["Tokens-to-Token ViT", "Vision Transformer", "T2T-ViT", "image classification", "ImageNet", "local structure modeling", "transformer efficiency"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/2012.15840v3.pdf",
    "title": "Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective\nwith Transformers",
    "abstract": "Most recent semantic segmentation methods adopt\na fully-convolutional network (FCN) with an encoder-\ndecoder architecture. The encoder progressively reduces\nthe spatial resolution and learns more abstract/semantic\nvisual concepts with larger receptive fields. Since context\nmodeling is critical for segmentation, the latest efforts have\nbeen focused on increasing the receptive field, through ei-\nther dilated/atrous convolutions or inserting attention mod-\nules. However, the encoder-decoder based FCN architec-\nture remains unchanged. In this paper, we aim to provide\nan alternative perspective by treating semantic segmenta-\ntion as a sequence-to-sequence prediction task. Specifically,\nwe deploy a pure transformer (i.e., without convolution and\nresolution reduction) to encode an image as a sequence of\npatches. With the global context modeled in every layer of\nthe transformer, this encoder can be combined with a simple\ndecoder to provide a powerful segmentation model, termed\nSEgmentation TRansformer (SETR). Extensive experiments\nshow that SETR achieves new state of the art on ADE20K\n(50.28% mIoU), Pascal Context (55.83% mIoU) and com-\npetitive results on Cityscapes. Particularly, we achieve the\nfirst position in the highly competitive ADE20K test server\nleaderboard on the day of submission.",
    "result": {
      "summary": "SETR redefines semantic segmentation as a sequence-to-sequence task using a pure transformer, avoiding convolutions and achieving state-of-the-art results on ADE20K and Pascal Context, and competitive outcomes on Cityscapes.",
      "keywords": ["SETR", "semantic segmentation", "sequence-to-sequence", "transformer", "global context", "ADE20K", "Pascal Context", "Cityscapes"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/2104.06399v2.pdf",
    "title": "Co-Scale Conv-Attentional Image Transformers",
    "abstract": "In this paper, we present Co-scale conv-attentional image\nTransformers (CoaT), a Transformer-based image classifier\nequipped with co-scale and conv-attentional mechanisms.\nFirst, the co-scale mechanism maintains the integrity of\nTransformers' encoder branches at individual scales, while\nallowing representations learned at different scales to ef-\nfectively communicate with each other; we design a series\nof serial and parallel blocks to realize the co-scale mecha-\nnism. Second, we devise a conv-attentional mechanism by\nrealizing a relative position embedding formulation in the\nfactorized attention module with an efficient convolution-like\nimplementation. CoaT empowers image Transformers with\nenriched multi-scale and contextual modeling capabilities.\nOn ImageNet, relatively small CoaT models attain superior\nclassification results compared with similar-sized convolu-\ntional neural networks and image/vision Transformers. The\neffectiveness of CoaT's backbone is also illustrated on ob-\nject detection and instance segmentation, demonstrating its\napplicability to downstream computer vision tasks.",
    "result": {
      "summary": "CoaT introduces co-scale and conv-attentional mechanisms to enhance image Transformers, enabling multi-scale interaction and efficient contextual modeling, showing superior results on ImageNet and various vision tasks.",
      "keywords": ["Co-scale conv-attentional image Transformers", "CoaT", "image classifier", "multi-scale interaction", "contextual modeling", "ImageNet", "object detection", "instance segmentation"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/2005.12872v3.pdf",
    "title": "End-to-End Object Detection with Transformers",
    "abstract": "Abstract. We present a new method that views object detection as a\ndirect set prediction problem. Our approach streamlines the detection\npipeline, effectively removing the need for many hand-designed compo-\nnents like a non-maximum suppression procedure or anchor generation\nthat explicitly encode our prior knowledge about the task. The main\ningredients of the new framework, called DEtection TRansformer or\nDETR, are a set-based global loss that forces unique predictions via bi-\npartite matching, and a transformer encoder-decoder architecture. Given\na fixed small set of learned object queries, DETR reasons about the re-\nlations of the objects and the global image context to directly output\nthe final set of predictions in parallel. The new model is conceptually\nsimple and does not require a specialized library, unlike many other\nmodern detectors. DETR demonstrates accuracy and run-time perfor-\nmance on par with the well-established and highly-optimized Faster R-\nCNN baseline on the challenging COCO object detection dataset. More-\nover, DETR can be easily generalized to produce panoptic segmentation\nin a unified manner. We show that it significantly outperforms com-\npetitive baselines. Training code and pretrained models are available at\nhttps: / /github. com/facebookresearch/ detr.",
    "result": {
      "summary": "DETR transforms object detection into a direct set prediction problem, eliminating many traditional components like non-maximum suppression, and uses a transformer architecture to output predictions in parallel, showing strong performance on COCO.",
      "keywords": ["DETR", "object detection", "transformers", "set prediction", "COCO dataset", "panoptic segmentation", "encoder-decoder architecture"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/1809.10853v3.pdf",
    "title": "ADAPTIVE INPUT REPRESENTATIONS\nFOR NEURAL LANGUAGE MODELING",
    "abstract": "We introduce adaptive input representations for neural language modeling which\nextend the adaptive softmax of Grave et al. (2017) to input representations of vari-\nable capacity. There are several choices on how to factorize the input and output\nlayers, and whether to model words, characters or sub-word units. We perform a\nsystematic comparison of popular choices for a self-attentional architecture. Our\nexperiments show that models equipped with adaptive embeddings are more than\ntwice as fast to train than the popular character input CNN while having a lower\nnumber of parameters. On the WIKITEXT-103 benchmark we achieve 18.7 per-\nplexity, an improvement of 10.5 perplexity compared to the previously best pub-\nlished result and on the BILLION WORD benchmark, we achieve 23.02 perplexity.1",
    "result": {
      "summary": "The paper introduces adaptive input representations for neural language modeling, enhancing training speed and reducing parameters, achieving significant improvements in perplexity on WIKITEXT-103 and BILLION WORD benchmarks.",
      "keywords": ["adaptive input representations", "neural language modeling", "adaptive embeddings", "self-attentional architecture", "WIKITEXT-103", "BILLION WORD", "perplexity improvement"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/2103.00112v3.pdf",
    "title": "Transformer in Transformer",
    "abstract": "Transformer is a new kind of neural architecture which encodes the input data as\npowerful features via the attention mechanism. Basically, the visual transformers\nfirst divide the input images into several local patches and then calculate both\nrepresentations and their relationship. Since natural images are of high complexity\nwith abundant detail and color information, the granularity of the patch dividing is\nnot fine enough for excavating features of objects in different scales and locations.\nIn this paper, we point out that the attention inside these local patches are also\nessential for building visual transformers with high performance and we explore\na new architecture, namely, Transformer iN Transformer (TNT). Specifically, we\nregard the local patches (e.g., 16x16) as \"visual sentences\" and present to further\ndivide them into smaller patches (e.g., 4x4) as \"visual words\". The attention of\neach word will be calculated with other words in the given visual sentence with\nnegligible computational costs. Features of both words and sentences will be ag-\ngregated to enhance the representation ability. Experiments on several benchmarks\ndemonstrate the effectiveness of the proposed TNT architecture, e.g., we achieve an\n81.5% top-1 accuracy on the ImageNet, which is about 1.7% higher than that of the\nstate-of-the-art visual transformer with similar computational cost. The PyTorch\ncode is available at https : // github · com/huawei -noah/ CV-Backbones, and\nthe MindSpore code is available at https : / /gitee · com/mindspore/models/\ntree/master /research/ cv /TNT.",
    "result": {
      "summary": "The Transformer in Transformer (TNT) model enhances visual transformers by refining the granularity of patch division, applying attention within local patches, and aggregating features from 'visual words' and 'sentences', achieving superior ImageNet accuracy.",
      "keywords": ["Transformer in Transformer", "TNT", "visual transformers", "local patches", "attention mechanism", "ImageNet", "feature aggregation"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/2201.03545v2.pdf",
    "title": "A ConvNet for the 2020s",
    "abstract": "The \"Roaring 20s\" of visual recognition began with the\nintroduction of Vision Transformers (ViTs), which quickly\nsuperseded ConvNets as the state-of-the-art image classifica-\ntion model. A vanilla ViT, on the other hand, faces difficulties\nwhen applied to general computer vision tasks such as object\ndetection and semantic segmentation. It is the hierarchical\nTransformers (e.g., Swin Transformers) that reintroduced sev-\neral ConvNet priors, making Transformers practically viable\nas a generic vision backbone and demonstrating remarkable\nperformance on a wide variety of vision tasks. However,\nthe effectiveness of such hybrid approaches is still largely\ncredited to the intrinsic superiority of Transformers, rather\nthan the inherent inductive biases of convolutions. In this\nwork, we reexamine the design spaces and test the limits of\nwhat a pure ConvNet can achieve. We gradually \"modernize\"\na standard ResNet toward the design of a vision Transformer,\nand discover several key components that contribute to the\nperformance difference along the way. The outcome of this\nexploration is a family of pure ConvNet models dubbed Con-\nvNeXt. Constructed entirely from standard ConvNet modules,\nConvNeXts compete favorably with Transformers in terms of\naccuracy and scalability, achieving 87.8% ImageNet top-1\naccuracy and outperforming Swin Transformers on COCO\ndetection and ADE20K segmentation, while maintaining the\nsimplicity and efficiency of standard ConvNets.",
    "result": {
      "summary": "The study modernizes ConvNets, creating ConvNeXt models that rival Transformers in accuracy and scalability on vision tasks like ImageNet classification and COCO detection, retaining ConvNet efficiency.",
      "keywords": ["ConvNet", "2020s", "Vision Transformers", "ConvNeXt", "ImageNet", "COCO detection", "ADE20K segmentation", "hierarchical Transformers"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/2107.10224v4.pdf",
    "title": "CYCLEMLP: A MLP-LIKE ARCHITECTURE FOR\nDENSE PREDICTION",
    "abstract": "This paper presents a simple MLP-like architecture, CycleMLP, which is a versa-\ntile backbone for visual recognition and dense predictions. As compared to mod-\nern MLP architectures, e.g. , MLP-Mixer (Tolstikhin et al., 2021), ResMLP (Tou-\nvron et al., 2021a), and gMLP (Liu et al., 2021a), whose architectures are cor-\nrelated to image size and thus are infeasible in object detection and segmen-\ntation, CycleMLP has two advantages compared to modern approaches. (1) It\ncan cope with various image sizes. (2) It achieves linear computational com-\nplexity to image size by using local windows. In contrast, previous MLPs have\nO(N2) computations due to fully spatial connections. We build a family of\nmodels which surpass existing MLPs and even state-of-the-art Transformer-based\nmodels, e.g. Swin Transformer (Liu et al., 2021b), while using fewer parame-\nters and FLOPs. We expand the MLP-like models' applicability, making them\na versatile backbone for dense prediction tasks. CycleMLP achieves competi-\ntive results on object detection, instance segmentation, and semantic segmenta-\ntion. In particular, CycleMLP-Tiny outperforms Swin-Tiny by 1.3% mloU on\nADE20K dataset with fewer FLOPs. Moreover, CycleMLP also shows excel-\nlent zero-shot robustness on ImageNet-C dataset. Code is available at https :",
    "result": {
      "summary": "CycleMLP offers a versatile, MLP-like architecture that handles different image sizes with linear computational complexity, outperforming existing MLPs and Transformer-based models on dense prediction tasks with fewer parameters and FLOPs.",
      "keywords": ["CycleMLP", "MLP-like architecture", "visual recognition", "dense prediction", "object detection", "instance segmentation", "semantic segmentation", "ADE20K", "ImageNet-C", "computational complexity"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/2110.14168v2.pdf",
    "title": "Training Verifiers to Solve Math Word Problems",
    "abstract": "State-of-the-art language models can match human performance on\nmany tasks, but they still struggle to robustly perform multi-step mathe-\nmatical reasoning. To diagnose the failures of current models and support\nresearch, we introduce GSM8K, a dataset of 8.5K high quality linguisti-\ncally diverse grade school math word problems. We find that even the\nlargest transformer models fail to achieve high test performance, despite\nthe conceptual simplicity of this problem distribution. To increase per-\nformance, we propose training verifiers to judge the correctness of model\ncompletions. At test time, we generate many candidate solutions and\nselect the one ranked highest by the verifier. We demonstrate that ver-\nification significantly improves performance on GSM8K, and we provide\nstrong empirical evidence that verification scales more effectively with\nincreased data than a finetuning baseline.",
    "result": {
      "summary": "Introducing the GSM8K dataset for diagnosing multi-step math problem-solving in language models, this paper proposes using verifiers to enhance solution accuracy by ranking model-generated answers, showing improved performance and scalability.",
      "keywords": ["GSM8K", "math word problems", "language models", "transformer models", "verifiers", "solution verification", "model performance"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1708.05031v2.pdf",
    "title": "Neural Collaborative Filtering",
    "abstract": "In recent years, deep neural networks have yielded immense\nsuccess on speech recognition, computer vision and natural\nlanguage processing. However, the exploration of deep neu-\nral networks on recommender systems has received relatively\nless scrutiny. In this work, we strive to develop techniques\nbased on neural networks to tackle the key problem in rec-\nommendation collaborative filtering on the basis of\nimplicit feedback.",
    "result": {
      "summary": "This work explores deep neural networks for recommender systems, particularly focusing on collaborative filtering with implicit feedback, an area previously less examined compared to other applications.",
      "keywords": ["neural collaborative filtering", "deep neural networks", "recommender systems", "implicit feedback", "collaborative filtering"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/2402.03620v1.pdf",
    "title": "SELF-DISCOVER: Large Language Models Self-Compose Reasoning Structures",
    "abstract": "We introduce SELF-DISCOVER, a general frame-\nwork for LLMs to self-discover the task-intrinsic\nreasoning structures to tackle complex reasoning\nproblems that are challenging for typical prompt-\ning methods. Core to the framework is a self-\ndiscovery process where LLMs select multiple\natomic reasoning modules such as critical think-\ning and step-by-step thinking, and compose them\ninto an explicit reasoning structure for LLMs to\nfollow during decoding. SELF-DISCOVER sub-\nstantially improves GPT-4 and PaLM 2's per-\nformance on challenging reasoning benchmarks\nsuch as BigBench-Hard, grounded agent reason-\ning, and MATH, by as much as 32% compared\nto Chain of Thought (CoT). Furthermore, SELF-\nDISCOVER outperforms inference-intensive meth-\nods such as CoT-Self-Consistency by more than\n20%, while requiring 10-40x fewer inference com-\npute. Finally, we show that the self-discovered\nreasoning structures are universally applicable\nacross model families: from PaLM 2-L to GPT-4,\nand from GPT-4 to Llama2, and share commonal-\nities with human reasoning patterns.",
    "result": {
      "summary": "SELF-DISCOVER enhances LLMs by enabling them to self-compose reasoning structures, improving performance by up to 32% on complex benchmarks. It outperforms other methods with significantly less compute.",
      "keywords": ["SELF-DISCOVER", "Large Language Models", "reasoning structures", "performance improvement", "complex reasoning"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/2109.00122v3.pdf",
    "title": "FINQA: A Dataset of Numerical Reasoning over Financial Data",
    "abstract": "The sheer volume of financial statements\nmakes it difficult for humans to access and an-\nalyze a business's financials. Robust numeri-\ncal reasoning likewise faces unique challenges\nin this domain. In this work, we focus on\nanswering deep questions over financial data,\naiming to automate the analysis of a large cor-\npus of financial documents. In contrast to ex-\nisting tasks on general domain, the finance do-\nmain includes complex numerical reasoning\nand understanding of heterogeneous represen-\ntations. To facilitate analytical progress, we\npropose a new large-scale dataset, FINQA,\nwith Question-Answering pairs over Financial\nreports, written by financial experts. We also\nannotate the gold reasoning programs to en-\nsure full explainability. We further introduce\nbaselines and conduct comprehensive experi-\nments in our dataset. The results demonstrate\nthat popular, large, pre-trained models fall far\nshort of expert humans in acquiring finance\nknowledge and in complex multi-step numer-\nical reasoning on that knowledge. Our dataset\nthe first of its kind - should therefore en-\nable significant, new community research into\ncomplex application domains. The dataset and\ncode are publicly available1.",
    "result": {
      "summary": "FINQA introduces a dataset for numerical reasoning over financial data, featuring expert-curated Q&A pairs and gold reasoning programs. It highlights gaps in current models' performance on financial reasoning.",
      "keywords": ["FINQA", "financial data", "numerical reasoning", "dataset", "question-answering"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1502.05477v5.pdf",
    "title": "Trust Region Policy Optimization",
    "abstract": "We describe an iterative procedure for optimizing\npolicies, with guaranteed monotonic improve-\nment. By making several approximations to the\ntheoretically-justified procedure, we develop a\npractical algorithm, called Trust Region Policy\nOptimization (TRPO). This algorithm is similar\nto natural policy gradient methods and is effec-\ntive for optimizing large nonlinear policies such\nas neural networks. Our experiments demon-\nstrate its robust performance on a wide variety\nof tasks: learning simulated robotic swimming,\nhopping, and walking gaits; and playing Atari\ngames using images of the screen as input. De-\nspite its approximations that deviate from the\ntheory, TRPO tends to give monotonic improve-\nment, with little tuning of hyperparameters.",
    "result": {
      "summary": "TRPO optimizes policies with guaranteed improvement, effective for nonlinear policies like neural networks. It shows robust performance across tasks including robotics and Atari games.",
      "keywords": ["Trust Region Policy Optimization", "TRPO", "policy optimization", "neural networks", "robotics", "Atari games"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/2309.05653v3.pdf",
    "title": "Preprint. Work in Progress",
    "abstract": "We introduce MAmmo TH, a series of open-source large language models (LLMs)\nspecifically tailored for general math problem-solving. The MAmmoTH models are\ntrained on MathInstruct, our meticulously curated instruction tuning dataset.\nMathInstruct is compiled from 13 math datasets with intermediate rationales,\nsix of which have rationales newly curated by us. It presents a unique hybrid\nof chain-of-thought (CoT) and program-of-thought (PoT) rationales, and also en-\nsures extensive coverage of diverse fields in math. The hybrid of CoT and PoT not\nonly unleashes the potential of tool use but also allows different thought processes\nfor different math problems. As a result, the MAmmo TH series substantially outper-\nform existing open-source models on nine mathematical reasoning datasets across\nall scales with an average accuracy gain between 16% and 32%. Remarkably,\nour MAmmoTH-7B model reaches 33% on MATH (a competition-level dataset),\nwhich exceeds the best open-source 7B model (WizardMath) by 23%, and the\nMAmmoTH-34B model achieves 44% accuracy on MATH, even surpassing GPT-\n4's CoT result. Our work underscores the importance of diverse problem coverage\nand the use of hybrid rationales in developing superior math generalist models.",
    "result": {
      "summary": "MAmmo TH, a series of LLMs for math problem-solving, trained on MathInstruct, excels with a hybrid of CoT and PoT rationales, achieving significant accuracy gains across various datasets.",
      "keywords": ["MAmmo TH", "math problem-solving", "LLMs", "MathInstruct", "hybrid rationales", "accuracy gains"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/0705.2011v1.pdf",
    "title": "Multi-Dimensional Recurrent Neural Networks",
    "abstract": "Recurrent neural networks (RNNs) have proved effective at one dimensional\nsequence learning tasks, such as speech and online handwriting recognition. Some\nof the properties that make RNNs suitable for such tasks, for example robustness\nto input warping, and the ability to access contextual information, are also desir-\nable in multidimensional domains. However, there has SO far been no direct way\nof applying RNNs to data with more than one spatio-temporal dimension. This pa-\nper introduces multi-dimensional recurrent neural networks (MDRNNs), thereby\nextending the potential applicability of RNNs to vision, video processing, medi-\ncal imaging and many other areas, while avoiding the scaling problems that have\nplagued other multi-dimensional models. Experimental results are provided for\ntwo image segmentation tasks.",
    "result": {
      "summary": "MDRNNs extend RNNs to multi-dimensional data, enhancing their application in fields like vision and medical imaging, and showing promising results in image segmentation tasks.",
      "keywords": ["Multi-Dimensional Recurrent Neural Networks", "MDRNNs", "RNNs", "image segmentation", "multi-dimensional data"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/2102.09672v1.pdf",
    "title": "Improved Denoising Diffusion Probabilistic Models",
    "abstract": "Denoising diffusion probabilistic models (DDPM)\nare a class of generative models which have re-\ncently been shown to produce excellent sam-\nples. We show that with a few simple modifi-\ncations, DDPMs can also achieve competitive log-\nlikelihoods while maintaining high sample quality.\nAdditionally, we find that learning variances of\nthe reverse diffusion process allows sampling with\nan order of magnitude fewer forward passes with\na negligible difference in sample quality, which\nis important for the practical deployment of these\nmodels. We additionally use precision and re-\ncall to compare how well DDPMs and GANs\ncover the target distribution. Finally, we show\nthat the sample quality and likelihood of these\nmodels scale smoothly with model capacity and\ntraining compute, making them easily scalable.\nWe release our code at https : / / github · com/",
    "result": {
      "summary": "Simple modifications to Denoising Diffusion Probabilistic Models (DDPMs) enhance their performance, allowing fewer forward passes while maintaining sample quality and demonstrating scalability.",
      "keywords": ["Denoising Diffusion Probabilistic Models", "DDPMs", "generative models", "sample quality", "scalability"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1311.2524v5.pdf",
    "title": "Rich feature hierarchies for accurate object detection and semantic segmentation\nTech report (v5)",
    "abstract": "Object detection performance, as measured on the\ncanonical PASCAL VOC dataset, has plateaued in the last\nfew years. The best-performing methods are complex en-\nsemble systems that typically combine multiple low-level\nimage features with high-level context. In this paper, we\npropose a simple and scalable detection algorithm that im-\nproves mean average precision (mAP) by more than 30%\nrelative to the previous best result on VOC 2012-achieving\na mAP of 53.3%. Our approach combines two key insights:\n(1) one can apply high-capacity convolutional neural net-\nworks (CNNs) to bottom-up region proposals in order to\nlocalize and segment objects and (2) when labeled training\ndata is scarce, supervised pre-training for an auxiliary task,\nfollowed by domain-specific fine-tuning, yields a significant\nperformance boost. Since we combine region proposals\nwith CNNs, we call our method R-CNN: Regions with CNN\nfeatures. We also compare R-CNN to OverFeat, a recently\nproposed sliding-window detector based on a similar CNN\narchitecture. We find that R-CNN outperforms OverFeat\nby a large margin on the 200-class ILSVRC2013 detection\ndataset. Source code for the complete system is available at\nhttp : / / www · CS berkeley . edu/ ~rbg/ rcnn.",
    "result": {
      "summary": "R-CNN combines region proposals with CNNs for object detection, significantly boosting mAP on VOC 2012 by over 30%. It outperforms traditional models and offers a method scalable for limited data.",
      "keywords": ["R-CNN", "object detection", "semantic segmentation", "CNN", "PASCAL VOC", "performance improvement"]
    }
    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1602.01783v2.pdf",
    "title": "Asynchronous Methods for Deep Reinforcement Learning",
    "abstract": "We propose a conceptually simple and\nlightweight framework for deep reinforce-\nment learning that uses asynchronous gradient\ndescent for optimization of deep neural network\ncontrollers. We present asynchronous variants of\nfour standard reinforcement learning algorithms\nand show that parallel actor-learners have a\nstabilizing effect on training allowing all four\nmethods to successfully train neural network\ncontrollers. The best performing method, an\nasynchronous variant of actor-critic, surpasses\nthe current state-of-the-art on the Atari domain\nwhile training for half the time on a single\nmulti-core CPU instead of a GPU. Furthermore,\nwe show that asynchronous actor-critic succeeds\non a wide variety of continuous motor control\nproblems as well as on a new task of navigating\nrandom 3D mazes using a visual input.",
    "result": {
      "summary": "A new deep RL framework using asynchronous gradient descent stabilizes training and improves efficiency, outperforming current models in Atari and extending to continuous control and 3D navigation.",
      "keywords": ["deep reinforcement learning", "asynchronous gradient descent", "actor-critic", "Atari", "continuous control", "3D navigation"]
    }
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1312.6229v4.pdf",
    "title": "OverFeat:\nIntegrated Recognition, Localization and Detection\nusing Convolutional Networks",
    "abstract": "We present an integrated framework for using Convolutional Networks for classi-\nfication, localization and detection. We show how a multiscale and sliding window\napproach can be efficiently implemented within a ConvNet. We also introduce a\nnovel deep learning approach to localization by learning to predict object bound-\naries. Bounding boxes are then accumulated rather than suppressed in order to\nincrease detection confidence. We show that different tasks can be learned simul-\ntaneously using a single shared network. This integrated framework is the winner\nof the localization task of the ImageNet Large Scale Visual Recognition Challenge\n2013 (ILSVRC2013) and obtained very competitive results for the detection and\nclassifications tasks. In post-competition work, we establish a new state of the art\nfor the detection task. Finally, we release a feature extractor from our best model\ncalled OverFeat.",
    "result": {
      "summary": "OverFeat is a ConvNet framework for classification, localization, and detection, achieving top results in ILSVRC2013. It uses a multiscale, sliding window approach and a unique method for increasing detection confidence.",
      "keywords": ["OverFeat", "Convolutional Networks", "classification", "localization", "detection", "ILSVRC2013"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1606.05908v3.pdf",
    "title": "Tutorial on Variational Autoencoders",
    "abstract": "In just three years, Variational Autoencoders (VAEs) have emerged\nas one of the most popular approaches to unsupervised learning of\ncomplicated distributions. VAEs are appealing because they are built\non top of standard function approximators (neural networks), and\ncan be trained with stochastic gradient descent. VAEs have already\nshown promise in generating many kinds of complicated data, in-\ncluding handwritten digits [1, 2], faces [1, 3, 4], house numbers [5, 6],\nCIFAR images [6], physical models of scenes [4], segmentation [7], and\npredicting the future from static images [8]. This tutorial introduces the\nintuitions behind VAEs, explains the mathematics behind them, and\ndescribes some empirical behavior. No prior knowledge of variational\nBayesian methods is assumed.",
    "result": {
      "summary": "Variational Autoencoders (VAEs) offer a popular method for unsupervised learning of complex distributions using neural networks and stochastic gradient descent, effectively generating diverse data types.",
      "keywords": ["Variational Autoencoders", "VAEs", "unsupervised learning", "neural networks", "data generation"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/2302.07577v2.pdf",
    "title": "Efficient Teacher: Semi-Supervised Object Detection for YOLOv5",
    "abstract": "Semi-Supervised Object Detection (SSOD) has been suc-\ncessful in improving the performance of both R-CNN se-\nries and anchor-free detectors. However, one-stage anchor-\nbased detectors lack the structure to generate high-quality\nor flexible pseudo labels, leading to serious inconsistency\nproblems in SSOD. In this paper, we propose the Efficient\nTeacher framework for scalable and effective one-stage\nanchor-based SSOD training, consisting of Dense Detec-\ntor, Pseudo Label Assigner, and Epoch Adaptor. Dense\nDetector is a baseline model that extends RetinaNet with\ndense sampling techniques inspired by YOLOv5. The Ef-\nficient Teacher framework introduces a novel pseudo la-\nbel assignment mechanism, named Pseudo Label Assigner,\nwhich makes more refined use of pseudo labels from Dense\nDetector. Epoch Adaptor is a method that enables a stable\nand efficient end-to-end semi-supervised training schedule\nfor Dense Detector. The Pseudo Label Assigner prevents\nthe occurrence of bias caused by a large number of low-\nquality pseudo labels that may interfere with the Dense De-\ntector during the student-teacher mutual learning mecha-\nnism, and the Epoch Adaptor utilizes domain and distribu-\ntion adaptation to allow Dense Detector to learn globally\ndistributed consistent features, making the training inde-\npendent of the proportion of labeled data. Our experiments\nshow that the Efficient Teacher framework achieves state-\nof-the-art results on VOC, COCO-standard, and COCO-\nadditional using fewer FLOPs than previous methods. To\nthe best of our knowledge, this is the first attempt to apply\nSemi-Supervised Object Detection to YOLOv5.",
    "result": {
      "summary": "Efficient Teacher is a new framework for semi-supervised object detection using YOLOv5, introducing mechanisms for quality pseudo label assignment and stable training, achieving top results on major datasets.",
      "keywords": ["Efficient Teacher", "Semi-Supervised Object Detection", "YOLOv5", "pseudo labels", "anchor-based detectors"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/2305.01937v1.pdf",
    "title": "Can Large Language Models Be an Alternative to Human Evaluation?",
    "abstract": "Human evaluation is indispensable and in-\nevitable for assessing the quality of texts gen-\nerated by machine learning models or writ-\nten by humans. However, human evaluation\nis very difficult to reproduce and its quality\nis notoriously unstable, hindering fair compar-\nisons among different natural language pro-\ncessing (NLP) models and algorithms. Re-\ncently, large language models (LLMs) have\ndemonstrated exceptional performance on un-\nseen tasks when only the task instructions are\nprovided. In this paper, we explore if such an\nability of the LLMs can be used as an alter-\nnative to human evaluation. We present the\nLLMs with the exact same instructions, sam-\nples to be evaluated, and questions used to con-\nduct human evaluation, and then ask the LLMs\nto generate responses to those questions; we\ndub this LLM evaluation. We use human evalu-\nation and LLM evaluation to evaluate the texts\nin two NLP tasks: open-ended story genera-\ntion and adversarial attacks. We show that the\nresult of LLM evaluation is consistent with the\nresults obtained by expert human evaluation:\nthe texts rated higher by human experts are\nalso rated higher by the LLMs. We also find\nthat the results of LLM evaluation are stable\nover different formatting of the task instruc-\ntions and the sampling algorithm used to gen-\nerate the answer. We are the first to show the\npotential of using LLMs to assess the quality\nof texts and discuss the limitations and ethical\nconsiderations of LLM evaluation.",
    "result": {
      "summary": "This paper explores using large language models (LLMs) for evaluating text quality as an alternative to human evaluation, finding LLM assessments align with expert human ratings across NLP tasks.",
      "keywords": ["Large Language Models", "human evaluation", "text quality assessment", "NLP tasks", "LLM evaluation"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/silver14.pdf",
    "title": "Deterministic Policy Gradient Algorithms",
    "abstract": "In this paper we consider deterministic policy\ngradient algorithms for reinforcement learning\nwith continuous actions. The deterministic pol-\nicy gradient has a particularly appealing form: it\nis the expected gradient of the action-value func-\ntion. This simple form means that the deter-\nministic policy gradient can be estimated much\nmore efficiently than the usual stochastic pol-\nicy gradient. To ensure adequate exploration,\nwe introduce an off-policy actor-critic algorithm\nthat learns a deterministic target policy from an\nexploratory behaviour policy. We demonstrate\nthat deterministic policy gradient algorithms can\nsignificantly outperform their stochastic counter-\nparts in high-dimensional action spaces.",
    "result": {
      "summary": "This paper discusses deterministic policy gradient algorithms in reinforcement learning for continuous actions, showing they can outperform stochastic versions in high-dimensional spaces.",
      "keywords": ["deterministic policy gradient", "reinforcement learning", "continuous actions", "high-dimensional action spaces"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1811.08883v1.pdf",
    "title": "Rethinking ImageNet Pre-training",
    "abstract": "We report competitive results on object detection and in-\nstance segmentation on the COCO dataset using standard\nmodels trained from random initialization. The results\nare no worse than their ImageNet pre-training counterparts\neven when using the hyper-parameters of the baseline sys-\ntem (Mask R-CNN) that were optimized for fine-tuning pre-\ntrained models, with the sole exception of increasing the\nnumber of training iterations SO the randomly initialized\nmodels may converge. Training from random initialization\nis surprisingly robust; our results hold even when: (i) us-\ning only 10% of the training data, (ii) for deeper and wider\nmodels, and (iii) for multiple tasks and metrics. Experi-\nments show that ImageNet pre-training speeds up conver-\ngence early in training, but does not necessarily provide\nregularization or improve final target task accuracy. To\npush the envelope we demonstrate 50.9 AP on COCO ob-\nject detection without using any external data-a result on\npar with the top COCO 2017 competition results that used\nImageNet pre-training. These observations challenge the\nconventional wisdom of ImageNet pre-training for depen-\ndent tasks and we expect these discoveries will encourage\npeople to rethink the current de facto paradigm of 'pre-\ntraining and fine-tuning' in computer vision.",
    "result": {
      "summary": "This study challenges the necessity of ImageNet pre-training for improving object detection and segmentation on COCO, showing comparable results with models trained from scratch.",
      "keywords": ["ImageNet pre-training", "random initialization", "object detection", "instance segmentation", "COCO dataset"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/2311.08355v2.pdf",
    "title": "Mustango: Toward Controllable Text-to-Music Generation",
    "abstract": "The quality of the text-to-music models has\nreached new heights due to recent advance-\nments in diffusion models. The controllabil-\nity of various musical aspects, however, has\nbarely been explored. In this paper, we pro-\npose Mustango: a music-domain-knowledge-\ninspired text-to-music system based on dif-\nfusion. Mustango aims to control the gen-\nerated music, not only with general text\ncaptions, but with more rich captions that\ncan include specific instructions related to\nchords, beats, tempo, and key. At the core\nof Mustango is MuNet, a Music-Domain-\nKnowledge-Informed UNet guidance module\nthat steers the generated music to include the\nmusic-specific conditions, which we predict\nfrom the text prompt, as well as the general\ntext embedding, during the reverse diffusion\nprocess. To overcome the limited availability\nof open datasets of music with text captions,\nwe propose a novel data augmentation method\nthat includes altering the harmonic, rhythmic,\nand dynamic aspects of music audio and using\nstate-of-the-art Music Information Retrieval\nmethods to extract the music features which\nwill then be appended to the existing descrip-\ntions in text format. We release the resulting\nMusicBench dataset which contains over 52K\ninstances and includes music-theory -based de-\nscriptions in the caption text. Through exten-\nsive experiments, we show that the quality of\nthe music generated by Mustango is state-of-\nthe-art, and the controllability through music-\nspecific text prompts greatly outperforms other\nmodels such as Musi cGen and AudioLDM2.",
    "result": {
      "summary": "Mustango, a text-to-music system using diffusion models, enhances control over musical aspects via detailed captions for chords, tempo, and more, significantly outperforming others in generating controlled music.",
      "keywords": ["text-to-music generation", "Mustango", "diffusion models", "music controllability", "MuNet"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/2301.12661v1.pdf",
    "title": "Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion\nModels",
    "abstract": "Large-scale multimodal generative modeling has\ncreated milestones in text-to-image and text-to-\nvideo generation. Its application to audio still\nlags behind for two main reasons: the lack of\nlarge-scale datasets with high-quality text-audio\npairs, and the complexity of modeling long con-\ntinuous audio data. In this work, we propose\nMake-An-Audio with a prompt-enhanced diffu-\nsion model that addresses these gaps by 1) in-\ntroducing pseudo prompt enhancement with a\ndistill-then-reprogram approach, it alleviates data\nscarcity with orders of magnitude concept compo-\nsitions by using language-free audios; 2) lever-\naging spectrogram autoencoder to predict the\nself-supervised audio representation instead of\nwaveforms. Together with robust contrastive\nlanguage-audio pretraining (CLAP) representa-\ntions, Make-An-Audio achieves state-of-the-art\nresults in both objective and subjective bench-\nmark evaluation. Moreover, we present its control-\nlability and generalization for X-to-Audio with\n\"No Modality Left Behind\", for the first time\nunlocking the ability to generate high-definition,\nhigh-fidelity audios given a user-defined modality\ninput. Audio samples are available at https :\n/ / Text-to-Audio github io",
    "result": {
      "summary": "Make-An-Audio introduces a prompt-enhanced diffusion model for text-to-audio, using pseudo prompts and a spectrogram autoencoder, achieving top results in audio generation with enhanced control.",
      "keywords": ["text-to-audio generation", "Make-An-Audio", "diffusion models", "audio modeling", "spectrogram autoencoder"]
    }
    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1804.07461v3.pdf",
    "title": "GLUE: A MULTI-TASK BENCHMARK AND ANALYSIS\nPLATFORM FOR NATURAL LANGUAGE UNDERSTAND-\nING",
    "abstract": "For natural language understanding (NLU) technology to be maximally useful, it\nmust be able to process language in a way that is not exclusive to a single task,\ngenre, or dataset. In pursuit of this objective, we introduce the General Language\nUnderstanding Evaluation (GLUE) benchmark, a collection of tools for evaluat-\ning the performance of models across a diverse set of existing NLU tasks. By\nincluding tasks with limited training data, GLUE is designed to favor and encour-\nage models that share general linguistic knowledge across tasks. GLUE also in-\ncludes a hand-crafted diagnostic test suite that enables detailed linguistic analysis\nof models. We evaluate baselines based on current methods for transfer and rep-\nresentation learning and find that multi-task training on all tasks performs better\nthan training a separate model per task. However, the low absolute performance\nof our best model indicates the need for improved general NLU systems.",
    "result": {
      "summary": "GLUE benchmark assesses NLU models across multiple tasks to promote general linguistic knowledge sharing, revealing the need for improved general NLU systems.",
      "keywords": ["GLUE benchmark", "natural language understanding", "NLU tasks", "linguistic knowledge sharing", "model evaluation"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/2211.12588v4.pdf",
    "title": "Program of Thoughts Prompting: Disentangling Computa-\ntion from Reasoning for Numerical Reasoning Tasks",
    "abstract": "Recently, there has been significant progress in teaching language models to perform step-by-\nstep reasoning to solve complex numerical reasoning tasks. Chain-of-thoughts prompting\n(CoT) is the state-of-art method for many of these tasks. CoT uses language models to\nproduce text describing reasoning, and computation, and finally the answer to a question.\nHere we propose 'Program of Thoughts' (PoT), which uses language models (mainly Codex)\nto generate text and programming language statements, and finally an answer. In PoT,\nthe computation can be delegated to a program interpreter, which is used to execute the\ngenerated program, thus decoupling complex computation from reasoning and language\nunderstanding. We evaluate PoT on five math word problem datasets and three financial-\nQA datasets in both few-shot and zero-shot settings. We find that PoT has an average\nperformance gain over CoT of around 12% across all datasets. By combining PoT with\nself-consistency decoding, we can achieve extremely strong performance on all the math\ndatasets and financial datasets. All of our data and code will be released.",
    "result": {
      "summary": "Program of Thoughts (PoT) enhances numerical reasoning in language models by separating computation and reasoning, showing a 12% performance gain over CoT across multiple datasets.",
      "keywords": ["Program of Thoughts", "PoT", "numerical reasoning", "Chain-of-thoughts", "language models", "performance gain"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/2309.03409v3.pdf",
    "title": "LARGE LANGUAGE MODELS AS OPTIMIZERS",
    "abstract": "Optimization is ubiquitous. While derivative-based algorithms have been powerful\ntools for various problems, the absence of gradient imposes challenges on many\nreal-world applications. In this work, we propose Optimization by PROmpting\n(OPRO), a simple and effective approach to leverage large language models (LLMs)\nas optimizers, where the optimization task is described in natural language. In\neach optimization step, the LLM generates new solutions from the prompt that\ncontains previously generated solutions with their values, then the new solutions are\nevaluated and added to the prompt for the next optimization step. We first showcase\nOPRO on linear regression and traveling salesman problems, then move on to our\nmain application in prompt optimization, where the goal is to find instructions\nthat maximize the task accuracy. With a variety of LLMs, we demonstrate that\nthe best prompts optimized by OPRO outperform human-designed prompts by\nup to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks. Code at",
    "result": {
      "summary": "OPRO leverages LLMs as optimizers for tasks described in natural language, improving solution quality iteratively. It notably enhances prompt optimization, outperforming human-designed prompts.",
      "keywords": ["OPRO", "large language models", "optimization", "prompt optimization", "natural language"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1506.02640v5.pdf",
    "title": "You Only Look Once:\nUnified, Real-Time Object Detection",
    "abstract": "We present YOLO, a new approach to object detection.\nPrior work on object detection repurposes classifiers to per-\nform detection. Instead, we frame object detection as a re-\ngression problem to spatially separated bounding boxes and\nassociated class probabilities. A single neural network pre-\ndicts bounding boxes and class probabilities directly from\nfull images in one evaluation. Since the whole detection\npipeline is a single network, it can be optimized end-to-end\ndirectly on detection performance.",
    "result": {
      "summary": "YOLO introduces a novel approach to object detection by treating it as a regression problem, using a single neural network to predict bounding boxes and class probabilities in real-time.",
      "keywords": ["YOLO", "object detection", "real-time", "neural network", "regression"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1507.01526v3.pdf",
    "title": "GRID LONG SHORT-TERM MEMORY",
    "abstract": "This paper introduces Grid Long Short-Term Memory, a network of LSTM cells\narranged in a multidimensional grid that can be applied to vectors, sequences or\nhigher dimensional data such as images. The network differs from existing deep\nLSTM architectures in that the cells are connected between network layers as\nwell as along the spatiotemporal dimensions of the data. The network provides\na unified way of using LSTM for both deep and sequential computation. We ap-\nply the model to algorithmic tasks such as 15-digit integer addition and sequence\nmemorization, where itis able to significantly outperform the standard LSTM. We\nthen give results for two empirical tasks. We find that 2D Grid LSTM achieves\n1.47 bits per character on the Wikipedia character prediction benchmark, which is\nstate-of-the-art among neural approaches. In addition, we use the Grid LSTM to\ndefine a novel two-dimensional translation model, the Reencoder, and show that it\noutperforms a phrase-based reference system on a Chinese-to-English translation\ntask.",
    "result": {
      "summary": "Grid LSTM introduces a novel LSTM architecture for handling vectors, sequences, and higher-dimensional data, excelling in algorithmic and empirical tasks with state-of-the-art results in character prediction and translation.",
      "keywords": ["Grid LSTM", "multidimensional data", "deep learning", "sequence computation", "translation model"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1509.02971v6.pdf",
    "title": "CONTINUOUS CONTROL WITH DEEP REINFORCEMENT\nLEARNING",
    "abstract": "We adapt the ideas underlying the success of Deep Q-Learning to the continuous\naction domain. We present an actor-critic, model-free algorithm based on the de-\nterministic policy gradient that can operate over continuous action spaces. Using\nthe same learning algorithm, network architecture and hyper-parameters, our al-\ngorithm robustly solves more than 20 simulated physics tasks, including classic\nproblems such as cartpole swing-up, dexterous manipulation, legged locomotion\nand car driving. Our algorithm is able to find policies whose performance is com-\npetitive with those found by a planning algorithm with full access to the dynamics\nof the domain and its derivatives. We further demonstrate that for many of the\ntasks the algorithm can learn policies \"end-to-end\": directly from raw pixel in-\nputs.",
    "result": {
      "summary": "This paper adapts Deep Q-Learning to continuous actions using an actor-critic algorithm, achieving robust solutions for over 20 physics-based tasks, and learning directly from raw pixels.",
      "keywords": ["continuous control", "deep reinforcement learning", "actor-critic", "policy gradient", "physics tasks"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf",
    "title": "ImageNet Classification with Deep Convolutional\nNeural Networks",
    "abstract": "We trained a large, deep convolutional neural network to classify the 1.2 million\nhigh-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif-\nferent classes. On the test data, we achieved top-1 and top-5 error rates of 37.5%\nand 17.0% which is considerably better than the previous state-of-the-art. The\nneural network, which has 60 million parameters and 650,000 neurons, consists\nof five convolutional layers, some of which are followed by max-pooling layers,\nand three fully-connected layers with a final 1000-way softmax. To make train-\ning faster, we used non-saturating neurons and a very efficient GPU implemen-\ntation of the convolution operation. To reduce overfitting in the fully-connected\nlayers we employed a recently-developed regularization method called \"dropout\"\nthat proved to be very effective. We also entered a variant of this model in the\nILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%,\ncompared to 26.2% achieved by the second-best entry.",
    "result": {
      "summary": "A deep convolutional neural network with 60 million parameters classifies ImageNet images into 1000 classes, achieving top error rates significantly better than prior methods, aided by dropout and efficient GPU use.",
      "keywords": ["deep convolutional neural network", "ImageNet classification", "dropout", "GPU efficiency", "error rates"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1705.03551v2.pdf",
    "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset\nfor Reading Comprehension",
    "abstract": "We present TriviaQA, a challenging\nreading comprehension dataset contain-\ning over 650K question-answer-evidence\ntriples. TriviaQA includes 95K question-\nanswer pairs authored by trivia enthusi-\nasts and independently gathered evidence\ndocuments, S1X per question on average,\nthat provide high quality distant super-\nvision for answering the questions. We\nshow that, in comparison to other recently\nintroduced large-scale datasets, TriviaQA\n(1) has relatively complex, compositional\nquestions, (2) has considerable syntactic\nand lexical variability between questions\nand corresponding answer-evidence sen-\ntences, and (3) requires more cross sen-\ntence reasoning to find answers. We also\npresent two baseline algorithms: a feature-\nbased classifier and a state-of-the-art neu-\nral network, that performs well on SQuAD\nreading comprehension. Neither approach\ncomes close to human performance (23%\nand 40% VS. 80%), suggesting that Trivi-\naQA is a challenging testbed that is worth\nsignificant future study. 1",
    "result": {
      "summary": "TriviaQA is a large-scale reading comprehension dataset with 650K+ QA pairs requiring complex reasoning. It poses a significant challenge as baseline models fall short compared to human performance.",
      "keywords": ["TriviaQA", "reading comprehension", "dataset", "question-answer pairs", "distant supervision"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/2105.05233v4.pdf",
    "title": "Diffusion Models Beat GANs on Image Synthesis",
    "abstract": "We show that diffusion models can achieve image sample quality superior to the\ncurrent state-of-the-art generative models. We achieve this on unconditional im-\nage synthesis by finding a better architecture through a series of ablations. For\nconditional image synthesis, we further improve sample quality with classifier guid-\nance: a simple, compute-efficient method for trading off diversity for fidelity using\ngradients from a classifier. We achieve an FID of 2.97 on ImageNet 128 x 128,\n4.59 on ImageNet 256x256, and 7.72 on ImageNet 512x512, and we match\nBigGAN-deep even with as few as 25 forward passes per sample, all while main-\ntaining better coverage of the distribution. Finally, we find that classifier guidance\ncombines well with upsampling diffusion models, further improving FID to 3.94\non ImageNet 256x256 and 3.85 on ImageNet 512x512. We release our code at\nhttps : //github · com/ openai / guided-diffusion.",
    "result": {
      "summary": "Diffusion models surpass GANs in image synthesis quality through architecture improvements and classifier guidance, achieving top FID scores on ImageNet across various resolutions.",
      "keywords": ["diffusion models", "image synthesis", "GANs", "classifier guidance", "FID scores", "ImageNet"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1707.06347v2.pdf",
    "title": "Proximal Policy Optimization Algorithms",
    "abstract": "We propose a new family of policy gradient methods for reinforcement learning, which al-\nternate between sampling data through interaction with the environment, and optimizing a\n\"surrogate\" objective function using stochastic gradient ascent. Whereas standard policy gra-\ndient methods perform one gradient update per data sample, we propose a novel objective\nfunction that enables multiple epochs of minibatch updates. The new methods, which we call\nproximal policy optimization (PPO), have some of the benefits of trust region policy optimiza-\ntion (TRPO), but they are much simpler to implement, more general, and have better sample\ncomplexity (empirically). Our experiments test PPO on a collection of benchmark tasks, includ-\ning simulated robotic locomotion and Atari game playing, and we show that PPO outperforms\nother online policy gradient methods, and overall strikes a favorable balance between sample\ncomplexity, simplicity, and wall-time.",
    "result": {
      "summary": "Proximal Policy Optimization (PPO) introduces a new family of policy gradient methods that optimize a surrogate objective, allowing multiple updates per sample and achieving better performance on benchmarks.",
      "keywords": ["Proximal Policy Optimization", "PPO", "policy gradient methods", "reinforcement learning", "sample complexity"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1504.08083v2.pdf",
    "title": "Fast R-CNN",
    "abstract": "This paper proposes a Fast Region-based Convolutional\nNetwork method (Fast R-CNN) for object detection. Fast\nR-CNN builds on previous work to efficiently classify ob-\nject proposals using deep convolutional networks. Com-\npared to previous work, Fast R-CNN employs several in-\nnovations to improve training and testing speed while also\nincreasing detection accuracy. Fast R-CNN trains the very\ndeep VGG16 network 9x faster than R-CNN, is 213x faster\nat test-time, and achieves a higher mAP on PASCAL VOC\n2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x\nfaster, tests 10x faster, and is more accurate. Fast R-CNN\nis implemented in Python and C++ (using Caffe) and is\navailable under the open-source MIT License at https ··",
    "result": {
      "summary": "Fast R-CNN improves upon previous object detection methods, offering faster training and testing speeds with higher accuracy, significantly outperforming R-CNN and SPPnet on benchmarks.",
      "keywords": ["Fast R-CNN", "object detection", "deep convolutional networks", "speed improvement", "accuracy enhancement"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1506.01497v3.pdf",
    "title": "Faster R-CNN: Towards Real-Time Object\nDetection with Region Proposal Networks",
    "abstract": "Abstract-State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations.\nAdvances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region\nproposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image\nconvolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional\nnetwork that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to\ngenerate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN\ninto a single network by sharing their convolutional features-using the recently popular terminology of neural networks with\n\"attention\" mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3],\nour detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection\naccuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO\n2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been\nmade publicly available.",
    "result": {
      "summary": "Faster R-CNN integrates a Region Proposal Network (RPN) with Fast R-CNN, significantly speeding up object detection by generating efficient region proposals and sharing convolutional features.",
      "keywords": ["Faster R-CNN", "Region Proposal Network", "object detection", "real-time", "RPN", "Fast R-CNN"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1606.05250v3.pdf",
    "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text",
    "abstract": "We present the Stanford Question Answer-\ning Dataset (SQuAD), a new reading compre-\nhension dataset consisting of 100,000+ ques-\ntions posed by crowdworkers on a set of\nWikipedia articles, where the answer to each\nquestion is a segment of text from the cor-\nresponding reading passage. We analyze the\ndataset to understand the types of reason-\ning required to answer the questions, lean-\ning heavily on dependency and constituency\ntrees. We build a strong logistic regression\nmodel, which achieves an F1 score of 51.0%,\na significant improvement over a simple base-\nline (20%). However, human performance\n(86.8%) is much higher, indicating that the\ndataset presents a good challenge problem for\nfuture research. The dataset is freely available\nat https: / / stanford-qa.com.",
    "result": {
      "summary": "SQuAD is a large-scale reading comprehension dataset from Stanford with 100,000+ questions on Wikipedia articles, presenting a significant challenge for AI with human performance far exceeding current models.",
      "keywords": ["SQuAD", "machine comprehension", "reading comprehension dataset", "Stanford", "Wikipedia"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1511.06581v3.pdf",
    "title": "Dueling Network Architectures for Deep Reinforcement Learning",
    "abstract": "In recent years there have been many successes\nof using deep representations in reinforcement\nlearning. Still, many of these applications use\nconventional architectures, such as convolutional\nnetworks, LSTMs, or auto-encoders. In this pa-\nper, we present a new neural network architec-\nture for model-free reinforcement learning. Our\ndueling network represents two separate estima-\ntors: one for the state value function and one for\nthe state-dependent action advantage function.\nThe main benefit of this factoring is to general-\nize learning across actions without imposing any\nchange to the underlying reinforcement learning\nalgorithm. Our results show that this architec-\nture leads to better policy evaluation in the pres-\nence of many similar-valued actions. Moreover,\nthe dueling architecture enables our RL agent to\noutperform the state-of-the-art on the Atari 2600\ndomain.",
    "result": {
      "summary": "Dueling network architecture introduces two separate estimators for state values and action advantages in reinforcement learning, improving policy evaluation and outperforming current models in Atari 2600 games.",
      "keywords": ["dueling network", "deep reinforcement learning", "architecture", "policy evaluation", "Atari 2600"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1511.06391v4.pdf",
    "title": "ORDER MATTERS: SEQUENCE TO SEQUENCE FOR SETS",
    "abstract": "Sequences have become first class citizens in supervised learning thanks to the\nresurgence of recurrent neural networks. Many complex tasks that require map-\nping from or to a sequence of observations can now be formulated with the\nsequence-to-sequence (seq2seq) framework which employs the chain rule to ef-\nficiently represent the joint probability of sequences. In many cases, however,\nvariable sized inputs and/or outputs might not be naturally expressed as sequences.\nFor instance, it is not clear how to input a set of numbers into a model where the\ntask is to sort them; similarly, we do not know how to organize outputs when\nthey correspond to random variables and the task is to model their unknown joint\nprobability. In this paper, we first show using various examples that the order in\nwhich we organize input and/or output data matters significantly when learning an\nunderlying model. We then discuss an extension of the seq2seq framework that\ngoes beyond sequences and handles input sets in a principled way. In addition,\nwe propose a loss which, by searching over possible orders during training, deals\nwith the lack of structure of output sets. We show empirical evidence of our claims\nregarding ordering, and on the modifications to the seq2seq framework on bench-\nmark language modeling and parsing tasks, as well as two artificial tasks - sorting\nnumbers and estimating the joint probability of unknown graphical models.",
    "result": {
      "summary": "This paper extends the sequence-to-sequence framework to handle sets, addressing challenges with variable sized inputs and outputs not naturally expressed as sequences, and introduces a loss to manage output set structures.",
      "keywords": ["sequence-to-sequence", "sets", "seq2seq framework", "variable sized inputs", "output structure"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1506.04834v3.pdf",
    "title": "Tree-Structured Composition in Neural Networks\nwithout Tree-Structured Architectures",
    "abstract": "Tree-structured neural networks encode a particular tree geometry for a sentence\nin the network design. However, these models have at best only slightly out-\nperformed simpler sequence-based models. We hypothesize that neural sequence\nmodels like LSTMs are in fact able to discover and implicitly use recursive com-\npositional structure, at least for tasks with clear cues to that structure in the data.\nWe demonstrate this possibility using an artificial data task for which recursive\ncompositional structure is crucial, and find an LSTM-based sequence model can\nindeed learn to exploit the underlying tree structure. However, its performance\nconsistently lags behind that of tree models, even on large training sets, suggest-\ning that tree-structured models are more effective at exploiting recursive structure.",
    "result": {
      "summary": "This study explores how neural sequence models like LSTMs can implicitly learn recursive compositional structures, though they still perform less effectively than dedicated tree-structured models.",
      "keywords": ["neural networks", "tree-structured", "LSTMs", "recursive composition", "sequence models"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1502.02367v4.pdf",
    "title": "Gated Feedback Recurrent Neural Networks",
    "abstract": "In this work, we propose a novel recurrent neu-\nral network (RNN) architecture. The proposed\nRNN, gated-feedback RNN (GF-RNN), extends\nthe existing approach of stacking multiple recur-\nrent layers by allowing and controlling signals\nflowing from upper recurrent layers to lower lay-\ners using a global gating unit for each pair of\nlayers. The recurrent signals exchanged between\nlayers are gated adaptively based on the previous\nhidden states and the current input. We evalu-\nated the proposed GF-RNN with different types\nof recurrent units, such as tanh, long short-term\nmemory and gated recurrent units, on the tasks\nof character-level language modeling and Python\nprogram evaluation. Our empirical evaluation of\ndifferent RNN units, revealed that in both tasks,\nthe GF-RNN outperforms the conventional ap-\nproaches to build deep stacked RNNs. We sug-\ngest that the improvement arises because the GF-\nRNN can adaptively assign different layers to dif-\nferent timescales and layer-to-layer interactions\n(including the top-down ones which are not usu-\nally present in a stacked RNN) by learning to gate\nthese interactions.",
    "result": {
      "summary": "Gated-feedback RNN (GF-RNN) introduces a novel architecture that controls signal flow between recurrent layers, improving performance on language modeling and code evaluation by adaptively gating layer interactions.",
      "keywords": ["GF-RNN", "recurrent neural network", "gated feedback", "language modeling", "Python code evaluation"]
    }
  }, {
    "pdf_path": "./pdf/AI_VIT_X/BF00992698.pdf",
    "title": "Q-Learning",
    "abstract": "Abstract. Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian\ndomains. It amounts to an incremental method for dynamic programming which imposes limited computational\ndemands. It works by successively improving its evaluations of the quality of particular actions at particular states. This paper presents and proves in detail a convergence theorem for Q-learning based on that outlined in Watkins\n(1989). We show that Q-learning converges to the optimum action-values with probability 1 so long as all actions\nare repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions\nto the cases of non-discounted, but absorbing, Markov environments, and where many Q values can be changed\neach iteration, rather than just one.",
    "result": {
      "summary": "Q-learning is a method where agents learn optimal actions in Markovian domains. This paper proves Q-learning's convergence to optimal action-values under certain conditions, and discusses its extensions.",
      "keywords": ["Q-learning", "Markovian domains", "optimal action-values", "convergence", "dynamic programming"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1409.4842v1.pdf",
    "title": "Going deeper with convolutions",
    "abstract": "We propose a deep convolutional neural network architecture codenamed Incep-\ntion, which was responsible for setting the new state of the art for classification\nand detection in the ImageNet Large-Scale Visual Recognition Challenge 2014\n(ILSVRC14). The main hallmark of this architecture is the improved utilization\nof the computing resources inside the network. This was achieved by a carefully\ncrafted design that allows for increasing the depth and width of the network while\nkeeping the computational budget constant. To optimize quality, the architectural\ndecisions were based on the Hebbian principle and the intuition of multi-scale\nprocessing. One particular incarnation used in our submission for ILSVRC14 is\ncalled GoogLeNet, a 22 layers deep network, the quality of which is assessed in\nthe context of classification and detection.",
    "result": {
      "summary": "Inception, a deep convolutional neural network, sets a new standard in the ImageNet challenge by optimizing resource use, increasing network depth and width without raising computational costs, based on Hebbian principles and multi-scale processing.",
      "keywords": ["Inception", "GoogLeNet", "deep convolutional neural network", "ImageNet", "network architecture"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1711.09020v3.pdf",
    "title": "StarGAN: Unified Generative Adversarial Networks\nfor Multi-Domain Image-to-Image Translation",
    "abstract": "Recent studies have shown remarkable success in image-\nto-image translation for two domains. However, existing\napproaches have limited scalability and robustness in han-\ndling more than two domains, since different models should\nbe built independently for every pair of image domains. To\naddress this limitation, we propose StarGAN, a novel and\nscalable approach that can perform image-to-image trans-\nlations for multiple domains using only a single model.\nSuch a unified model architecture ofStarGAN allows simul-\ntaneous training of multiple datasets with different domains\nwithin a single network. This leads to StarGAN's superior\nquality of translated images compared to existing models as\nwell as the novel capability offlexibly translating an input\nimage to any desired target domain. We empirically demon-\nstrate the effectiveness of our approach on a facial attribute\ntransfer and a facial expression synthesis tasks.",
    "result": {
      "summary": "StarGAN introduces a scalable unified GAN architecture for multi-domain image-to-image translation, efficiently handling multiple domains with a single model and improving image quality across diverse tasks.",
      "keywords": ["StarGAN", "image-to-image translation", "GAN", "multi-domain", "unified model architecture"]
    }
    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1503.00075v3.pdf",
    "title": "Improved Semantic Representations From\nTree-Structured Long Short- Term Memory Networks",
    "abstract": "Because of their superior ability to pre-\nserve sequence information over time,\nLong Short-Term Memory (LSTM) net-\nworks, a type of recurrent neural net-\nwork with a more complex computational\nunit, have obtained strong results on a va-\nriety of sequence modeling tasks. The\nonly underlying LSTM structure that has\nbeen explored SO far is a linear chain.\nHowever, natural language exhibits syn-\ntactic properties that would naturally com-\nbine words to phrases. We introduce the\nTree-LSTM, a generalization of LSTMs to\ntree-structured network topologies. Tree-\nLSTMs outperform all existing systems\nand strong LSTM baselines on two tasks:\npredicting the semantic relatedness of two\nsentences (SemEval 2014, Task 1) and\nsentiment classification (Stanford Senti-\nment Treebank).",
    "result": {
      "summary": "Tree-LSTM extends traditional LSTMs to tree-structured networks, better capturing language's syntactic properties, resulting in superior performance on semantic relatedness and sentiment classification tasks.",
      "keywords": ["Tree-LSTM", "semantic representations", "LSTM networks", "syntactic properties", "sequence modeling"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/glove.pdf",
    "title": "Glo Ve: Global Vectors for Word Representation",
    "abstract": "Recent methods for learning vector space\nrepresentations of words have succeeded\nin capturing fine-grained semantic and\nsyntactic regularities using vector arith-\nmetic, but the origin of these regularities\nhas remained opaque. We analyze and\nmake explicit the model properties needed\nfor such regularities to emerge in word\nvectors. The result is a new global log-\nbilinear regression model that combines\nthe advantages of the two major model\nfamilies in the literature: global matrix\nfactorization and local context window\nmethods. Our model efficiently leverages\nstatistical information by training only on\nthe nonzero elements in a word-word CO-\noccurrence matrix, rather than on the en-\ntire sparse matrix or on individual context\nwindows in a large corpus. The model pro-\nduces a vector space with meaningful sub-\nstructure, as evidenced by its performance\nof 75% on a recent word analogy task. It\nalso outperforms related models on simi-\nlarity tasks and named entity recognition.",
    "result": {
      "summary": "GloVe introduces a global log-bilinear regression model that merges matrix factorization and context window approaches to produce word vectors, capturing semantic regularities and outperforming on various NLP tasks.",
      "keywords": ["GloVe", "word representation", "vector space", "semantic regularities", "log-bilinear regression"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/2302.04166v2.pdf",
    "title": "GPTScore: Evaluate as You Desire",
    "abstract": "Generative Artificial Intelligence (AI) has enabled\nthe development of sophisticated models that are\ncapable of producing high-caliber text, images,\nand other outputs through the utilization of large\npre-trained models. Nevertheless, assessing the\nquality of the generation is an even more ardu-\nous task than the generation itself, and this is-\nsue has not been given adequate consideration\nrecently. This paper proposes a novel evalua-\ntion framework, GPTSCORE, which utilizes the\nemergent abilities (e.g., zero-shot instruction) of\ngenerative pre-trained models to score generated\ntexts. There are 19 pre-trained models explored\nin this paper, ranging in size from 80M (e.g.,\nFLAN-T5-small) to 175B (e.g., GPT3). Exper-\nimental results on four text generation tasks, 22\nevaluation aspects, and corresponding 37 datasets\ndemonstrate that this approach can effectively\nallow us to achieve what one desires to evalu-\nate for texts simply by natural language instruc-\ntions. This nature helps us overcome several\nlong-standing challenges in text evaluation-how\nto achieve customized, multi-faceted evaluation\nwithout the need for annotated samples. We make\nour code publicly available. 1",
    "result": {
      "summary": "GPTScore introduces an evaluation framework for generative AI outputs using pre-trained models to score texts across multiple dimensions, effectively addressing the challenge of custom, multifaceted text evaluation without annotated samples.",
      "keywords": ["GPTScore", "generative AI", "evaluation framework", "pre-trained models", "text generation"]
    }
  }, {
    "pdf_path": "./pdf/AI_VIT_X/2109.06835v1.pdf",
    "title": "The Perils of Using Mechanical Turk\nto Evaluate Open-Ended Text Generation",
    "abstract": "Recent text generation research has increas-\ningly focused on open-ended domains such as\nstory and poetry generation. Because mod-\nels built for such tasks are difficult to evaluate\nautomatically, most researchers in the space\njustify their modeling choices by collecting\ncrowdsourced human judgments of text qual-\nity (e.g., Likert scores of coherence or gram-\nmaticality) from Amazon Mechanical Turk\n(AMT). In this paper, we first conduct a sur-\nvey of 45 open-ended text generation papers\nand find that the vast majority of them fail to\nreport crucial details about their AMT tasks,\nhindering reproducibility. We then run a se-\nries of story evaluation experiments with both\nAMT workers and English teachers and dis-\ncover that even with strict qualification fil-\nters, AMT workers (unlike teachers) fail to\ndistinguish between model-generated text and\nhuman-generated references. We show that\nAMT worker judgments improve when they\nare shown model-generated output alongside\nhuman-generated references, which enables\nthe workers to better calibrate their ratings.\nFinally, interviews with the English teachers\nprovide deeper insights into the challenges of\nthe evaluation process, particularly when rat-\ning model-generated text.",
    "result": {
      "summary": "This study critiques the use of Amazon Mechanical Turk for evaluating open-ended text generation, highlighting issues with reproducibility and validity. Experiments suggest AMT workers struggle to differentiate between human and model-generated texts without comparative examples.",
      "keywords": ["Mechanical Turk", "text generation evaluation", "open-ended domains", "reproducibility", "AMT", "human judgments"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/2204.08292v1.pdf",
    "title": "StepGame: A New Benchmark for\nRobust Multi-Hop Spatial Reasoning in Texts",
    "abstract": "Inferring spatial relations in natural language is a crucial abil-\nity an intelligent system should possess. The bAbI dataset\ntries to capture tasks relevant to this domain (task 17 and 19).\nHowever, these tasks have several limitations. Most impor-\ntantly, they are limited to fixed expressions, they are limited\nin the number of reasoning steps required to solve them, and\nthey fail to test the robustness of models to input that contains\nirrelevant or redundant information. In this paper, we present\na new Question-Answering dataset called StepGame for ro-\nbust multi-hop spatial reasoning in texts. Our experiments\ndemonstrate that state-of-the-art models on the bAbI dataset\nstruggle on the StepGame dataset. Moreover, we propose a\nTensor-Product based Memory-Augmented Neural Network\n(TP-MANN) specialized for spatial reasoning tasks. Experi-\nmental results on both datasets show that our model outper-\nforms all the baselines with superior generalization and ro-\nbustness performance.",
    "result": {
      "summary": "StepGame introduces a dataset enhancing multi-hop spatial reasoning in texts, addressing limitations of the bAbI dataset by including diverse expressions and robustness tests. TP-MANN, a new model, shows superior performance and generalization on this dataset.",
      "keywords": ["StepGame", "spatial reasoning", "multi-hop reasoning", "bAbI dataset", "Tensor-Product Memory-Augmented Neural Network", "TP-MANN", "dataset", "neural network"]
    }
    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/nature14236.pdf",
    "title": "Human-level control through deep reinforcement\nlearning",
    "abstract": "The theory ofreinforcement learning provides a normative account1,\ndeeply rooted in psychological2 and neuroscientific3 perspectives on\nanimal behaviour, of how agents may optimize their control of an\nenvironment. To use reinforcement learning successfully in situations\napproaching real-world complexity, however, agents are confronted\nwith a difficult task: they must derive efficient representations ofthe\nenvironment from high-dimensional sensory inputs, and use these\nto generalize past experience to new situations. Remarkably, humans\nand other animals seem to solve this problem through a harmonious\ncombination ofreinforcement learning and hierarchical sensory pro-\ncessing systems4,5, the former evidenced by a wealth of neural data\nrevealing notable parallels between the phasic signals emitted by dopa-\nminergic neurons and temporal difference reinforcement learning\nalgorithms3. While reinforcement learning agents have achieved some\nsuccesses in a variety of domains6-8, their applicability has previously\nbeen limited to domains in which useful features can be handcrafted,\nor to domains with fully observed, low-dimensional state spaces.\nHere we use recent advances in training deep neural networks9-11 to\ndevelop a novel artificial agent, termed a deep Q-network, that can\nlearn successful policies directly from high-dimensional sensory inputs\nusing end-to-end reinforcement learning. We tested this agent on\nthe challenging domain of classic Atari 2600 games12. We demon-\nstrate that the deep Q-network agent, receiving only the pixels and\nthe game score as inputs, was able to surpass the performance of all\nprevious algorithms and achieve a level comparable to that of a pro-\nfessional human games tester across a set of 49 games, using the same\nalgorithm, network architecture and hyperparameters. This work\nbridges the divide between high-dimensional sensory inputs and\nactions, resulting in the first artificial agent that is capable oflearn-\ning to excel at a diverse array of challenging tasks.",
    "result": {
      "summary": "The study presents a deep Q-network agent capable of human-level control in Atari 2600 games, learning directly from high-dimensional sensory inputs through deep reinforcement learning, surpassing prior algorithms and demonstrating broad applicability.",
      "keywords": ["deep reinforcement learning", "human-level control", "deep Q-network", "high-dimensional sensory inputs", "Atari 2600", "neural networks", "temporal difference learning"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1409.0473v7.pdf",
    "title": "NEURAL MACHINE TRANSLATION\nBY JOINTLY LEARNING TO ALIGN AND TRANSLATE",
    "abstract": "Neural machine translation is a recently proposed approach to machine transla-\ntion. Unlike the traditional statistical machine translation, the neural machine\ntranslation aims at building a single neural network that can be jointly tuned to\nmaximize the translation performance. The models proposed recently for neu-\nral machine translation often belong to a family of encoder-decoders and encode\na source sentence into a fixed-length vector from which a decoder generates a\ntranslation. In this paper, we conjecture that the use of a fixed-length vector is a\nbottleneck in improving the performance of this basic encoder-decoder architec-\nture, and propose to extend this by allowing a model to automatically (soft-)search\nfor parts of a source sentence that are relevant to predicting a target word, without\nhaving to form these parts as a hard segment explicitly. With this new approach,\nwe achieve a translation performance comparable to the existing state-of-the-art\nphrase-based system on the task of English-to-French translation. Furthermore,\nqualitative analysis reveals that the (soft-)alignments found by the model agree\nwell with our intuition.",
    "result": {
      "summary": "This paper improves neural machine translation by replacing fixed-length vectors with a model that dynamically aligns and translates, enhancing performance to match state-of-the-art phrase-based systems in English-to-French translation.",
      "keywords": ["neural machine translation", "encoder-decoder", "dynamic alignment", "translation performance", "English-to-French translation"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1810.04805v2.pdf",
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding",
    "abstract": "We introduce a new language representa-\ntion model called BERT, which stands for\nBidirectional Encoder Representations from\nTransformers. Unlike recent language repre-\nsentation models (Peters et al., 2018a; Rad-\nford et al., 2018), BERT is designed to pre-\ntrain deep bidirectional representations from\nunlabeled text by jointly conditioning on both\nleft and right context in all layers. As a re-\nsult, the pre-trained BERT model can be fine-\ntuned with just one additional output layer\nto create state-of-the-art models for a wide\nrange of tasks, such as question answering and\nlanguage inference, without substantial task-\nspecific architecture modifications.",
    "result": {
      "summary": "BERT introduces a language model using deep bidirectional transformers, pre-trained on unlabeled text to understand context from both sides, enhancing performance on tasks like question answering and language inference with minimal task-specific modifications.",
      "keywords": ["BERT", "language representation", "bidirectional transformers", "pre-training", "language understanding", "question answering", "language inference"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/2303.16634v3.pdf",
    "title": "G-EVAL: NLG Evaluation using GPT-4 with Better Human Alignment",
    "abstract": "The quality of texts generated by natural lan-\nguage generation (NLG) systems is hard to\nmeasure automatically. Conventional reference-\nbased metrics, such as BLEU and ROUGE,\nhave been shown to have relatively low cor-\nrelation with human judgments, especially for\ntasks that require creativity and diversity. Re-\ncent studies suggest using large language mod-\nels (LLMs) as reference-free metrics for NLG\nevaluation, which have the benefit of being ap-\nplicable to new tasks that lack human refer-\nences. However, these LLM-based evaluators\nstill have lower human correspondence than\nmedium-size neural evaluators. In this work,\nwe present G-EVAL, a framework of using\nlarge language models with chain-of-thoughts\n(CoT) and a form-filling paradigm, to assess the\nquality of NLG outputs. We experiment with\ntwo generation tasks, text summarization and\ndialogue generation. We show that G-EVAL\nwith GPT-4 as the backbone model achieves a\nSpearman correlation of 0.514 with human on\nsummarization task, outperforming all previous\nmethods by a large margin. We also propose\nanalysis on the behavior of LLM-based eval-\nuators, and highlight the potential concern of\nLLM-based evaluators having a bias towards\nthe LLM-generated texts. 1",
    "result": {
      "summary": "G-EVAL leverages GPT-4 and a chain-of-thought approach to evaluate NLG systems, showing improved human alignment in text summarization and dialogue generation tasks, and addressing biases in LLM-based evaluators.",
      "keywords": ["G-EVAL", "NLG evaluation", "GPT-4", "chain-of-thought", "text summarization", "dialogue generation", "language model evaluators", "human alignment"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/2010.02502v4.pdf",
    "title": "DENOISING DIFFUSION IMPLICIT MODELS",
    "abstract": "Denoising diffusion probabilistic models (DDPMs) have achieved high qual-\nity image generation without adversarial training, yet they require simulating a\nMarkov chain for many steps in order to produce a sample. To accelerate sam-\npling, we present denoising diffusion implicit models (DDIMs), a more efficient\nclass of iterative implicit probabilistic models with the same training procedure as\nDDPMs. In DDPMs, the generative process is defined as the reverse of a particular\nMarkovian diffusion process. We generalize DDPMs via a class of non-Markovian\ndiffusion processes that lead to the same training objective. These non-Markovian\nprocesses can correspond to generative processes that are deterministic, giving rise\nto implicit models that produce high quality samples much faster. We empirically\ndemonstrate that DDIMs can produce high quality samples 10x to 50x faster in\nterms of wall-clock time compared to DDPMs, allow us to trade off computation\nfor sample quality, perform semantically meaningful image interpolation directly\nin the latent space, and reconstruct observations with very low error.",
    "result": {
      "summary": "DDIMs enhance DDPMs by using non-Markovian diffusion processes for faster high-quality image generation, offering faster sampling, image interpolation, and low-error reconstruction capabilities.",
      "keywords": ["DDIMs", "DDPMs", "denoising diffusion probabilistic models", "non-Markovian diffusion", "image generation", "sampling efficiency", "image interpolation", "latent space"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1310.4546v1.pdf",
    "title": "Distributed Representations of Words and Phrases\nand their Compositionality",
    "abstract": "The recently introduced continuous Skip-gram model is an efficient method for\nlearning high-quality distributed vector representations that capture a large num-\nber of precise syntactic and semantic word relationships. In this paper we present\nseveral extensions that improve both the quality of the vectors and the training\nspeed. By subsampling of the frequent words we obtain significant speedup and\nalso learn more regular word representations. We also describe a simple alterna-\ntive to the hierarchical softmax called negative sampling.\nAn inherent limitation of word representations is their indifference to word order\nand their inability to represent idiomatic phrases. For example, the meanings of\n\"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\" · Motivated\nby this example, we present a simple method for finding phrases in text, and show",
    "result": {
      "summary": "This paper enhances the Skip-gram model by improving vector quality and training speed via subsampling frequent words and introducing negative sampling. It addresses word representation limitations by capturing phrases to better handle idiomatic expressions.",
      "keywords": ["Skip-gram model", "distributed representations", "negative sampling", "word vector quality", "training speed", "idiomatic phrases", "semantic relationships"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1312.6114v11.pdf",
    "title": "Auto-Encoding Variational Bayes",
    "abstract": "How can we perform efficient inference and learning in directed probabilistic\nmodels, in the presence of continuous latent variables with intractable posterior\ndistributions, and large datasets? We introduce a stochastic variational inference\nand learning algorithm that scales to large datasets and, under some mild differ-\nentiability conditions, even works in the intractable case. Our contributions are\ntwo-fold. First, we show that a reparameterization of the variational lower bound\nyields a lower bound estimator that can be straightforwardly optimized using stan-\ndard stochastic gradient methods. Second, we show that for i.i.d. datasets with\ncontinuous latent variables per datapoint, posterior inference can be made espe-\ncially efficient by fitting an approximate inference model (also called a recogni-\ntion model) to the intractable posterior using the proposed lower bound estimator.\nTheoretical advantages are reflected in experimental results.",
    "result": {
      "summary": "This paper presents a stochastic variational inference algorithm for models with continuous latent variables and large datasets, offering a reparameterized lower bound estimator optimized via stochastic gradients and efficient posterior inference through an approximate model.",
      "keywords": ["stochastic variational inference", "continuous latent variables", "reparameterization", "lower bound estimator", "stochastic gradient methods", "posterior inference", "recognition model"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/2006.11239v2.pdf",
    "title": "Denoising Diffusion Probabilistic Models",
    "abstract": "We present high quality image synthesis results using diffusion probabilistic models,\na class of latent variable models inspired by considerations from nonequilibrium\nthermodynamics. Our best results are obtained by training on a weighted variational\nbound designed according to a novel connection between diffusion probabilistic\nmodels and denoising score matching with Langevin dynamics, and our models nat-\nurally admit a progressive lossy decompression scheme that can be interpreted as a\ngeneralization of autoregressive decoding. On the unconditional CIFAR10 dataset,\nwe obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On\n256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our imple-\nmentation is available at https : / / github · com/hoj onathanho/diffusion.",
    "result": {
      "summary": "The paper introduces diffusion probabilistic models for high-quality image synthesis, leveraging a novel connection with denoising score matching and Langevin dynamics, achieving top results on CIFAR10 and comparable quality to ProgressiveGAN on LSUN.",
      "keywords": ["diffusion probabilistic models", "image synthesis", "denoising score matching", "Langevin dynamics", "CIFAR10", "LSUN", "Inception score", "FID score", "ProgressiveGAN"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/nature16961.pdf",
    "title": "Mastering the game of Go with deep\nneural networks and tree search",
    "abstract": "The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its\nenormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach\nto computer Go that uses 'value networks' to evaluate board positions and 'policy networks' to select moves. These deep\nneural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement\nlearning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-\nof-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a\nnew search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm,\nour program AlphaGo achieved a 99. 8% winning rate against other Go programs, and defeated the human European Go\nchampion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the\nfull -sized game of Go, a feat previously thought to be at least a decade away.",
    "result": {
      "summary": "AlphaGo integrates deep neural networks and tree search to master Go, using 'value networks' for evaluation and 'policy networks' for move selection, trained through human expert games and self-play, surpassing human professional players.",
      "keywords": ["AlphaGo", "deep neural networks", "tree search", "game of Go", "value networks", "policy networks", "Monte Carlo tree search", "reinforcement learning", "supervised learning"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1406.2661v1.pdf",
    "title": "Generative Adversarial Nets",
    "abstract": "We propose a new framework for estimating generative models via an adversar-\nial process, in which we simultaneously train two models: a generative model G\nthat captures the data distribution, and a discriminative model D that estimates\nthe probability that a sample came from the training data rather than G. The train-\ning procedure for G is to maximize the probability of D making a mistake. This\nframework corresponds to a minimax two-player game. In the space of arbitrary\nfunctions G and D, a unique solution exists, with G recovering the training data\ndistribution and D equal to 1/2 everywhere. In the case where G and D are defined\nby multilayer perceptrons, the entire system can be trained with backpropagation.\nThere is no need for any Markov chains or unrolled approximate inference net-\nworks during either training or generation of samples. Experiments demonstrate\nthe potential of the framework through qualitative and quantitative evaluation of\nthe generated samples.",
    "result": {
      "summary": "Generative Adversarial Nets (GANs) involve training a generative model and a discriminative model in an adversarial process to capture and differentiate data distributions, offering a robust framework that operates without Markov chains or inference networks.",
      "keywords": ["Generative Adversarial Nets", "GANs", "generative model", "discriminative model", "adversarial training", "minimax game", "backpropagation"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1406.1078v3.pdf",
    "title": "Learning Phrase Representations using RNN Encoder-Decoder\nfor Statistical Machine Translation",
    "abstract": "In this paper, we propose a novel neu-\nral network model called RNN Encoder-\nDecoder that consists of two recurrent\nneural networks (RNN). One RNN en-\ncodes a sequence of symbols into a fixed-\nlength vector representation, and the other\ndecodes the representation into another se-\nquence of symbols. The encoder and de-\ncoder of the proposed model are jointly\ntrained to maximize the conditional prob-\nability of a target sequence given a source\nsequence. The performance of a statisti-\ncal machine translation system is empiri-\ncally found to improve by using the con-\nditional probabilities of phrase pairs com-\nputed by the RNN Encoder-Decoder as an\nadditional feature in the existing log-linear\nmodel. Qualitatively, we show that the\nproposed model learns a semantically and\nsyntactically meaningful representation of\nlinguistic phrases.",
    "result": {
      "summary": "The RNN Encoder-Decoder model enhances statistical machine translation by encoding and decoding sequences of symbols into meaningful linguistic representations, improving performance when integrated as a feature in existing models.",
      "keywords": ["RNN Encoder-Decoder", "statistical machine translation", "recurrent neural networks", "sequence encoding", "linguistic representation"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/P02-1040.pdf",
    "title": "BLEU: a Method for Automatic Evaluation of Machine Translation",
    "abstract": "Human evaluations of machine translation\nare extensive but expensive. Human eval-\nuations can take months to finish and in-\nvolve human labor that can not be reused.\nWe propose a method of automatic ma-\nchine translation evaluation that is quick,\ninexpensive, and language-independent,\nthat correlates highly with human evalu-\nation, and that has little marginal cost per\nrun. We present this method as an auto-\nmated understudy to skilled human judges\nwhich substitutes for them when there is\nneed for quick or frequent evaluations. 1",
    "result": {
      "summary": "The BLEU method provides a quick, inexpensive, and language-independent way to automatically evaluate machine translation, offering high correlation with human judgment and low marginal cost per evaluation.",
      "keywords": ["BLEU", "machine translation evaluation", "automatic evaluation", "language-independent", "correlation with human judgment"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1411.4555v2.pdf",
    "title": "Show and Tell: A Neural Image Caption Generator",
    "abstract": "Automatically describing the content of an image is a\nfundamental problem in artificial intelligence that connects\ncomputer vision and natural language processing. In this\npaper, we present a generative model based on a deep re-\ncurrent architecture that combines recent advances in com-\nputer vision and machine translation and that can be used\nto generate natural sentences describing an image. The\nmodel is trained to maximize the likelihood of the target de-\nscription sentence given the training image. Experiments\non several datasets show the accuracy of the model and the\nfluency of the language it learns solely from image descrip-\ntions. Our model is often quite accurate, which we verify\nboth qualitatively and quantitatively. For instance, while\nthe current state-of-the-art BLEU-1 score (the higher the\nbetter) on the Pascal dataset is 25, our approach yields 59,\nto be compared to human performance around 69. We also\nshow BLEU-1 score improvements on Flickr30k, from 56 to\n66, and on SBU, from 19 to 28. Lastly, on the newly released\nCOCO dataset, we achieve a BLEU-4 of27.7, which is the\ncurrent state-of-the-art.",
    "result": {
      "summary": "The paper introduces a neural image caption generator that uses a deep recurrent architecture to produce natural sentences describing images, achieving state-of-the-art results across multiple datasets.",
      "keywords": ["neural image caption generator", "deep recurrent architecture", "computer vision", "machine translation", "BLEU score", "image description"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1409.3215v3.pdf",
    "title": "Sequence to Sequence Learning\nwith Neural Networks",
    "abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excel-\nlent performance on difficult learning tasks. Although DNNs work well whenever\nlarge labeled training sets are available, they cannot be used to map sequences to\nsequences. In this paper, we present a general end-to-end approach to sequence\nlearning that makes minimal assumptions on the sequence structure. Our method\nuses a multilayered Long Short-Term Memory (LSTM) to map the input sequence\nto a vector of a fixed dimensionality, and then another deep LSTM to decode the\ntarget sequence from the vector. Our main result is that on an English to French\ntranslation task from the WMT' 14 dataset, the translations produced by the LSTM\nachieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU\nscore was penalized on out-of-vocabulary words. Additionally, the LSTM did not\nhave difficulty on long sentences. For comparison, a phrase-based SMT system\nachieves a BLEU score of 33.3 on the same dataset. When we used the LSTM\nto rerank the 1000 hypotheses produced by the aforementioned SMT system, its\nBLEU score increases to 36.5, which is close to the previous best result on this\ntask. The LSTM also learned sensible phrase and sentence representations that\nare sensitive to word order and are relatively invariant to the active and the pas-\nsive voice. Finally, we found that reversing the order of the words in all source\nsentences (but not target sentences) improved the LSTM's performance markedly,\nbecause doing SO introduced many short term dependencies between the source\nand the target sentence which made the optimization problem easier.",
    "result": {
      "summary": "This paper introduces an end-to-end sequence learning approach using LSTM networks for sequence-to-sequence tasks, demonstrating superior performance in English to French translation on the WMT'14 dataset with a notable BLEU score improvement.",
      "keywords": ["sequence to sequence learning", "LSTM", "neural networks", "English to French translation", "BLEU score", "deep learning", "multilayered LSTM", "WMT'14"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/2112.10752v2.pdf",
    "title": "High-Resolution Image Synthesis with Latent Diffusion Models",
    "abstract": "By decomposing the image formation process into a se-\nquential application of denoising autoencoders, diffusion\nmodels (DMs) achieve state-of-the-art synthesis results on\nimage data and beyond. Additionally, their formulation al-\nlows for a guiding mechanism to control the image gen-\neration process without retraining. However, since these\nmodels typically operate directly in pixel space, optimiza-\ntion of powerful DMs often consumes hundreds of GPU\ndays and inference is expensive due to sequential evalu-\nations. To enable DM training on limited computational\nresources while retaining their quality and flexibility, we\napply them in the latent space of powerful pretrained au-\ntoencoders. In contrast to previous work, training diffusion\nmodels on such a representation allows for the first time\nto reach a near-optimal point between complexity reduc-\ntion and detail preservation, greatly boosting visual fidelity.\nBy introducing cross-attention layers into the model archi-\ntecture, we turn diffusion models into powerful and flexi-\nble generators for general conditioning inputs such as text\nor bounding boxes and high-resolution synthesis becomes\npossible in a convolutional manner. Our latent diffusion\nmodels (LDMs) achieve new state-of-the-art scores for im-\nage inpainting and class-conditional image synthesis and\nhighly competitive performance on various tasks, includ-\ning text-to-image synthesis, unconditional image generation\nand super-resolution, while significantly reducing computa-\ntional requirements compared to pixel-based DMs.",
    "result": {
      "summary": "Latent Diffusion Models (LDMs) improve high-resolution image synthesis by operating in the latent space of pretrained autoencoders, enhancing visual fidelity and reducing computational demands compared to traditional pixel-based diffusion models.",
      "keywords": ["Latent Diffusion Models", "LDMs", "high-resolution image synthesis", "autoencoders", "diffusion models", "image generation", "cross-attention layers", "image inpainting", "text-to-image synthesis", "super-resolution"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/2104.05832v1.pdf",
    "title": "SPARTQA: A Textual Question Answering Benchmark\nfor Spatial Reasoning",
    "abstract": "This paper proposes a question-answering\n(QA) benchmark for spatial reasoning on nat-\nural language text which contains more real-\nistic spatial phenomena not covered by prior\nwork and is challenging for state-of-the-art\nlanguage models (LM). We propose a distant\nsupervision method to improve on this task.\nSpecifically, we design grammar and reason-\ning rules to automatically generate a spatial de-\nscription of visual scenes and corresponding\nQA pairs. Experiments show that further pre-\ntraining LMs on these automatically generated\ndata significantly improves LMs' capability on\nspatial understanding, which in turn helps to\nbetter solve two external datasets, bAbI, and\nboolQ. We hope that this work can foster inves-\ntigations into more sophisticated models for\nspatial reasoning over text.",
    "result": {
      "summary": "SPARTQA introduces a QA benchmark for spatial reasoning in natural language, using grammar-based rules to generate data that enhances language models' understanding of space, improving performance on external datasets like bAbI and boolQ.",
      "keywords": ["SPARTQA", "question answering", "spatial reasoning", "language models", "distant supervision", "data generation", "bAbI", "boolQ"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/2210.09261v1.pdf",
    "title": "Challenging BIG-Bench tasks and\nwhether chain-of-thought can solve them",
    "abstract": "BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that focuses on tasks believed to be beyond\nthe capabilities of current language models. Language models have already made good progress on this\nbenchmark, with the best model in the BIG-Bench paper outperforming average reported human-rater results\non 65% of the BIG-Bench tasks via few-shot prompting. But on what tasks do language models fall short of\naverage human-rater performance, and are those tasks actually unsolvable by current language models?",
    "result": {
      "summary": "BIG-Bench evaluates tasks beyond current language models' capabilities. While some tasks see models outperforming humans, this paper explores tasks where models underperform and discusses their solvability.",
      "keywords": ["BIG-Bench", "language models", "benchmark evaluation", "human-rater performance", "few-shot prompting"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1301.3781v3.pdf",
    "title": "Efficient Estimation of Word Representations in\nVector Space",
    "abstract": "We propose two novel model architectures for computing continuous vector repre-\nsentations of words from very large data sets. The quality of these representations\nis measured in a word similarity task, and the results are compared to the previ-\nously best performing techniques based on different types of neural networks. We\nobserve large improvements in accuracy at much lower computational cost, i.e. it\ntakes less than a day to learn high quality word vectors from a 1.6 billion words\ndata set. Furthermore, we show that these vectors provide state-of-the-art perfor-\nmance on our test set for measuring syntactic and semantic word similarities.",
    "result": {
      "summary": "The paper introduces two model architectures for generating word vector representations from large datasets, achieving higher accuracy and reduced computational costs compared to previous methods, and delivering top performance in syntactic and semantic similarity tasks.",
      "keywords": ["word representations", "vector space", "model architectures", "computational efficiency", "semantic similarity", "syntactic similarity"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/2304.13731v2.pdf",
    "title": "Text-to-Audio Generation using Instruction-Tuned\nLLM and Latent Diffusion Model",
    "abstract": "The immense scale of the recent large language models (LLM) allows many in-\nteresting properties, such as, instruction- and chain-of-thought-based fine-tuning,\nthat has significantly improved zero- and few-shot performance in many natu-\nral language processing (NLP) tasks. Inspired by such successes, we adopt such\nan instruction-tuned LLM FLAN-T5 as the text encoder for text-to-audio (TTA)\ngeneration-a task where the goal is to generate an audio from its textual de-\nscription. The prior works on TTA either pre-trained a joint text-audio encoder\nor used a non-instruction-tuned model, such as, T5. Consequently, our latent dif-\nfusion model (LDM)-based approach (TANGO) outperforms the state-of-the-art\nAudioLDM on most metrics and stays comparable on the rest on AudioCaps test\nset, despite training the LDM on a 63 times smaller dataset and keeping the text\nencoder frozen. This improvement might also be attributed to the adoption of au-\ndio pressure level-based sound mixing for the training set augmentation, whereas\nthe prior methods take a random mix.",
    "result": {
      "summary": "The paper presents TANGO, a text-to-audio generation model using an instruction-tuned LLM (FLAN-T5) and a latent diffusion model (LDM). It surpasses previous models on AudioCaps test metrics, benefiting from efficient training and specialized sound mixing techniques.",
      "keywords": ["text-to-audio generation", "instruction-tuned LLM", "latent diffusion model", "FLAN-T5", "TANGO", "AudioCaps", "sound mixing", "NLP"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1609.01704v7.pdf",
    "title": "HIERARCHICAL MULTISCALE\nRECURRENT NEURAL NETWORKS",
    "abstract": "Learning both hierarchical and temporal representation has been among the long-\nstanding challenges of recurrent neural networks. Multiscale recurrent neural\nnetworks have been considered as a promising approach to resolve this issue, yet\nthere has been a lack of empirical evidence showing that this type of models can\nactually capture the temporal dependencies by discovering the latent hierarchical\nstructure of the sequence. In this paper, we propose a novel multiscale approach,\ncalled the hierarchical multiscale recurrent neural network, that can capture the\nlatent hierarchical structure in the sequence by encoding the temporal dependencies\nwith different timescales using a novel update mechanism. We show some evidence\nthat the proposed model can discover underlying hierarchical structure in the\nsequences without using explicit boundary information. We evaluate our proposed\nmodel on character-level language modelling and handwriting sequence generation.",
    "result": {
      "summary": "The paper introduces a hierarchical multiscale recurrent neural network designed to capture latent hierarchical structures and temporal dependencies in sequences without explicit boundary markers, showing promising results in character-level language modeling and handwriting generation.",
      "keywords": ["hierarchical multiscale", "recurrent neural network", "temporal dependencies", "latent hierarchical structure", "language modeling", "handwriting sequence generation"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1509.06461v3.pdf",
    "title": "Deep Reinforcement Learning with Double Q-learning",
    "abstract": "The popular Q-learning algorithm is known to overestimate\naction values under certain conditions. It was not previously\nknown whether, in practice, such overestimations are com-\nmon, whether they harm performance, and whether they can\ngenerally be prevented. In this paper, we answer all these\nquestions affirmatively. In particular, we first show that the\nrecent DQN algorithm, which combines Q-learning with a\ndeep neural network, suffers from substantial overestimations\nin some games in the Atari 2600 domain. We then show that\nthe idea behind the Double Q-learning algorithm, which was\nintroduced in a tabular setting, can be generalized to work\nwith large-scale function approximation. We propose a spe-\ncific adaptation to the DQN algorithm and show that the re-\nsulting algorithm not only reduces the observed overestima-\ntions, as hypothesized, but that this also leads to much better\nperformance on several games.",
    "result": {
      "summary": "This paper addresses the overestimation issue in the Q-learning algorithm, particularly in the DQN variant used for Atari games. It adapts Double Q-learning to deep neural networks, significantly improving performance by reducing overestimations.",
      "keywords": ["Deep Reinforcement Learning", "Double Q-learning", "Q-learning", "DQN", "overestimation", "Atari 2600", "neural networks"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1312.5602v1.pdf",
    "title": "Playing Atari with Deep Reinforcement Learning",
    "abstract": "We present the first deep learning model to successfully learn control policies di-\nrectly from high-dimensional sensory input using reinforcement learning. The\nmodel is a convolutional neural network, trained with a variant of Q-learning,\nwhose input is raw pixels and whose output is a value function estimating future\nrewards. We apply our method to seven Atari 2600 games from the Arcade Learn-\ning Environment, with no adjustment of the architecture or learning algorithm. We\nfind that it outperforms all previous approaches on six of the games and surpasses\na human expert on three of them.",
    "result": {
      "summary": "The paper introduces a convolutional neural network model trained via a Q-learning variant, which learns control policies directly from raw pixel input. Applied to Atari 2600 games, it outperforms existing methods and exceeds human expert levels in several games.",
      "keywords": ["deep learning", "reinforcement learning", "convolutional neural network", "Q-learning", "Atari 2600", "Arcade Learning Environment", "control policies"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1909.09586v1.pdf",
    "title": "- Understanding LSTM -\na tutorial into Long Short- Term Memory\nRecurrent Neural Networks",
    "abstract": "Long Short-Term Memory Recurrent Neural Networks (LSTM-RNN)\nare one of the most powerful dynamic classifiers publicly known. The net-\nwork itself and the related learning algorithms are reasonably well docu-\nmented to get an idea how it works. This paper will shed more light into\nunderstanding how LSTM-RNNs evolved and why they work impressively\nwell, focusing on the early, ground-breaking publications. We significantly\nimproved documentation and fixed a number of errors and inconsistencies\nthat accumulated in previous publications. To support understanding we\nas well revised and unified the notation used.",
    "result": {
      "summary": "This tutorial paper enhances understanding of Long Short-Term Memory Recurrent Neural Networks (LSTM-RNNs), focusing on their evolution and effectiveness based on seminal publications. It also corrects prior documentation errors and standardizes notation.",
      "keywords": ["LSTM-RNN", "dynamic classifiers", "neural networks", "documentation improvement", "notation unification"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/2303.15056v2.pdf",
    "title": "CHATGPT OUTPERFORMS CROWD- WORKERS\nFOR TEXT-ANNOTATION TASKS",
    "abstract": "Many NLP applications require manual text annotations for a variety of tasks, notably\nto train classifiers or evaluate the performance of unsupervised models. Depending\non the size and degree of complexity, the tasks may be conducted by crowd-workers\non platforms such as MTurk as well as trained annotators, such as research assistants.\nUsing four samples of tweets and news articles (n = 6,183), we show that ChatGPT\noutperforms crowd-workers for several annotation tasks, including relevance, stance,\ntopics, and frame detection. Across the four datasets, the zero-shot accuracy of\nChatGPT exceeds that of crowd-workers by about 25 percentage points on average,\nwhile ChatGPT's intercoder agreement exceeds that of both crowd-workers and\ntrained annotators for all tasks. Moreover, the per-annotation cost of ChatGPT is\nless than $0.003-about thirty times cheaper than MTurk. These results demonstrate\nthe potential of large language models to drastically increase the efficiency of text\nclassification.",
    "result": {
      "summary": "The study demonstrates that ChatGPT outperforms crowd-workers in text-annotation tasks such as relevance, stance, topics, and frame detection on tweets and news articles, with higher accuracy and lower costs than MTurk.",
      "keywords": ["ChatGPT", "text-annotation", "crowd-workers", "MTurk", "text classification", "large language models", "intercoder agreement", "efficiency"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/2211.10435v2.pdf",
    "title": "PAL: Program-aided Language Models",
    "abstract": "Large language models (LLMs) have recently\ndemonstrated an impressive ability to perform\narithmetic and symbolic reasoning tasks, when\nprovided with a few examples at test time (\"few-\nshot prompting\"). Much of this success can be\nattributed to prompting methods such as \"chain-\nof-thought\", which employ LLMs for both under-\nstanding the problem description by decomposing\nit into steps, as well as solving each step of the\nproblem. While LLMs seem to be adept at this\nsort of step-by-step decomposition, LLMs often\nmake logical and arithmetic mistakes in the solu-\ntion part, even when the problem is decomposed\ncorrectly. In this paper, we present Program-\nAided Language models (PAL): a novel approach\nthat uses the LLM to read natural language prob-\nlems and generate programs as the intermediate\nreasoning steps, but offloads the solution step to a\nruntime such as a Python interpreter. With PAL,\ndecomposing the natural language problem into\nrunnable steps remains the only learning task for\nthe LLM, while solving is delegated to the inter-\npreter. We demonstrate this synergy between a\nneural LLM and a symbolic interpreter across 13\nmathematical, symbolic, and algorithmic reason-\ning tasks from BIG-Bench Hard and other bench-\nmarks. In all these natural language reasoning\ntasks, generating code using an LLM and rea-\nsoning using a Python interpreter leads to more\naccurate results than much larger models. For ex-\nample, PAL using CODEX achieves state-of-the-\nart few-shot accuracy on the GSM8K benchmark\nof math word problems, surpassing PaLM-540B\nwhich uses chain-of-thought by absolute 15% top-\n1. Our code and data are publicly available at\nhttp : / / reasonwithpal · com ·",
    "result": {
      "summary": "PAL enhances LLMs by integrating with a Python interpreter to handle arithmetic and symbolic reasoning more accurately. This approach delegates execution to the interpreter after the LLM formulates problems as runnable code, achieving superior results on various reasoning tasks.",
      "keywords": ["Program-aided Language Models", "PAL", "Large Language Models", "LLMs", "Python interpreter", "symbolic reasoning", "arithmetic reasoning", "BIG-Bench Hard", "GSM8K"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1409.1556v6.pdf",
    "title": "VERY DEEP CONVOLUTIONAL NETWORKS\nFOR LARGE-SCALE IMAGE RECOGNITION",
    "abstract": "In this work we investigate the effect of the convolutional network depth on its\naccuracy in the large-scale image recognition setting. Our main contribution is\na thorough evaluation of networks of increasing depth using an architecture with\nvery small (3 x 3) convolution filters, which shows that a significant improvement\non the prior-art configurations can be achieved by pushing the depth to 16-19\nweight layers. These findings were the basis of our ImageNet Challenge 2014\nsubmission, where our team secured the first and the second places in the localisa-\ntion and classification tracks respectively. We also show that our representations\ngeneralise well to other datasets, where they achieve state-of-the-art results. We\nhave made our two best-performing ConvNet models publicly available to facili-\ntate further research on the use of deep visual representations in computer vision.",
    "result": {
      "summary": "This study explores how increasing the depth of convolutional networks, using small (3x3) filters, significantly enhances accuracy in large-scale image recognition, demonstrated through top results in the ImageNet Challenge 2014.",
      "keywords": ["deep convolutional networks", "image recognition", "network depth", "ImageNet Challenge", "state-of-the-art", "3x3 convolution filters"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1511.06018v2.pdf",
    "title": "SEGMENTAL RECURRENT NEURAL NETWORKS",
    "abstract": "We introduce segmental recurrent neural networks (SRNNs) which define,\ngiven an input sequence, a joint probability distribution over segmentations of\nthe input and labelings of the segments. Representations of the input segments\n(i.e., contiguous subsequences of the input) are computed by encoding their con-\nstituent tokens using bidirectional recurrent neural nets, and these \"segment em-\nbeddings\" are used to define compatibility scores with output labels. These lo-\ncal compatibility scores are integrated using a global semi-Markov conditional\nrandom field. Both fully supervised training-in which segment boundaries and\nlabels are observed -as well as partially supervised training-in which segment\nboundaries are latent-are straightforward. Experiments on handwriting recog-\nnition and joint Chinese word segmentation/POS tagging show that, compared to\nmodels that do not explicitly represent segments such as BIO tagging schemes and\nconnectionist temporal classification (CTC), SRNNs obtain substantially higher\naccuracies.",
    "result": {
      "summary": "SRNNs are introduced as a model that combines bidirectional RNNs and a semi-Markov CRF to jointly segment and label sequences, showing superior accuracy in handwriting recognition and Chinese word segmentation/POS tagging compared to existing models.",
      "keywords": ["segmental recurrent neural networks", "SRNNs", "bidirectional RNNs", "semi-Markov conditional random field", "sequence labeling", "handwriting recognition", "Chinese word segmentation", "POS tagging"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1511.06434v2.pdf",
    "title": "UNSUPERVISED REPRESENTATION LEARNING\nWITH DEEP CONVOLUTIONAL\nGENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "In recent years, supervised learning with convolutional networks (CNNs) has\nseen huge adoption in computer vision applications. Comparatively, unsupervised\nlearning with CNNs has received less attention. In this work we hope to help\nbridge the gap between the success of CNNs for supervised learning and unsuper-\nvised learning. We introduce a class of CNNs called deep convolutional generative\nadversarial networks (DCGANs), that have certain architectural constraints, and\ndemonstrate that they are a strong candidate for unsupervised learning. Training\non various image datasets, we show convincing evidence that our deep convolu-\ntional adversarial pair learns a hierarchy of representations from object parts to\nscenes in both the generator and discriminator. Additionally, we use the learned\nfeatures for novel tasks - demonstrating their applicability as general image repre-\nsentations.",
    "result": {
      "summary": "This paper introduces deep convolutional generative adversarial networks (DCGANs) as a potent method for unsupervised learning in CNNs, demonstrating their ability to learn hierarchical representations of images and their effectiveness as general image representations.",
      "keywords": ["unsupervised learning", "convolutional networks", "DCGANs", "deep convolutional generative adversarial networks", "image representations", "hierarchical representations"]
    }    
  }
]