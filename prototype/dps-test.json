[{
    "title": "MixFormer: Mixing Features across Windows and Dimensions",
    "domain": "Computer Vision",
    "problem": "Local window self attention suffers from limited receptive field and weak modeling capability",
    "solution": "Combine local window self attention with depth-wise convolution in parallel"
}, {
    "title": "A ConvNet for the 2020s",
    "domain": "Computer Vision",
    "problem": "Vision transformer lacks the utilization of convolutional inductive bias, therefore struggles in traditional tasks like object detection.",
    "solution": "Suggests modernized ResNet architecture built on top of pure ConvNet that can compete with transformers."
}, {
    "title": "Attention Mechanisms in Computer Vision: A Survey",
    "domain": "Computer Vision",
    "problem": "There are too many vision transformers out there.",
    "solution": "Provides comprehensive views of various vision transformer mechanisms."
}, {
    "title": "Masked Autoencoders Are Scalable Vision Learners",
    "domain": "Computer Vision",
    "problem": "Previous self-supervised learning methods for computer vision may lack scalability or struggle with complex image representations.",
    "solution": "Masked autoencoders (MAE) overcomes these challenges by using an asymmetric encoder-decoder architecture and employing a high masking proportion."
}, {
    "title": "CYCLEMLP: A MLP-LIKE ARCHITECTURE FOR DENSE PREDICTION",
    "domain": "Computer Vision",
    "problem": "Several limitations such as dependence on image size and computational complexity in existing MLP architectures for visual recognition and dense prediction tasks.",
    "solution": "CycleMLP, a versatile and efficient MLP-like architecture with linear computational complexity that handles various image sizes and outperforms existing models."
}, {
    "title": "CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows ",
    "domain": "Computer vision",
    "problem": "Limitation of self-attention algorithm in Transformer, high computatinal cost and weak model capability",
    "solution": "Use both vertical and horizontal stripes in parallel for self-attention and find better local positional encoding method "
}, {
    "title": "Glance-and-Gaze Vision Transformer",
    "domain": "Computer Vision",
    "problem": "Quadratic computation and memory cost of self-attention for long sequence length",
    "solution": "Model both long-range and local context by applying self-attention to partitioned input and convolution method to add local context"
}, {
    "title": "SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers",
    "domain": "Computer Vision - Semantic Segmentation ",
    "problem": "Low Performance, Decoder architecture is too complex",
    "solution": "Make decoder simple and lightweight with multilayer perceptron "
}, {
    "title": "MSG-Transformer: Exchanging Local Spatial Information by Manipulating Messenger Tokens",
    "domain": "Computer Vision",
    "problem": "Computational overhead of transformer architecture leads processing high resolution visual data difficult",
    "solution": "use specialized tokens as messengers for flexible exchange of visual information across regions and reduce computational complexity"
}, {
    "title": "Visformer: The Vision-friendly Transformer",
    "domain": "Computer Vision and Machine Learning",
    "problem": "Overfitting in Transformer-based models, especially when the training data is limited.",
    "solution": "Investigating ways to combine the strengths of Transformer-based and convolution-based models to improve visual recognition performance."
}, {
    "title": "Co-Scale Conv-Attentional Image Transformers",
    "domain": "Computer Vision and Machine Learning",
    "problem": "The need for improved multi-scale and contextual modeling capabilities in Transformer-based image classifiers.",
    "solution": "Exploring mechanisms to enhance the performance and effectiveness of Transformer-based models for image classification and other related computer vision tasks."
}]