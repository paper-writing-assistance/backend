[
  {
    "pdf_path": "./pdf/AI_VIT_O/2103.15808v1.pdf",
    "title": "CvT: Introducing Convolutions to Vision Transformers",
    "abstract": "We present in this paper a new architecture, named Con-\nvolutional vision Transformer (CvT), that improves Vision\nTransformer (ViT) in performance and efficiency by intro-\nducing convolutions into ViT to yield the best of both de-\nsigns. This is accomplished through two primary modifica-\ntions: a hierarchy of Transformers containing a new convo-\nlutional token embedding, and a convolutional Transformer\nblock leveraging a convolutional projection. These changes\nintroduce desirable properties of convolutional neural net-\nworks (CNNs) to the ViT architecture (i.e. shift, scale,\nand distortion invariance) while maintaining the merits of\nTransformers (i.e. dynamic attention, global context, and\nbetter generalization). We validate CvT by conducting ex-\ntensive experiments, showing that this approach achieves\nstate-of-the-art performance over other Vision Transform-\ners and ResNets on ImageNet-1k, with fewer parame-\nters and lower FLOPs. In addition, performance gains\nare maintained when pretrained on larger datasets (e.g.\nImageNet-22k) and fine-tuned to downstream tasks. Pre-\ntrained on ImageNet-22k, our CvT-W24 obtains a top-1 ac-\ncuracy of 87.7% on the ImageNet-1k val set. Finally, our\nresults show that the positional encoding, a crucial com-\nponent in existing Vision Transformers, can be safely re-\nmoved in our model, simplifying the design for higher res-\nolution vision tasks. Code will be released at https :",
    "result": {
      "domain": "Computer Vision",
      "problem": "Previous Vision Transformers (ViTs) face challenges in performance and efficiency, and lack the desirable properties of convolutional neural networks (CNNs) such as shift, scale, and distortion invariance.",
      "solution": "Introducing convolutions into Vision Transformers to create a new architecture called Convolutional vision Transformer (CvT). This involves a hierarchy of Transformers with convolutional token embedding and a convolutional Transformer block with convolutional projection, combining the strengths of both CNNs and Transformers.",
      "keywords": ["Vision Transformer", "Convolutional Neural Networks", "ImageNet", "Dynamic Attention", "Global Context", "Shift Invariance", "Scale Invariance", "Distortion Invariance"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/2111.06377v3.pdf",
    "title": "Masked Autoencoders Are Scalable Vision Learners",
    "abstract": "This paper shows that masked autoencoders (MAE) are\nscalable self-supervised learners for computer vision. Our\nMAE approach is simple: we mask random patches of the\ninput image and reconstruct the missing pixels. It is based\non two core designs. First, we develop an asymmetric\nencoder-decoder architecture, with an encoder that oper-\nates only on the visible subset of patches (without mask to-\nkens), along with a lightweight decoder that reconstructs\nthe original image from the latent representation and mask\ntokens. Second, we find that masking a high proportion\nof the input image, e.g., 75%, yields a nontrivial and\nmeaningful self-supervisory task. Coupling these two de-\nsigns enables us to train large models efficiently and ef-\nfectively: we accelerate training (by 3x or more) and im-\nprove accuracy. Our scalable approach allows for learning\nhigh-capacity models that generalize well: e.g., a vanilla\nViT-Huge model achieves the best accuracy (87.8%) among\nmethods that use only ImageNet-1K data. Transfer per-\nformance in downstream tasks outperforms supervised pre-\ntraining and shows promising scaling behavior.",
    "result":{
      "domain": "Computer Vision",
      "problem": "Previous self-supervised learning methods for computer vision may lack scalability or struggle with complex image representations.",
      "solution": "Introducing masked autoencoders (MAE) as scalable self-supervised learners for computer vision. MAE employs an asymmetric encoder-decoder architecture, focusing on reconstructing randomly masked patches of the input image, which enables efficient and effective training of large models.",
      "keywords": ["masked autoencoders", "MAE", "scalable", "self-supervised learning", "computer vision", "asymmetric encoder-decoder", "ImageNet-1K", "training efficiency", "transfer performance"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/2105.15168v3.pdf",
    "title": "MSG- Transformer: Exchanging Local Spatial Information by\nManipulating Messenger Tokens",
    "abstract": "Transformers have offered a new methodology of de-\nsigning neural networks for visual recognition. Compared\nto convolutional networks, Transformers enjoy the ability\nof referring to global features at each stage, yet the at-\ntention module brings higher computational overhead that\nobstructs the application of Transformers to process high-\nresolution visual data. This paper aims to alleviate the\nconflict between efficiency and flexibility, for which we pro-\npose a specialized token for each region that serves as a\nmessenger (MSG). Hence, by manipulating these MSG to-\nkens, one can flexibly exchange visual information across\nregions and the computational complexity is reduced. We\nthen integrate the MSG token into a multi-scale architec-\nture named MSG-Transformer. In standard image classi-\nfication and object detection, MSG-Transformer achieves\ncompetitive performance and the inference on both GPU\nand CPU is accelerated. Code is available at https :\n\\ \\ github Â· com/hustvl /MSG- Transformer.",
    "result": {
      "domain": "Computer Vision",
      "problem": "Transformers face challenges in efficiently processing high-resolution visual data due to their attention mechanism's computational overhead.",
      "solution": "Introducing a specialized token called Messenger Tokens (MSG) to alleviate the computational overhead of Transformers. By manipulating these MSG tokens, visual information exchange across regions becomes flexible, reducing computational complexity. Integrating MSG tokens into a multi-scale architecture named MSG-Transformer achieves competitive performance in image classification and object detection while accelerating inference on both GPU and CPU.",
      "keywords": ["MSG-Transformer", "Transformers", "messenger tokens", "visual recognition", "computational efficiency", "image classification", "object detection"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/2010.11929v2.pdf",
    "title": "AN IMAGE IS WORTH 16x16 WORDS:\nTRANSFORMERS FOR IMAGE RECOGNITION AT SCALE",
    "abstract": "While the Transformer architecture has become the de-facto standard for natural\nlanguage processing tasks, its applications to computer vision remain limited. In\nvision, attention is either applied in conjunction with convolutional networks, or\nused to replace certain components of convolutional networks while keeping their\noverall structure in place. We show that this reliance on CNNs is not necessary\nand a pure transformer applied directly to sequences of image patches can perform\nvery well on image classification tasks. When pre-trained on large amounts of\ndata and transferred to multiple mid-sized or small image recognition benchmarks\n(ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent\nresults compared to state-of-the-art convolutional networks while requiring sub-\nstantially fewer computational resources to train. 1",
    "result": {
      "domain": "Computer Vision",
      "problem": "Current approaches in computer vision rely on convolutional neural networks (CNNs) alongside or instead of transformers, limiting their scalability and computational efficiency.",
      "solution": "Demonstrating the effectiveness of using a pure transformer architecture directly on sequences of image patches for image recognition tasks. The Vision Transformer (ViT) model, when pretrained on large datasets and transferred to various benchmarks, achieves excellent results compared to state-of-the-art CNNs while requiring fewer computational resources.",
      "keywords": ["Transformers", "image recognition", "Vision Transformer", "ViT", "CNN replacement", "image classification", "computational efficiency"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/2005.00928v2.pdf",
    "title": "Quantifying Attention Flow in Transformers",
    "abstract": "In the Transformer model, \"self-attention\"\ncombines information from attended embed-\ndings into the representation of the focal em-\nbedding in the next layer. Thus, across lay-\ners of the Transformer, information originating\nfrom different tokens gets increasingly mixed.\nThis makes attention weights unreliable as ex-\nplanations probes. In this paper, we consider\nthe problem of quantifying this flow of infor-\nmation through self-attention. We propose two\nmethods for approximating the attention to in-\nput tokens given attention weights, attention\nrollout and attentionflow, as post hoc methods\nwhen we use attention weights as the relative\nrelevance of the input tokens. We show that\nthese methods give complementary views on\nthe flow of information, and compared to raw\nattention, both yield higher correlations with\nimportance scores of input tokens obtained us-\ning an ablation method and input gradients.",
    "result": {
      "domain": "Natural Language Processing",
      "problem": "In the Transformer model, the mixing of information across layers makes attention weights unreliable as explanations probes, hindering the interpretation of attention mechanisms.",
      "solution": "Introducing two methods, attention rollout and attention flow, for quantifying the flow of information through self-attention in Transformers. These methods provide complementary views on information flow and yield higher correlations with importance scores of input tokens compared to raw attention.",
      "keywords": ["Transformers", "self-attention", "attention flow", "quantifying information", "attention rollout", "attentionflow", "explanations probes"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/2106.02277v1.pdf",
    "title": "Glance-and-Gaze Vision Transformer",
    "abstract": "Recently, there emerges a series of vision Transformers, which show superior\nperformance with a more compact model size than conventional convolutional\nneural networks, thanks to the strong ability of Transformers to model long-range\ndependencies. However, the advantages of vision Transformers also come with a\nprice: Self-attention, the core part of Transformer, has a quadratic complexity to\nthe input sequence length. This leads to a dramatic increase of computation and\nmemory cost with the increase of sequence length, thus introducing difficulties\nwhen applying Transformers to the vision tasks that require dense predictions based\non high-resolution feature maps.",
    "result": {
      "domain": "Computer Vision",
      "problem": "Vision Transformers show superior performance with compact model sizes compared to convolutional neural networks, but their self-attention mechanism's quadratic complexity to input sequence length leads to increased computation and memory costs, posing challenges for vision tasks requiring dense predictions based on high-resolution feature maps.",
      "solution": "Introducing the Glance-and-Gaze Vision Transformer to address the computational and memory challenges associated with vision Transformers. This model aims to efficiently process high-resolution feature maps by incorporating mechanisms to reduce the computational and memory overhead of self-attention.",
      "keywords": ["Glance-and-Gaze Vision Transformer", "vision Transformers", "self-attention", "computational cost", "high-resolution feature maps", "long-range dependencies"]
    }
  }, {
    "pdf_path": "./pdf/AI_VIT_O/2102.12122v2.pdf",
    "title": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction\nwithout Convolutions",
    "abstract": "Although convolutional neural networks (CNNs) have\nachieved great success in computer vision, this work inves-\ntigates a simpler, convolution-free backbone network use-\nful for many dense prediction tasks. Unlike the recently-\nproposed Vision Transformer (ViT) that was designed for\nimage classification specifically, we introduce the Pyra-\nmid Vision Transformer (PVT), which overcomes the diffi-\nculties of porting Transformer to various dense prediction\ntasks. PVT has several merits compared to current state\nof the arts. (1) Different from ViT that typically yields low-\nresolution outputs and incurs high computational and mem-\nory costs, PVT not only can be trained on dense partitions\nof an image to achieve high output resolution, which is im-\nportant for dense prediction, but also uses a progressive\nshrinking pyramid to reduce the computations of large fea-\nture maps. (2) PVT inherits the advantages of both CNN\nand Transformer, making it a unified backbone for vari-",
    "result": {
      "domain": "Computer Vision",
      "problem": "While convolutional neural networks (CNNs) have been successful in computer vision, there is a need for a convolution-free backbone network suitable for various dense prediction tasks.",
      "solution": "Introducing the Pyramid Vision Transformer (PVT) as a versatile backbone network for dense prediction tasks. Unlike Vision Transformer (ViT), which is tailored for image classification, PVT overcomes the challenges of applying Transformers to dense prediction tasks. PVT achieves high output resolution by training on dense partitions of an image and reduces computational costs by using a progressive shrinking pyramid.",
      "keywords": ["Pyramid Vision Transformer", "PVT", "dense prediction", "convolution-free backbone", "high-resolution outputs", "Vision Transformer", "computational efficiency"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/1512.03385v1.pdf",
    "title": "Deep Residual Learning for Image Recognition",
    "abstract": "Deeper neural networks are more difficult to train. We\npresent a residual learning framework to ease the training\nof networks that are substantially deeper than those used\npreviously. We explicitly reformulate the layers as learn-\ning residual functions with reference to the layer inputs, in-\nstead of learning unreferenced functions. We provide com-\nprehensive empirical evidence showing that these residual\nnetworks are easier to optimize, and can gain accuracy from\nconsiderably increased depth. On the ImageNet dataset we\nevaluate residual nets with a depth of up to 152 layers-8x\ndeeper than VGG nets [41] but still having lower complex-\nity. An ensemble of these residual nets achieves 3.57% error\non the ImageNet test set. This result won the 1st place on the\nILSVRC 2015 classification task. We also present analysis\non CIFAR-10 with 100 and 1000 layers.",
    "result": {
      "domain": "Computer Vision",
      "problem": "Training deeper neural networks becomes increasingly difficult due to issues such as vanishing gradients and optimization challenges.",
      "solution": "Introducing a residual learning framework to ease the training of substantially deeper networks by reformulating layers as learning residual functions with reference to the layer inputs. Empirical evidence demonstrates that these residual networks are easier to optimize and can achieve higher accuracy with increased depth.",
      "keywords": ["Deep Residual Learning", "image recognition", "neural networks", "residual networks", "ImageNet", "deep learning", "ILSVRC 2015"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/2103.14030v2.pdf",
    "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows",
    "abstract": "This paper presents a new vision Transformer, called\nSwin Transformer, that capably serves as a general-purpose\nbackbone for computer vision. Challenges in adapting\nTransformer, from language to vision arise from differences\nbetween the two domains, such as large variations in the\nscale of visual entities and the high resolution of pixels\nin images compared to words in text. To address these\ndifferences, we propose a hierarchical Transformer whose\nrepresentation is computed with Shifted windows. The\nshifted windowing scheme brings greater efficiency by lim-\niting self-attention computation to non-overlapping local\nwindows while also allowing for cross-window connection.\nThis hierarchical architecture has the flexibility to model\nat various scales and has linear computational complexity\nwith respect to image size. These qualities of Swin Trans-\nformer make it compatible with a broad range of vision\ntasks, including image classification (87.3 top-1 accuracy\non ImageNet-1K) and dense prediction tasks such as object\ndetection (58.7 box AP and 51.1 mask AP on COCO test-\ndev) and semantic segmentation (53.5 mIoU on ADE20K\nval). Its performance surpasses the previous state-of-the-\nart by a large margin of +2.7 box AP and +2.6 mask AP on\nCOCO, and +3.2 mIoU on ADE20K, demonstrating the po-\ntential of Transformer-based models as vision backbones.\nThe hierarchical design and the shifted window approach\nalso prove beneficial for all-MLP architectures. The code\nand models are publicly available at https : / /github.\ncom/mi crosoft / Swin-Transformer.",
    "result": {
      "domain": "Computer Vision",
      "problem": "Adapting Transformer models from language to vision presents challenges due to differences such as scale variations and high image resolution.",
      "solution": "Introducing Swin Transformer, a hierarchical vision Transformer using Shifted windows to serve as a general-purpose backbone for computer vision tasks. The hierarchical architecture, coupled with the shifted windowing scheme, enables efficient self-attention computation and allows modeling at various scales with linear computational complexity. Swin Transformer demonstrates superior performance on various vision tasks, including image classification, object detection, and semantic segmentation, surpassing previous state-of-the-art methods.",
      "keywords": ["Swin Transformer", "vision Transformer", "hierarchical", "shifted windows", "image classification", "object detection", "semantic segmentation", "computational efficiency", "ImageNet-1K", "COCO", "ADE20K"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/2111.07624v1.pdf",
    "title": "Attention Mechanisms in Computer Vision:\nA Survey",
    "abstract": "Abstract-Humans can naturally and effectively find salient regions in complex scenes. Motivated by this observation, attention\nmechanisms were introduced into computer vision with the aim of imitating this aspect of the human visual system. Such an attention\nmechanism can be regarded as a dynamic weight adjustment process based on features of the input image. Attention mechanisms have\nachieved great success in many visual tasks, including image classification, object detection, semantic segmentation, video\nunderstanding, image generation, 3D vision, multi-modal tasks and self-supervised learning. In this survey, we provide a comprehensive\nreview of various attention mechanisms in computer vision and categorize them according to approach, such as channel attention, spatial\nattention, temporal attention and branch attention; a related repository https://hub.com/MengfasoGuokAreernermew-Aters is\ndedicated to collecting related work. We also suggest future directions for attention mechanism research.",
    "result": {
      "domain": "Computer Vision",
      "problem": "Humans possess natural and effective mechanisms for identifying salient regions in complex scenes, inspiring the introduction of attention mechanisms into computer vision to mimic this aspect of the human visual system.",
      "solution": "Providing a comprehensive review of various attention mechanisms in computer vision, categorized by approach, such as channel attention, spatial attention, temporal attention, and branch attention. Attention mechanisms have demonstrated significant success in numerous visual tasks, including image classification, object detection, semantic segmentation, video understanding, and more.",
      "keywords": ["attention mechanisms", "computer vision", "image classification", "object detection", "semantic segmentation", "video understanding", "3D vision", "multi-modal tasks", "self-supervised learning"]
    }       
  }, {
    "pdf_path": "./pdf/AI_VIT_O/2104.12533v5.pdf",
    "title": "Visformer: The Vision-friendly Transformer",
    "abstract": "Abstract-The past few years have witnessed the rapid devel-\nopment of applying the Transformer module to vision problems.\nWhile some researchers have demonstrated that Transformer-\nbased models enjoy a favorable ability of fitting data, there are\nstill growing number of evidences showing that these models\nsuffer over-fitting especially when the training data is limited.\nThis paper offers an empirical study by performing step-by-\nstep operations to gradually transit a Transformer-based model\nto a convolution-based model. The results we obtain during\nthe transition process deliver useful messages for improving\nvisual recognition. Based on these observations, we propose a\nnew architecture named Visformer, which is abbreviated from\nthe 'Vision-friendly Transformer'. With the same computational\ncomplexity, Visformer outperforms both the Transformer-based\nand convolution-based models in terms of ImageNet classification\nand object detection performance, and the advantage becomes\nmore significant when the model complexity is lower or the\ntraining set is smaller. The code is available at https://github.\ncom/danczs/Visformer.",
    "result": {
      "domain": "Computer Vision",
      "problem": "While Transformer-based models have shown promise in fitting data for vision tasks, evidence suggests they may suffer from overfitting, particularly with limited training data.",
      "solution": "Conducting an empirical study to gradually transition a Transformer-based model to a convolution-based model, resulting in observations useful for improving visual recognition. Based on these findings, introducing Visformer, a new architecture designed to be more vision-friendly. Visformer outperforms both Transformer-based and convolution-based models in ImageNet classification and object detection, especially with lower model complexity or smaller training sets.",
      "keywords": ["Visformer", "Vision-friendly Transformer", "Transformer", "convolutional models", "over-fitting", "ImageNet classification", "object detection"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/2204.02557v2.pdf",
    "title": "MixFormer: Mixing Features across Windows and Dimensions",
    "abstract": "While local-window self-attention performs notably in\nvision tasks, it suffers from limited receptive field and weak\nmodeling capability issues. This is mainly because it per-\nforms self-attention within non-overlapped windows and\nshares weights on the channel dimension. We propose Mix-\nFormer to find a solution. First, we combine local-window\nself-attention with depth-wise convolution in a parallel de-\nsign, modeling cross-window connections to enlarge the re-\nceptive fields. Second, we propose bi-directional interac-\ntions across branches to provide complementary clues in\nthe channel and spatial dimensions. These two designs are\nintegrated to achieve efficient feature mixing among win-\ndows and dimensions. Our MixFormer provides compet-\nitive results on image classification with EfficientNet and\nshows better results than RegNet and Swin Transformer.\nPerformance in downstream tasks outperforms its alterna-\ntives by significant margins with less computational costs\nin 5 dense prediction tasks on MS COCO, ADE20k, and\nLVIS. Code is available at https : / / github Â· com /\nPaddl ePaddl e/Paddl eClas.",
    "result": {
      "domain": "Computer Vision",
      "problem": "Local-window self-attention in vision tasks suffers from limited receptive field and weak modeling capability due to performing self-attention within non-overlapped windows and sharing weights on the channel dimension.",
      "solution": "Introducing MixFormer to address these issues by combining local-window self-attention with depth-wise convolution in a parallel design, enlarging receptive fields through cross-window connections, and proposing bi-directional interactions across branches to provide complementary clues in channel and spatial dimensions. MixFormer achieves competitive results in image classification with EfficientNet and outperforms alternatives such as RegNet and Swin Transformer in downstream tasks with lower computational costs.",
      "keywords": ["MixFormer", "feature mixing", "local-window self-attention", "depth-wise convolution", "bi-directional interactions", "image classification", "dense prediction tasks", "EfficientNet", "RegNet", "Swin Transformer"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/2103.11886v4.pdf",
    "title": "Deep ViT: Towards Deeper Vision Transformer",
    "abstract": "Vision transformers (ViTs) have been successfully ap-\nplied in image classification tasks recently. In this paper,\nwe show that, unlike convolution neural networks (CNNs)\nthat can be improved by stacking more convolutional lay-\ners, the performance of ViTs saturate fast when scaled to\nbe deeper. More specifically, we empirically observe that\nsuch scaling difficulty is caused by the attention collapse\nissue: as the transformer goes deeper, the attention maps\ngradually become similar and even much the same after\ncertain layers. In other words, the feature maps tend to\nbe identical in the top layers of deep ViT models. This\nfact demonstrates that in deeper layers of ViTs, the self-\nattention mechanism fails to learn effective concepts for\nrepresentation learning and hinders the model from get-\nting expected performance gain. Based on above obser-\nvation, we propose a simple yet effective method, named\nRe-attention, to re-generate the attention maps to increase\ntheir diversity at different layers with negligible computa-\ntion and memory cost. The proposed method makes it feasi-\nble to train deeper ViT models with consistent performance\nimprovements via minor modification to existing ViT mod-\nels. Notably, when training a deep ViT model with 32 trans-\nformer blocks, the Top-1 classification accuracy can be im-\nproved by 1.6% on ImageNet. Code is publicly available at\nhttps : //github Â· com/ zhoudaquan/ dvit_repo.",
    "result": {
      "domain": "Computer Vision",
      "problem": "Vision Transformers (ViTs) face challenges in achieving performance improvements with increased depth compared to convolutional neural networks (CNNs), as deeper ViTs suffer from attention collapse, where attention maps become similar or even identical in deeper layers.",
      "solution": "Introducing Deep ViT, a method to address the attention collapse issue by proposing Re-attention, a simple yet effective method to regenerate attention maps and increase their diversity at different layers with minimal computational and memory cost. Re-attention enables training deeper ViT models with consistent performance improvements, achieving a 1.6% improvement in Top-1 classification accuracy on ImageNet with 32 transformer blocks.",
      "keywords": ["Deep ViT", "Vision Transformers", "attention collapse", "Re-attention", "deeper layers", "image classification", "performance improvement", "ImageNet"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/2304.02643v1.pdf",
    "title": "Segment Anything",
    "abstract": "We introduce the Segment Anything (SA) project: a new\ntask, model, and datasetfor image segmentation. Using our\nefficient model in a data collection loop, we built the largest\nsegmentation dataset to date (by far), with over 1 billion\nmasks on 11M licensed and privacy respecting images. The\nmodel is designed and trained to be promptable, SO it can\ntransfer zero-shot to new image distributions and tasks. We\nevaluate its capabilities on numerous tasks and find that\nits zero-shot performance is impressive - often competitive\nwith or even superior to prior fully supervised results. We\nare releasing the Segment Anything Model (SAM) and cor-\nresponding dataset (SA-1B) of 1B masks and 11M images at\nhttps://segment-anything.com to foster research into foun-\ndation models for computer vision.",
    "result": {
      "summary": "The Segment Anything project introduces a promptable model and the largest dataset for image segmentation, featuring over 1 billion masks. The model excels in zero-shot tasks, matching or surpassing supervised methods.",
      "keywords": ["Segment Anything", "image segmentation", "largest dataset", "zero-shot performance", "promptable model", "foundation models", "computer vision"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/2012.12877v2.pdf",
    "title": "Training data-efficient image transformers\n& distillation through attention",
    "abstract": "Recently, neural networks purely based on attention were shown to ad-\ndress image understanding tasks such as image classification. These high-\nperforming vision transformers are pre-trained with hundreds of millions\nof images using a large infrastructure, thereby limiting their adoption.",
    "result": {
      "summary": "The paper explores attention-based vision transformers for image classification, noting their high performance but also the extensive pre-training and infrastructure requirements limiting broader use.",
      "keywords": ["image transformers", "attention mechanisms", "image classification", "data efficiency", "infrastructure requirements"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/2010.04159v4.pdf",
    "title": "DEFORMABLE DETR: DEFORMABLE TRANSFORMERS\nFOR END-TO-END OBJECT DETECTION",
    "abstract": "DETR has been recently proposed to eliminate the need for many hand-designed\ncomponents in object detection while demonstrating good performance. However,\nit suffers from slow convergence and limited feature spatial resolution, due to the\nlimitation of Transformer attention modules in processing image feature maps. To\nmitigate these issues, we proposed Deformable DETR, whose attention modules\nonly attend to a small set of key sampling points around a reference. Deformable\nDETR can achieve better performance than DETR (especially on small objects)\nwith 10x less training epochs. Extensive experiments on the COCO benchmark\ndemonstrate the effectiveness of our approach. Code is released at https : / /\ngithub Â· com/ fundamentalvision /Deformable-DETR.",
    "result": {
      "domain": "Computer Vision",
      "problem": "Existing image segmentation tasks, models, and datasets may not adequately cover diverse visual domains or support promptable and zero-shot transfer learning.",
      "solution": "Introducing the Segment Anything (SA) project, which includes a new task, model, and dataset for image segmentation. The SA dataset, the largest segmentation dataset to date, contains over 1 billion masks on 11 million licensed and privacy-respecting images. The model, designed to be promptable, demonstrates impressive zero-shot performance, often competitive with or superior to prior fully supervised results, across numerous tasks.",
      "keywords": ["Deformable DETR", "DETR", "object detection", "transformer attention", "COCO benchmark", "efficient training", "feature spatial resolution"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/1911.03584v2.pdf",
    "title": "ON THE RELATIONSHIP BETWEEN SELF-ATTENTION\nAND CONVOLUTIONAL LAYERS",
    "abstract": "Recent trends of incorporating attention mechanisms in vision have led re-\nsearchers to reconsider the supremacy of convolutional layers as a primary build-\ning block. Beyond helping CNNs to handle long-range dependencies, Ramachan-\ndran et al. (2019) showed that attention can completely replace convolution and\nachieve state-of-the-art performance on vision tasks. This raises the question: do\nlearned attention layers operate similarly to convolutional layers? This work pro-\nvides evidence that attention layers can perform convolution and, indeed, they of-\nten learn to do so in practice. Specifically, we prove that a multi-head self-attention\nlayer with sufficient number of heads is at least as expressive as any convolutional\nlayer. Our numerical experiments then show that self-attention layers attend to\npixel-grid patterns similarly to CNN layers, corroborating our analysis. Our code\nis publicly available1",
    "result": {
      "domain": "Computer Vision",
      "problem": "Recent trends in computer vision have raised questions about the role of convolutional layers compared to attention mechanisms in modeling long-range dependencies and achieving state-of-the-art performance.",
      "solution": "Providing evidence that attention layers can perform convolution and can learn to do so in practice. This work demonstrates that a multi-head self-attention layer with a sufficient number of heads is at least as expressive as any convolutional layer. Numerical experiments show that self-attention layers attend to pixel-grid patterns similarly to CNN layers, supporting the analysis.",
      "keywords": ["self-attention", "convolutional layers", "vision tasks", "multi-head self-attention", "CNN", "expressiveness"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/2005.14165v4.pdf",
    "title": "Language Models are Few-Shot Learners",
    "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training\non a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic\nin architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of\nthousands of examples. By contrast, humans can generally perform a new language task from only\na few examples or from simple instructions - something which current NLP systems still largely\nstruggle to do. Here we show that scaling up language models greatly improves task-agnostic,\nfew-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-\ntuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion\nparameters, 10x more than any previous non-sparse language model, and test its performance in\nthe few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning,\nwith tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3\nachieves strong performance on many NLP datasets, including translation, question-answering, and\ncloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as\nunscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same\ntime, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some\ndatasets where GPT-3 faces methodological issues related to training on large web corpora. Finally,\nwe find that GPT-3 can generate samples of news articles which human evaluators have difficulty\ndistinguishing from articles written by humans. We discuss broader societal impacts of this finding\nand of GPT-3 in general.",
    "result": {
      "domain": "Natural Language Processing",
      "problem": "While current NLP systems achieve substantial gains on tasks by pre-training on large text corpora followed by fine-tuning, they still struggle to perform new language tasks from few examples or simple instructions, unlike humans.",
      "solution": "Demonstrating that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, training GPT-3, an autoregressive language model with 175 billion parameters, without any gradient updates or fine-tuning, achieves strong performance on various NLP datasets, including translation, question-answering, and cloze tasks, as well as tasks requiring on-the-fly reasoning or domain adaptation.",
      "keywords": ["GPT-3", "few-shot learning", "language models", "NLP tasks", "no fine-tuning", "task-agnostic"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/1805.00932v1.pdf",
    "title": "Exploring the Limits of\nWeakly Supervised Pretraining",
    "abstract": "Abstract. State-of-the-art visual perception models for a wide range\nof tasks rely on supervised pretraining. ImageNet classification is the de\nfacto pretraining task for these models. Yet, ImageNet is now nearly ten\nyears old and is by modern standards \"small\" Even SO, relatively little is\nknown about the behavior of pretraining with datasets that are multiple\norders of magnitude larger. The reasons are obvious: such datasets are\ndifficult to collect and annotate. In this paper, we present a unique study\nof transfer learning with large convolutional networks trained to predict\nhashtags on billions of social media images. Our experiments demon-\nstrate that training for large-scale hashtag prediction leads to excellent\nresults. We show improvements on several image classification and object\ndetection tasks, and report the highest ImageNet-1k single-crop, top-1\naccuracy to date: 85.4% (97.6% top-5). We also perform extensive ex-\nperiments that provide novel empirical data on the relationship between\nlarge-scale pretraining and transfer learning performance.",
    "result": {
      "domain": "Computer Vision",
      "problem": "State-of-the-art visual perception models rely on supervised pretraining, typically using datasets like ImageNet. However, the scalability and effectiveness of pretraining with datasets significantly larger than ImageNet remain relatively unexplored due to challenges in collecting and annotating such datasets.",
      "solution": "Presenting a unique study of transfer learning with large convolutional networks trained to predict hashtags on billions of social media images. The experiments demonstrate that training for large-scale hashtag prediction leads to excellent results, with improvements observed on several image classification and object detection tasks. The study also reports the highest ImageNet-1k single-crop, top-1 accuracy to date: 85.4% (97.6% top-5), along with providing extensive empirical data on the relationship between large-scale pretraining and transfer learning performance.",
      "keywords": ["weakly supervised pretraining", "large-scale datasets", "hashtag prediction", "image classification", "object detection", "ImageNet", "transfer learning"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/2105.15203v3.pdf",
    "title": "SegFormer: Simple and Efficient Design for Semantic\nSegmentation with Transformers",
    "abstract": "We present SegFormer, a simple, efficient yet powerful semantic segmentation\nframework which unifies Transformers with lightweight multilayer perceptron\n(MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises\na novel hierarchically structured Transformer encoder which outputs multiscale\nfeatures. It does not need positional encoding, thereby avoiding the interpolation of\npositional codes which leads to decreased performance when the testing resolution\ndiffers from training. 2) SegFormer avoids complex decoders. The proposed\nMLP decoder aggregates information from different layers, and thus combining\nboth local attention and global attention to render powerful representations. We\nshow that this simple and lightweight design is the key to efficient segmentation\non Transformers. We scale our approach up to obtain a series of models from\nSegFormer-B0 to SegFormer-B5, reaching significantly better performance and\nefficiency than previous counterparts. For example, SegFormer-B4 achieves 50.3%\nmIoU on ADE20K with 64M parameters, being 5x smaller and 2.2% better than\nthe previous best method. Our best model, SegFormer-B5, achieves 84.0% mloU on\nCityscapes validation set and shows excellent zero-shot robustness on Cityscapes-C.\nCode will be released at: gi thub.com/NVlabs /SegF ormer.",
    "result": {
      "domain": "Computer Vision",
      "problem": "Semantic segmentation frameworks often rely on complex architectures with positional encoding and intricate decoders, which can lead to decreased performance and inefficiency, especially when testing resolutions differ from training.",
      "solution": "Introducing SegFormer, a simple and efficient semantic segmentation framework that unifies Transformers with lightweight multilayer perceptron (MLP) decoders. SegFormer features a hierarchically structured Transformer encoder that outputs multiscale features without positional encoding, avoiding performance degradation due to positional code interpolation. The proposed MLP decoder aggregates information from different layers, combining local and global attention to render powerful representations. Scaling up SegFormer to various model sizes achieves significantly better performance and efficiency compared to previous methods.",
      "keywords": ["SegFormer", "semantic segmentation", "Transformers", "MLP decoder", "multi-scale features", "positional encoding", "efficient segmentation"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/2107.00652v3.pdf",
    "title": "CSWin Transformer: A General Vision Transformer Backbone with\nCross-Shaped Windows",
    "abstract": "We present CSWin Transformer, an efficient and effec-\ntive Transformer-based backbone for general-purpose vision\ntasks. A challenging issue in Transformer design is that\nglobal self-attention is very expensive to compute whereas\nlocal self-attention often limits the field of interactions of\neach token. To address this issue, we develop the Cross-\nShaped Window self-attention mechanism for computing\nself-attention in the horizontal and vertical stripes in parallel\nthat form a cross-shaped window, with each stripe obtained\nby splitting the inputfeature into stripes of equal width. We\nprovide a mathematical analysis of the effect of the stripe\nwidth and vary the stripe width for different layers of the\nTransformer network which achieves strong modeling capa-\nbility while limiting the computation cost. We also introduce\nLocally-enhanced Positional Encoding (LePE), which han-\ndles the local positional information better than existing\nencoding schemes. LePE naturally supports arbitrary input\nresolutions, and is thus especially effective and friendly for\ndownstream tasks. Incorporated with these designs and a hi-\nerarchical structure, CSWin Transformer demonstrates com-\npetitive performance on common vision tasks. Specifically,\nit achieves 85.4% Top-1 accuracy on ImageNet-1K without\nany extra training data or label, 53.9 box AP and 46.4 mask\nAP on the COCO detection task, and 52.2 mIOU on the\nADE20K semantic segmentation task, surpassing previous\nstate-of-the-art Swin Transformer backbone by +1.2, +2.0,\n+1.4, and +2.0 respectively under the similar FLOPs setting.\nBy further pretraining on the larger dataset ImageNet-21K,\nwe achieve 87.5% Top-1 accuracy on ImageNet-1K and high\nsegmentation performance on ADE20K with 55.7 mIoU.",
    "result": {
      "domain": "Computer Vision",
      "problem": "Designing efficient and effective Transformer-based backbones for vision tasks faces challenges such as the high computational cost of global self-attention and the limited field of interactions with local self-attention.",
      "solution": "Introducing CSWin Transformer, which addresses these challenges with the Cross-Shaped Window self-attention mechanism, computing self-attention in horizontal and vertical stripes that form a cross-shaped window. The stripe width is varied for different layers to balance modeling capability and computation cost. Additionally, incorporating Locally-enhanced Positional Encoding (LePE) improves handling of local positional information, supporting arbitrary input resolutions effectively. With these designs and a hierarchical structure, CSWin Transformer achieves competitive performance on common vision tasks, surpassing the previous state-of-the-art Swin Transformer backbone on ImageNet-1K, COCO detection, and ADE20K semantic segmentation tasks.",
      "keywords": ["CSWin Transformer", "Cross-Shaped Window", "self-attention", "Locally-enhanced Positional Encoding", "vision tasks", "ImageNet-1K", "COCO", "ADE20K"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/1904.09925v5.pdf",
    "title": "Attention Augmented Convolutional Networks",
    "abstract": "Convolutional networks have been the paradigm of\nchoice in many computer vision applications. The convolu-\ntion operation however has a significant weakness in that it\nonly operates on a local neighborhood, thus missing global\ninformation. Self-attention, on the other hand, has emerged\nas a recent advance to capture long range interactions, but\nhas mostly been applied to sequence modeling and gener-\native modeling tasks. In this paper, we consider the use of\nself-attentionfor discriminative visual tasks as an alterna-\ntive to convolutions. We introduce a novel two-dimensional\nrelative self-attention mechanism that proves competitive\nin replacing convolutions as a stand-alone computational\nprimitive for image classification. We find in control exper-\niments that the best results are obtained when combining\nboth convolutions and self-attention. We therefore propose\nto augment convolutional operators with this self-attention\nmechanism by concatenating convolutional feature maps\nwith a set offeature maps produced via self-attention. Ex-\ntensive experiments show that Attention Augmentation leads\nto consistent improvements in image classification on Im-\nageNet and object detection on COCO across many dif-\nferent models and scales, including ResNets and a state-\nof-the art mobile constrained network, while keeping the\nnumber of parameters similar. In particular, our method\nachieves a 1.3% top-1 accuracy improvement on ImageNet\nclassification over a ResNet50 baseline and outperforms\nother attention mechanisms for images such as Squeeze-\nand-Excitation [17]. It also achieves an improvement of\n1.4 mAP in COCO Object Detection on top of a RetinaNet\nbaseline.",
    "result":  {
      "domain": "Computer Vision",
      "problem": "Convolutional networks, while effective, have limitations in capturing global information due to their local operation. Self-attention, although capable of capturing long-range interactions, has primarily been used in sequence and generative modeling tasks, not extensively in discriminative visual tasks like image classification.",
      "solution": "Introducing Attention Augmented Convolutional Networks, which explore the use of self-attention for discriminative visual tasks as an alternative to convolutions. The proposed two-dimensional relative self-attention mechanism proves competitive in replacing convolutions for image classification tasks. By augmenting convolutional operators with self-attention, consistent improvements are observed in image classification on ImageNet and object detection on COCO across various models and scales, while maintaining a similar number of parameters. The method achieves notable accuracy improvements over baseline models like ResNet50 and outperforms other attention mechanisms such as Squeeze-and-Excitation.",
      "keywords": ["Attention Augmented Convolutional Networks", "self-attention", "convolutions", "image classification", "object detection", "ImageNet", "COCO", "global information"]
    }
  }, {
    "pdf_path": "./pdf/AI_VIT_O/2101.11986v3.pdf",
    "title": "Tokens-to- Token ViT: Training Vision Transformers from Scratch on ImageNet",
    "abstract": "Transformers, which are popular for language modeling,\nhave been explored for solving vision tasks recently, e.g.,\nthe Vision Transformer (ViT) for image classification. The\nViT model splits each image into a sequence of tokens with\nfixed length and then applies multiple Transformer layers\nto model their global relation for classification. However,\nViT achieves inferior performance to CNNs when trained\nfrom scratch on a midsize dataset like ImageNet. We find\nit is because: 1) the simple tokenization of input images\nfails to model the important local structure such as edges\nand lines among neighboring pixels, leading to low train-\ning sample efficiency; 2) the redundant attention backbone\ndesign of ViT leads to limited feature richness for fixed com-\nputation budgets and limited training samples. To overcome\nsuch limitations, we propose a new Tokens-To-Token Vi-\nsion Transformer (T2T- ViT), which incorporates 1) a layer-\nwise Tokens-to-Token (T2T) transformation to progressively\nstructurize the image to tokens by recursively aggregating\nneighboring Tokens into one Token (Tokens-to-Token), such\nthat local structure represented by surrounding tokens can\nbe modeled and tokens length can be reduced; 2) an ef-\nficient backbone with a deep-narrow structure for vision\ntransformer motivated by CNN architecture design after\nempirical study. Notably, T2T-ViT reduces the parameter\ncount and MACs of vanilla ViT by half, while achieving\nmore than 3.0% improvement when trained from scratch on\nImageNet. It also outperforms ResNets and achieves com-\nparable performance with MobileNets by directly training\non ImageNet. For example, T2T-ViT with comparable size\nto ResNet50 (21.5M parameters) can achieve 83.3% topl\naccuracy in image resolution 384x384 on ImageNet. 1",
    "result": {
      "domain": "Computer Vision",
      "problem": "Transformers like Vision Transformer (ViT) struggle to achieve competitive performance when trained from scratch on midsize datasets like ImageNet. This is due to the failure of simple tokenization methods to capture important local structures in images and the limited feature richness of ViT's attention backbone design for fixed computation budgets and training samples.",
      "solution": "Introducing Tokens-to-Token Vision Transformer (T2T-ViT), which addresses the limitations of ViT by incorporating layer-wise Tokens-to-Token (T2T) transformation to progressively structure images into tokens, capturing local structures effectively. Additionally, T2T-ViT adopts an efficient backbone with a deep-narrow structure inspired by CNN architecture design, reducing the parameter count and computational complexity while achieving more than a 3.0% improvement when trained from scratch on ImageNet. T2T-ViT outperforms ResNets and achieves comparable performance with MobileNets directly trained on ImageNet.",
      "keywords": ["Tokens-to-Token ViT", "Vision Transformer", "T2T-ViT", "image classification", "ImageNet", "local structure modeling", "transformer efficiency"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/2012.15840v3.pdf",
    "title": "Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective\nwith Transformers",
    "abstract": "Most recent semantic segmentation methods adopt\na fully-convolutional network (FCN) with an encoder-\ndecoder architecture. The encoder progressively reduces\nthe spatial resolution and learns more abstract/semantic\nvisual concepts with larger receptive fields. Since context\nmodeling is critical for segmentation, the latest efforts have\nbeen focused on increasing the receptive field, through ei-\nther dilated/atrous convolutions or inserting attention mod-\nules. However, the encoder-decoder based FCN architec-\nture remains unchanged. In this paper, we aim to provide\nan alternative perspective by treating semantic segmenta-\ntion as a sequence-to-sequence prediction task. Specifically,\nwe deploy a pure transformer (i.e., without convolution and\nresolution reduction) to encode an image as a sequence of\npatches. With the global context modeled in every layer of\nthe transformer, this encoder can be combined with a simple\ndecoder to provide a powerful segmentation model, termed\nSEgmentation TRansformer (SETR). Extensive experiments\nshow that SETR achieves new state of the art on ADE20K\n(50.28% mIoU), Pascal Context (55.83% mIoU) and com-\npetitive results on Cityscapes. Particularly, we achieve the\nfirst position in the highly competitive ADE20K test server\nleaderboard on the day of submission.",
    "result": {
      "domain": "Computer Vision",
      "problem": "Most recent semantic segmentation methods rely on fully-convolutional networks (FCNs) with encoder-decoder architectures, where the encoder reduces spatial resolution to capture abstract visual concepts with larger receptive fields. While context modeling is crucial, efforts to increase the receptive field through dilated/atrous convolutions or attention modules haven't changed the fundamental FCN architecture.",
      "solution": "Introducing Semantic Segmentation Transformer (SETR), which treats semantic segmentation as a sequence-to-sequence prediction task. SETR employs a pure transformer without convolution and resolution reduction to encode an image as a sequence of patches. With global context modeled in every layer, SETR combined with a simple decoder provides a powerful segmentation model. It achieves new state-of-the-art performance on ADE20K and Pascal Context datasets and competitive results on Cityscapes.",
      "keywords": ["SETR", "semantic segmentation", "sequence-to-sequence", "transformer", "global context", "ADE20K", "Pascal Context", "Cityscapes"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/2104.06399v2.pdf",
    "title": "Co-Scale Conv-Attentional Image Transformers",
    "abstract": "In this paper, we present Co-scale conv-attentional image\nTransformers (CoaT), a Transformer-based image classifier\nequipped with co-scale and conv-attentional mechanisms.\nFirst, the co-scale mechanism maintains the integrity of\nTransformers' encoder branches at individual scales, while\nallowing representations learned at different scales to ef-\nfectively communicate with each other; we design a series\nof serial and parallel blocks to realize the co-scale mecha-\nnism. Second, we devise a conv-attentional mechanism by\nrealizing a relative position embedding formulation in the\nfactorized attention module with an efficient convolution-like\nimplementation. CoaT empowers image Transformers with\nenriched multi-scale and contextual modeling capabilities.\nOn ImageNet, relatively small CoaT models attain superior\nclassification results compared with similar-sized convolu-\ntional neural networks and image/vision Transformers. The\neffectiveness of CoaT's backbone is also illustrated on ob-\nject detection and instance segmentation, demonstrating its\napplicability to downstream computer vision tasks.",
    "result": {
      "domain": "Computer Vision",
      "problem": "Existing Transformer-based image classifiers lack effective mechanisms for maintaining integrity at individual scales and enabling communication between representations learned at different scales. Additionally, traditional attention mechanisms have limitations in capturing relative positions efficiently.",
      "solution": "Introducing Co-scale Conv-Attentional Image Transformers (CoaT), equipped with co-scale and conv-attentional mechanisms. The co-scale mechanism maintains integrity at individual scales while enabling effective communication between representations learned at different scales. The conv-attentional mechanism utilizes a relative position embedding formulation in the factorized attention module with an efficient convolution-like implementation. CoaT enhances multi-scale and contextual modeling capabilities, achieving superior classification results on ImageNet compared to similar-sized convolutional neural networks and image Transformers. Additionally, CoaT demonstrates effectiveness in object detection and instance segmentation tasks.",
      "keywords": ["Co-scale conv-attentional image Transformers", "CoaT", "image classifier", "multi-scale interaction", "contextual modeling", "ImageNet", "object detection", "instance segmentation"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/2005.12872v3.pdf",
    "title": "End-to-End Object Detection with Transformers",
    "abstract": "Abstract. We present a new method that views object detection as a\ndirect set prediction problem. Our approach streamlines the detection\npipeline, effectively removing the need for many hand-designed compo-\nnents like a non-maximum suppression procedure or anchor generation\nthat explicitly encode our prior knowledge about the task. The main\ningredients of the new framework, called DEtection TRansformer or\nDETR, are a set-based global loss that forces unique predictions via bi-\npartite matching, and a transformer encoder-decoder architecture. Given\na fixed small set of learned object queries, DETR reasons about the re-\nlations of the objects and the global image context to directly output\nthe final set of predictions in parallel. The new model is conceptually\nsimple and does not require a specialized library, unlike many other\nmodern detectors. DETR demonstrates accuracy and run-time perfor-\nmance on par with the well-established and highly-optimized Faster R-\nCNN baseline on the challenging COCO object detection dataset. More-\nover, DETR can be easily generalized to produce panoptic segmentation\nin a unified manner. We show that it significantly outperforms com-\npetitive baselines. Training code and pretrained models are available at\nhttps: / /github. com/facebookresearch/ detr.",
    "result": {
      "domain": "Computer Vision",
      "problem": "Traditional object detection pipelines involve hand-designed components such as non-maximum suppression and anchor generation, which can be complex and cumbersome to optimize.",
      "solution": "Introducing a new method called DEtection TRansformer (DETR) that views object detection as a direct set prediction problem. DETR utilizes a set-based global loss for unique predictions via bipartite matching and employs a transformer encoder-decoder architecture. By reasoning about object relations and global image context, DETR directly outputs the final set of predictions in parallel, streamlining the detection pipeline. DETR achieves accuracy and runtime performance comparable to established detectors like Faster R-CNN on the COCO object detection dataset, while offering conceptual simplicity and ease of generalization to tasks like panoptic segmentation.",
      "keywords": ["DETR", "object detection", "transformers", "set prediction", "COCO dataset", "panoptic segmentation", "encoder-decoder architecture"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/1809.10853v3.pdf",
    "title": "ADAPTIVE INPUT REPRESENTATIONS\nFOR NEURAL LANGUAGE MODELING",
    "abstract": "We introduce adaptive input representations for neural language modeling which\nextend the adaptive softmax of Grave et al. (2017) to input representations of vari-\nable capacity. There are several choices on how to factorize the input and output\nlayers, and whether to model words, characters or sub-word units. We perform a\nsystematic comparison of popular choices for a self-attentional architecture. Our\nexperiments show that models equipped with adaptive embeddings are more than\ntwice as fast to train than the popular character input CNN while having a lower\nnumber of parameters. On the WIKITEXT-103 benchmark we achieve 18.7 per-\nplexity, an improvement of 10.5 perplexity compared to the previously best pub-\nlished result and on the BILLION WORD benchmark, we achieve 23.02 perplexity.1",
    "result": {
      "domain": "Natural Language Processing",
      "problem": "Traditional neural language models often suffer from slow training times and high parameter counts, especially when using character-based input representations.",
      "solution": "Introducing adaptive input representations for neural language modeling, extending the adaptive softmax approach to input representations of variable capacity. By systematically comparing different factorization choices for input and output layers, including words, characters, or sub-word units, the proposed method achieves faster training times and lower parameter counts compared to popular character-based CNN approaches. Experimental results on benchmarks like WIKITEXT-103 and BILLION WORD demonstrate significant improvements in perplexity scores, outperforming previous state-of-the-art results.",
      "keywords": ["adaptive input representations", "neural language modeling", "adaptive embeddings", "self-attentional architecture", "WIKITEXT-103", "BILLION WORD", "perplexity improvement"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/2103.00112v3.pdf",
    "title": "Transformer in Transformer",
    "abstract": "Transformer is a new kind of neural architecture which encodes the input data as\npowerful features via the attention mechanism. Basically, the visual transformers\nfirst divide the input images into several local patches and then calculate both\nrepresentations and their relationship. Since natural images are of high complexity\nwith abundant detail and color information, the granularity of the patch dividing is\nnot fine enough for excavating features of objects in different scales and locations.\nIn this paper, we point out that the attention inside these local patches are also\nessential for building visual transformers with high performance and we explore\na new architecture, namely, Transformer iN Transformer (TNT). Specifically, we\nregard the local patches (e.g., 16x16) as \"visual sentences\" and present to further\ndivide them into smaller patches (e.g., 4x4) as \"visual words\". The attention of\neach word will be calculated with other words in the given visual sentence with\nnegligible computational costs. Features of both words and sentences will be ag-\ngregated to enhance the representation ability. Experiments on several benchmarks\ndemonstrate the effectiveness of the proposed TNT architecture, e.g., we achieve an\n81.5% top-1 accuracy on the ImageNet, which is about 1.7% higher than that of the\nstate-of-the-art visual transformer with similar computational cost. The PyTorch\ncode is available at https : // github Â· com/huawei -noah/ CV-Backbones, and\nthe MindSpore code is available at https : / /gitee Â· com/mindspore/models/\ntree/master /research/ cv /TNT.",
    "result": {
      "domain": "Computer Vision",
      "problem": "Existing visual transformers may not effectively capture features at different scales and locations in natural images due to the granularity of patch division.",
      "solution": "Introducing a new architecture called Transformer in Transformer (TNT) to enhance the performance of visual transformers. TNT further divides local patches into smaller patches, treating them as 'visual words' and calculating attention within each patch. This allows for better feature extraction across different scales and locations. Experimental results demonstrate the effectiveness of TNT, achieving higher accuracy on benchmarks like ImageNet compared to state-of-the-art visual transformers with similar computational costs.",
      "keywords": ["Transformer in Transformer", "TNT", "visual transformers", "local patches", "attention mechanism", "ImageNet", "feature aggregation"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/2201.03545v2.pdf",
    "title": "A ConvNet for the 2020s",
    "abstract": "The \"Roaring 20s\" of visual recognition began with the\nintroduction of Vision Transformers (ViTs), which quickly\nsuperseded ConvNets as the state-of-the-art image classifica-\ntion model. A vanilla ViT, on the other hand, faces difficulties\nwhen applied to general computer vision tasks such as object\ndetection and semantic segmentation. It is the hierarchical\nTransformers (e.g., Swin Transformers) that reintroduced sev-\neral ConvNet priors, making Transformers practically viable\nas a generic vision backbone and demonstrating remarkable\nperformance on a wide variety of vision tasks. However,\nthe effectiveness of such hybrid approaches is still largely\ncredited to the intrinsic superiority of Transformers, rather\nthan the inherent inductive biases of convolutions. In this\nwork, we reexamine the design spaces and test the limits of\nwhat a pure ConvNet can achieve. We gradually \"modernize\"\na standard ResNet toward the design of a vision Transformer,\nand discover several key components that contribute to the\nperformance difference along the way. The outcome of this\nexploration is a family of pure ConvNet models dubbed Con-\nvNeXt. Constructed entirely from standard ConvNet modules,\nConvNeXts compete favorably with Transformers in terms of\naccuracy and scalability, achieving 87.8% ImageNet top-1\naccuracy and outperforming Swin Transformers on COCO\ndetection and ADE20K segmentation, while maintaining the\nsimplicity and efficiency of standard ConvNets.",
    "result": {
      "domain": "Computer Vision",
      "problem": "Vision Transformers (ViTs) quickly became the state-of-the-art image classification model but faced challenges when applied to general computer vision tasks like object detection and semantic segmentation. Although hierarchical Transformers reintroduced ConvNet priors, the effectiveness of hybrid approaches was still largely attributed to the intrinsic superiority of Transformers over convolutions.",
      "solution": "The paper explores the design space of ConvNets and creates a family of pure ConvNet models called ConvNeXt. These models compete favorably with Transformers in accuracy and scalability, achieving 87.8% ImageNet top-1 accuracy and outperforming Swin Transformers on COCO detection and ADE20K segmentation while maintaining the simplicity and efficiency of standard ConvNets.",
      "keywords": ["ConvNet", "2020s", "Vision Transformers", "ConvNeXt", "ImageNet", "COCO detection", "ADE20K segmentation", "hierarchical Transformers"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_O/2107.10224v4.pdf",
    "title": "CYCLEMLP: A MLP-LIKE ARCHITECTURE FOR\nDENSE PREDICTION",
    "abstract": "This paper presents a simple MLP-like architecture, CycleMLP, which is a versa-\ntile backbone for visual recognition and dense predictions. As compared to mod-\nern MLP architectures, e.g. , MLP-Mixer (Tolstikhin et al., 2021), ResMLP (Tou-\nvron et al., 2021a), and gMLP (Liu et al., 2021a), whose architectures are cor-\nrelated to image size and thus are infeasible in object detection and segmen-\ntation, CycleMLP has two advantages compared to modern approaches. (1) It\ncan cope with various image sizes. (2) It achieves linear computational com-\nplexity to image size by using local windows. In contrast, previous MLPs have\nO(N2) computations due to fully spatial connections. We build a family of\nmodels which surpass existing MLPs and even state-of-the-art Transformer-based\nmodels, e.g. Swin Transformer (Liu et al., 2021b), while using fewer parame-\nters and FLOPs. We expand the MLP-like models' applicability, making them\na versatile backbone for dense prediction tasks. CycleMLP achieves competi-\ntive results on object detection, instance segmentation, and semantic segmenta-\ntion. In particular, CycleMLP-Tiny outperforms Swin-Tiny by 1.3% mloU on\nADE20K dataset with fewer FLOPs. Moreover, CycleMLP also shows excel-\nlent zero-shot robustness on ImageNet-C dataset. Code is available at https :",
    "result": {
      "domain": "Computer Vision",
      "problem": "Modern MLP architectures like MLP-Mixer, ResMLP, and gMLP are often correlated with image size, making them infeasible for tasks like object detection and segmentation. Previous MLPs have computational complexities of O(N^2) due to fully spatial connections, limiting their scalability to larger image sizes.",
      "solution": "The paper introduces CycleMLP, a simple MLP-like architecture designed to be a versatile backbone for visual recognition and dense predictions. CycleMLP addresses the limitations of previous MLP architectures by being able to cope with various image sizes and achieving linear computational complexity to image size through the use of local windows. It surpasses existing MLPs and state-of-the-art Transformer-based models like Swin Transformer while using fewer parameters and FLOPs.",
      "keywords": ["CycleMLP", "MLP-like architecture", "visual recognition", "dense prediction", "object detection", "instance segmentation", "semantic segmentation", "ADE20K", "ImageNet-C", "computational complexity"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/2110.14168v2.pdf",
    "title": "Training Verifiers to Solve Math Word Problems",
    "abstract": "State-of-the-art language models can match human performance on\nmany tasks, but they still struggle to robustly perform multi-step mathe-\nmatical reasoning. To diagnose the failures of current models and support\nresearch, we introduce GSM8K, a dataset of 8.5K high quality linguisti-\ncally diverse grade school math word problems. We find that even the\nlargest transformer models fail to achieve high test performance, despite\nthe conceptual simplicity of this problem distribution. To increase per-\nformance, we propose training verifiers to judge the correctness of model\ncompletions. At test time, we generate many candidate solutions and\nselect the one ranked highest by the verifier. We demonstrate that ver-\nification significantly improves performance on GSM8K, and we provide\nstrong empirical evidence that verification scales more effectively with\nincreased data than a finetuning baseline.",
    "result": {
      "domain": "Natural Language Processing",
      "problem": "State-of-the-art language models struggle to robustly perform multi-step mathematical reasoning, even on conceptually simple problem distributions like grade school math word problems.",
      "solution": "The paper introduces GSM8K, a dataset of 8.5K high-quality linguistically diverse grade school math word problems to diagnose the failures of current models and support research in mathematical reasoning. To improve performance, the paper proposes training verifiers to judge the correctness of model completions. At test time, the model generates multiple candidate solutions, and the verifier selects the one ranked highest, significantly improving performance on GSM8K.",
      "keywords": ["GSM8K", "math word problems", "language models", "transformer models", "verifiers", "solution verification", "model performance"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1708.05031v2.pdf",
    "title": "Neural Collaborative Filtering",
    "abstract": "In recent years, deep neural networks have yielded immense\nsuccess on speech recognition, computer vision and natural\nlanguage processing. However, the exploration of deep neu-\nral networks on recommender systems has received relatively\nless scrutiny. In this work, we strive to develop techniques\nbased on neural networks to tackle the key problem in rec-\nommendation collaborative filtering on the basis of\nimplicit feedback.",
    "result": {
      "domain": "Recommender Systems",
      "problem": "Lack of exploration of deep neural networks in recommender systems compared to other domains like speech recognition, computer vision, and natural language processing.",
      "solution": "Developing techniques based on neural networks to address the collaborative filtering problem in recommendation systems using implicit feedback.",
      "keywords": ["neural collaborative filtering", "deep neural networks", "recommender systems", "implicit feedback", "collaborative filtering"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/2402.03620v1.pdf",
    "title": "SELF-DISCOVER: Large Language Models Self-Compose Reasoning Structures",
    "abstract": "We introduce SELF-DISCOVER, a general frame-\nwork for LLMs to self-discover the task-intrinsic\nreasoning structures to tackle complex reasoning\nproblems that are challenging for typical prompt-\ning methods. Core to the framework is a self-\ndiscovery process where LLMs select multiple\natomic reasoning modules such as critical think-\ning and step-by-step thinking, and compose them\ninto an explicit reasoning structure for LLMs to\nfollow during decoding. SELF-DISCOVER sub-\nstantially improves GPT-4 and PaLM 2's per-\nformance on challenging reasoning benchmarks\nsuch as BigBench-Hard, grounded agent reason-\ning, and MATH, by as much as 32% compared\nto Chain of Thought (CoT). Furthermore, SELF-\nDISCOVER outperforms inference-intensive meth-\nods such as CoT-Self-Consistency by more than\n20%, while requiring 10-40x fewer inference com-\npute. Finally, we show that the self-discovered\nreasoning structures are universally applicable\nacross model families: from PaLM 2-L to GPT-4,\nand from GPT-4 to Llama2, and share commonal-\nities with human reasoning patterns.",
    "result": {
      "domain": "Natural Language Processing",
      "problem": "Traditional prompt-based methods struggle to handle complex reasoning problems efficiently.",
      "solution": "Introducing SELF-DISCOVER, a framework for Large Language Models (LLMs) to autonomously discover task-specific reasoning structures, enabling them to tackle challenging reasoning tasks more effectively.",
      "keywords": ["SELF-DISCOVER", "Large Language Models", "reasoning structures", "performance improvement", "complex reasoning"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/2109.00122v3.pdf",
    "title": "FINQA: A Dataset of Numerical Reasoning over Financial Data",
    "abstract": "The sheer volume of financial statements\nmakes it difficult for humans to access and an-\nalyze a business's financials. Robust numeri-\ncal reasoning likewise faces unique challenges\nin this domain. In this work, we focus on\nanswering deep questions over financial data,\naiming to automate the analysis of a large cor-\npus of financial documents. In contrast to ex-\nisting tasks on general domain, the finance do-\nmain includes complex numerical reasoning\nand understanding of heterogeneous represen-\ntations. To facilitate analytical progress, we\npropose a new large-scale dataset, FINQA,\nwith Question-Answering pairs over Financial\nreports, written by financial experts. We also\nannotate the gold reasoning programs to en-\nsure full explainability. We further introduce\nbaselines and conduct comprehensive experi-\nments in our dataset. The results demonstrate\nthat popular, large, pre-trained models fall far\nshort of expert humans in acquiring finance\nknowledge and in complex multi-step numer-\nical reasoning on that knowledge. Our dataset\nthe first of its kind - should therefore en-\nable significant, new community research into\ncomplex application domains. The dataset and\ncode are publicly available1.",
    "result": {
      "domain": "Finance",
      "problem": "Accessing and analyzing financial statements manually is challenging due to their sheer volume. Additionally, robust numerical reasoning in the finance domain faces unique challenges, including complex numerical reasoning and understanding heterogeneous representations.",
      "solution": "The paper proposes a new large-scale dataset called FINQA, consisting of Question-Answering pairs over financial reports written by financial experts. The dataset includes annotated gold reasoning programs to ensure full explainability. Baselines are introduced and comprehensive experiments are conducted on the dataset. The results highlight the limitations of popular, large, pre-trained models in acquiring finance knowledge and performing complex multi-step numerical reasoning.",
      "keywords": ["FINQA", "financial data", "numerical reasoning", "dataset", "question-answering"]
    } 
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1502.05477v5.pdf",
    "title": "Trust Region Policy Optimization",
    "abstract": "We describe an iterative procedure for optimizing\npolicies, with guaranteed monotonic improve-\nment. By making several approximations to the\ntheoretically-justified procedure, we develop a\npractical algorithm, called Trust Region Policy\nOptimization (TRPO). This algorithm is similar\nto natural policy gradient methods and is effec-\ntive for optimizing large nonlinear policies such\nas neural networks. Our experiments demon-\nstrate its robust performance on a wide variety\nof tasks: learning simulated robotic swimming,\nhopping, and walking gaits; and playing Atari\ngames using images of the screen as input. De-\nspite its approximations that deviate from the\ntheory, TRPO tends to give monotonic improve-\nment, with little tuning of hyperparameters.",
    "result": {
      "domain": "Reinforcement Learning",
      "problem": "Optimizing policies for reinforcement learning tasks can be challenging, especially for large nonlinear policies like neural networks. Existing methods may lack guaranteed monotonic improvement or may require extensive hyperparameter tuning.",
      "solution": "The paper presents an iterative procedure for optimizing policies called Trust Region Policy Optimization (TRPO), which guarantees monotonic improvement. By making approximations to the theoretically-justified procedure, TRPO becomes a practical algorithm effective for optimizing large nonlinear policies such as neural networks. Experimental results demonstrate its robust performance across various tasks, including learning simulated robotic behaviors and playing Atari games.",
      "keywords": ["Trust Region Policy Optimization", "TRPO", "policy optimization", "neural networks", "robotics", "Atari games"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/2309.05653v3.pdf",
    "title": "Preprint. Work in Progress",
    "abstract": "We introduce MAmmo TH, a series of open-source large language models (LLMs)\nspecifically tailored for general math problem-solving. The MAmmoTH models are\ntrained on MathInstruct, our meticulously curated instruction tuning dataset.\nMathInstruct is compiled from 13 math datasets with intermediate rationales,\nsix of which have rationales newly curated by us. It presents a unique hybrid\nof chain-of-thought (CoT) and program-of-thought (PoT) rationales, and also en-\nsures extensive coverage of diverse fields in math. The hybrid of CoT and PoT not\nonly unleashes the potential of tool use but also allows different thought processes\nfor different math problems. As a result, the MAmmo TH series substantially outper-\nform existing open-source models on nine mathematical reasoning datasets across\nall scales with an average accuracy gain between 16% and 32%. Remarkably,\nour MAmmoTH-7B model reaches 33% on MATH (a competition-level dataset),\nwhich exceeds the best open-source 7B model (WizardMath) by 23%, and the\nMAmmoTH-34B model achieves 44% accuracy on MATH, even surpassing GPT-\n4's CoT result. Our work underscores the importance of diverse problem coverage\nand the use of hybrid rationales in developing superior math generalist models.",
    "result": {
      "domain": "Natural Language Processing",
      "problem": "Existing open-source large language models (LLMs) may not be optimized specifically for general math problem-solving, and they may lack diverse problem coverage and hybrid rationales, limiting their performance on mathematical reasoning tasks.",
      "solution": "The paper introduces MAmmoTH, a series of open-source LLMs tailored for general math problem-solving, trained on MathInstruct, a meticulously curated instruction tuning dataset. MathInstruct combines chain-of-thought (CoT) and program-of-thought (PoT) rationales, ensuring extensive coverage of diverse fields in math. The hybrid rationales enable different thought processes for different math problems, leading to substantial performance improvements over existing models on nine mathematical reasoning datasets.",
      "keywords": ["MAmmo TH", "math problem-solving", "LLMs", "MathInstruct", "hybrid rationales", "accuracy gains"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/0705.2011v1.pdf",
    "title": "Multi-Dimensional Recurrent Neural Networks",
    "abstract": "Recurrent neural networks (RNNs) have proved effective at one dimensional\nsequence learning tasks, such as speech and online handwriting recognition. Some\nof the properties that make RNNs suitable for such tasks, for example robustness\nto input warping, and the ability to access contextual information, are also desir-\nable in multidimensional domains. However, there has SO far been no direct way\nof applying RNNs to data with more than one spatio-temporal dimension. This pa-\nper introduces multi-dimensional recurrent neural networks (MDRNNs), thereby\nextending the potential applicability of RNNs to vision, video processing, medi-\ncal imaging and many other areas, while avoiding the scaling problems that have\nplagued other multi-dimensional models. Experimental results are provided for\ntwo image segmentation tasks.",
    "result": {
      "domain": "Machine Learning",
      "problem": "Recurrent neural networks (RNNs) have been effective for one-dimensional sequence learning tasks but lack a direct application to data with more than one spatio-temporal dimension. This limitation hinders their use in multidimensional domains such as vision, video processing, and medical imaging.",
      "solution": "The paper introduces multi-dimensional recurrent neural networks (MDRNNs) to extend the applicability of RNNs to multidimensional domains. MDRNNs address the scaling problems faced by other multi-dimensional models, making them suitable for various tasks including vision, video processing, and medical imaging. Experimental results are provided for image segmentation tasks to demonstrate the effectiveness of MDRNNs.",
      "keywords": ["Multi-Dimensional Recurrent Neural Networks", "MDRNNs", "RNNs", "image segmentation", "multi-dimensional data"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/2102.09672v1.pdf",
    "title": "Improved Denoising Diffusion Probabilistic Models",
    "abstract": "Denoising diffusion probabilistic models (DDPM)\nare a class of generative models which have re-\ncently been shown to produce excellent sam-\nples. We show that with a few simple modifi-\ncations, DDPMs can also achieve competitive log-\nlikelihoods while maintaining high sample quality.\nAdditionally, we find that learning variances of\nthe reverse diffusion process allows sampling with\nan order of magnitude fewer forward passes with\na negligible difference in sample quality, which\nis important for the practical deployment of these\nmodels. We additionally use precision and re-\ncall to compare how well DDPMs and GANs\ncover the target distribution. Finally, we show\nthat the sample quality and likelihood of these\nmodels scale smoothly with model capacity and\ntraining compute, making them easily scalable.\nWe release our code at https : / / github Â· com/",
    "result": {
      "domain": "Generative Models",
      "problem": "While denoising diffusion probabilistic models (DDPMs) have shown excellent sample quality, achieving competitive log-likelihoods remains a challenge. Additionally, sampling with DDPMs typically requires a high number of forward passes, which may not be practical for deployment.",
      "solution": "The paper proposes simple modifications to DDPMs to achieve competitive log-likelihoods while maintaining high sample quality. Learning variances of the reverse diffusion process enables sampling with significantly fewer forward passes without compromising sample quality, enhancing the practical deployment of these models. Precision and recall metrics are used to compare DDPMs and GANs in covering the target distribution. Furthermore, the scalability of sample quality and likelihood with model capacity and training compute is demonstrated.",
      "keywords": ["Denoising Diffusion Probabilistic Models", "DDPMs", "generative models", "sample quality", "scalability"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1311.2524v5.pdf",
    "title": "Rich feature hierarchies for accurate object detection and semantic segmentation\nTech report (v5)",
    "abstract": "Object detection performance, as measured on the\ncanonical PASCAL VOC dataset, has plateaued in the last\nfew years. The best-performing methods are complex en-\nsemble systems that typically combine multiple low-level\nimage features with high-level context. In this paper, we\npropose a simple and scalable detection algorithm that im-\nproves mean average precision (mAP) by more than 30%\nrelative to the previous best result on VOC 2012-achieving\na mAP of 53.3%. Our approach combines two key insights:\n(1) one can apply high-capacity convolutional neural net-\nworks (CNNs) to bottom-up region proposals in order to\nlocalize and segment objects and (2) when labeled training\ndata is scarce, supervised pre-training for an auxiliary task,\nfollowed by domain-specific fine-tuning, yields a significant\nperformance boost. Since we combine region proposals\nwith CNNs, we call our method R-CNN: Regions with CNN\nfeatures. We also compare R-CNN to OverFeat, a recently\nproposed sliding-window detector based on a similar CNN\narchitecture. We find that R-CNN outperforms OverFeat\nby a large margin on the 200-class ILSVRC2013 detection\ndataset. Source code for the complete system is available at\nhttp : / / www Â· CS berkeley . edu/ ~rbg/ rcnn.",
    "result": {
      "domain": "Computer Vision",
      "problem": "Object detection performance on datasets like PASCAL VOC has plateaued with existing methods, which are often complex ensemble systems combining multiple low-level image features with high-level context. Additionally, labeled training data scarcity poses challenges for performance improvement.",
      "solution": "The paper proposes a simple and scalable detection algorithm that significantly improves mean average precision (mAP) on the VOC 2012 dataset by more than 30% relative to previous methods, achieving a mAP of 53.3%. The approach combines high-capacity convolutional neural networks (CNNs) with bottom-up region proposals for object localization and segmentation. Additionally, supervised pre-training for an auxiliary task followed by domain-specific fine-tuning boosts performance, especially in data-scarce scenarios. The method, called R-CNN (Regions with CNN features), outperforms similar methods like OverFeat by a large margin on the ILSVRC2013 detection dataset.",
      "keywords": ["R-CNN", "object detection", "semantic segmentation", "CNN", "PASCAL VOC", "performance improvement"]
    }
    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1602.01783v2.pdf",
    "title": "Asynchronous Methods for Deep Reinforcement Learning",
    "abstract": "We propose a conceptually simple and\nlightweight framework for deep reinforce-\nment learning that uses asynchronous gradient\ndescent for optimization of deep neural network\ncontrollers. We present asynchronous variants of\nfour standard reinforcement learning algorithms\nand show that parallel actor-learners have a\nstabilizing effect on training allowing all four\nmethods to successfully train neural network\ncontrollers. The best performing method, an\nasynchronous variant of actor-critic, surpasses\nthe current state-of-the-art on the Atari domain\nwhile training for half the time on a single\nmulti-core CPU instead of a GPU. Furthermore,\nwe show that asynchronous actor-critic succeeds\non a wide variety of continuous motor control\nproblems as well as on a new task of navigating\nrandom 3D mazes using a visual input.",
    "result": {
      "domain": "Reinforcement Learning",
      "problem": "Training deep reinforcement learning (RL) algorithms can be computationally expensive and time-consuming, especially when using GPUs. Synchronous methods may suffer from instability during training.",
      "solution": "The paper proposes a lightweight framework for deep reinforcement learning that employs asynchronous gradient descent for optimizing deep neural network controllers. Asynchronous variants of four standard RL algorithms are presented, demonstrating that parallel actor-learners stabilize training and enable successful training of neural network controllers. The best-performing method, an asynchronous variant of actor-critic, outperforms the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Additionally, asynchronous actor-critic is successful on various continuous motor control problems and a new task of navigating random 3D mazes using visual input.",
      "keywords": ["deep reinforcement learning", "asynchronous gradient descent", "actor-critic", "Atari", "continuous control", "3D navigation"]
    }
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1312.6229v4.pdf",
    "title": "OverFeat:\nIntegrated Recognition, Localization and Detection\nusing Convolutional Networks",
    "abstract": "We present an integrated framework for using Convolutional Networks for classi-\nfication, localization and detection. We show how a multiscale and sliding window\napproach can be efficiently implemented within a ConvNet. We also introduce a\nnovel deep learning approach to localization by learning to predict object bound-\naries. Bounding boxes are then accumulated rather than suppressed in order to\nincrease detection confidence. We show that different tasks can be learned simul-\ntaneously using a single shared network. This integrated framework is the winner\nof the localization task of the ImageNet Large Scale Visual Recognition Challenge\n2013 (ILSVRC2013) and obtained very competitive results for the detection and\nclassifications tasks. In post-competition work, we establish a new state of the art\nfor the detection task. Finally, we release a feature extractor from our best model\ncalled OverFeat.",
    "result": {
      "domain": "Computer Vision",
      "problem": "Traditional methods for object recognition, localization, and detection often require separate algorithms and processing steps, leading to inefficiencies and increased complexity. Additionally, accurately predicting object boundaries is challenging.",
      "solution": "The paper presents an integrated framework for using Convolutional Networks (ConvNet) for classification, localization, and detection tasks. It introduces a multiscale and sliding window approach efficiently implemented within a ConvNet and proposes a novel deep learning approach to object localization by predicting object boundaries. Bounding boxes are accumulated rather than suppressed to increase detection confidence. Tasks like classification, localization, and detection are learned simultaneously using a single shared network. This integrated framework achieved competitive results in the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and established a new state of the art for the detection task in post-competition work. Additionally, the paper releases a feature extractor called OverFeat from the best model.",
      "keywords": ["OverFeat", "Convolutional Networks", "classification", "localization", "detection", "ILSVRC2013"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1606.05908v3.pdf",
    "title": "Tutorial on Variational Autoencoders",
    "abstract": "In just three years, Variational Autoencoders (VAEs) have emerged\nas one of the most popular approaches to unsupervised learning of\ncomplicated distributions. VAEs are appealing because they are built\non top of standard function approximators (neural networks), and\ncan be trained with stochastic gradient descent. VAEs have already\nshown promise in generating many kinds of complicated data, in-\ncluding handwritten digits [1, 2], faces [1, 3, 4], house numbers [5, 6],\nCIFAR images [6], physical models of scenes [4], segmentation [7], and\npredicting the future from static images [8]. This tutorial introduces the\nintuitions behind VAEs, explains the mathematics behind them, and\ndescribes some empirical behavior. No prior knowledge of variational\nBayesian methods is assumed.",
    "result": {
      "domain": "Machine Learning",
      "problem": "Unsupervised learning of complicated distributions can be challenging, requiring sophisticated techniques and algorithms.",
      "solution": "Variational Autoencoders (VAEs) have emerged as a popular approach to unsupervised learning of complicated distributions in just a few years. VAEs leverage standard function approximators (neural networks) and can be trained with stochastic gradient descent, making them appealing for various applications. They have shown promise in generating various kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes empirical behavior, making it accessible even for those without prior knowledge of variational Bayesian methods.",
      "keywords": ["Variational Autoencoders", "VAEs", "unsupervised learning", "neural networks", "data generation"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/2302.07577v2.pdf",
    "title": "Efficient Teacher: Semi-Supervised Object Detection for YOLOv5",
    "abstract": "Semi-Supervised Object Detection (SSOD) has been suc-\ncessful in improving the performance of both R-CNN se-\nries and anchor-free detectors. However, one-stage anchor-\nbased detectors lack the structure to generate high-quality\nor flexible pseudo labels, leading to serious inconsistency\nproblems in SSOD. In this paper, we propose the Efficient\nTeacher framework for scalable and effective one-stage\nanchor-based SSOD training, consisting of Dense Detec-\ntor, Pseudo Label Assigner, and Epoch Adaptor. Dense\nDetector is a baseline model that extends RetinaNet with\ndense sampling techniques inspired by YOLOv5. The Ef-\nficient Teacher framework introduces a novel pseudo la-\nbel assignment mechanism, named Pseudo Label Assigner,\nwhich makes more refined use of pseudo labels from Dense\nDetector. Epoch Adaptor is a method that enables a stable\nand efficient end-to-end semi-supervised training schedule\nfor Dense Detector. The Pseudo Label Assigner prevents\nthe occurrence of bias caused by a large number of low-\nquality pseudo labels that may interfere with the Dense De-\ntector during the student-teacher mutual learning mecha-\nnism, and the Epoch Adaptor utilizes domain and distribu-\ntion adaptation to allow Dense Detector to learn globally\ndistributed consistent features, making the training inde-\npendent of the proportion of labeled data. Our experiments\nshow that the Efficient Teacher framework achieves state-\nof-the-art results on VOC, COCO-standard, and COCO-\nadditional using fewer FLOPs than previous methods. To\nthe best of our knowledge, this is the first attempt to apply\nSemi-Supervised Object Detection to YOLOv5.",
    "result": {
      "domain": "Computer Vision",
      "problem": "Semi-Supervised Object Detection (SSOD) techniques have been successful for improving the performance of R-CNN series and anchor-free detectors. However, one-stage anchor-based detectors like YOLOv5 struggle to generate high-quality or flexible pseudo labels, leading to inconsistency problems in SSOD.",
      "solution": "The paper proposes the Efficient Teacher framework for scalable and effective one-stage anchor-based SSOD training, comprising Dense Detector, Pseudo Label Assigner, and Epoch Adaptor. Dense Detector extends RetinaNet with dense sampling techniques inspired by YOLOv5. The framework introduces a novel pseudo label assignment mechanism, Pseudo Label Assigner, refining the use of pseudo labels from Dense Detector. Epoch Adaptor enables stable and efficient semi-supervised training for Dense Detector by preventing bias from low-quality pseudo labels and allowing global distribution consistent feature learning. Experimental results demonstrate that the Efficient Teacher framework achieves state-of-the-art results on VOC, COCO-standard, and COCO-additional datasets while using fewer FLOPs compared to previous methods. This is the first attempt to apply Semi-Supervised Object Detection to YOLOv5.",
      "keywords": ["Efficient Teacher", "Semi-Supervised Object Detection", "YOLOv5", "pseudo labels", "anchor-based detectors"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/2305.01937v1.pdf",
    "title": "Can Large Language Models Be an Alternative to Human Evaluation?",
    "abstract": "Human evaluation is indispensable and in-\nevitable for assessing the quality of texts gen-\nerated by machine learning models or writ-\nten by humans. However, human evaluation\nis very difficult to reproduce and its quality\nis notoriously unstable, hindering fair compar-\nisons among different natural language pro-\ncessing (NLP) models and algorithms. Re-\ncently, large language models (LLMs) have\ndemonstrated exceptional performance on un-\nseen tasks when only the task instructions are\nprovided. In this paper, we explore if such an\nability of the LLMs can be used as an alter-\nnative to human evaluation. We present the\nLLMs with the exact same instructions, sam-\nples to be evaluated, and questions used to con-\nduct human evaluation, and then ask the LLMs\nto generate responses to those questions; we\ndub this LLM evaluation. We use human evalu-\nation and LLM evaluation to evaluate the texts\nin two NLP tasks: open-ended story genera-\ntion and adversarial attacks. We show that the\nresult of LLM evaluation is consistent with the\nresults obtained by expert human evaluation:\nthe texts rated higher by human experts are\nalso rated higher by the LLMs. We also find\nthat the results of LLM evaluation are stable\nover different formatting of the task instruc-\ntions and the sampling algorithm used to gen-\nerate the answer. We are the first to show the\npotential of using LLMs to assess the quality\nof texts and discuss the limitations and ethical\nconsiderations of LLM evaluation.",
    "result": {
      "domain": "Natural Language Processing",
      "problem": "Human evaluation is crucial for assessing the quality of texts generated by machine learning models or humans, but it is difficult to reproduce and prone to instability, making fair comparisons challenging.",
      "solution": "Large language models (LLMs) have demonstrated exceptional performance on unseen tasks when provided with task instructions alone. The paper explores whether LLMs can serve as an alternative to human evaluation by presenting them with the same instructions, samples, and questions used in human evaluation and asking them to generate responses. This process, termed LLM evaluation, is used to evaluate texts in two natural language processing (NLP) tasks: open-ended story generation and adversarial attacks. The results of LLM evaluation align with expert human evaluation, indicating its potential for assessing text quality. The stability of LLM evaluation results across different formatting of task instructions and sampling algorithms is also demonstrated, along with discussions on limitations and ethical considerations.",
      "keywords": ["Large Language Models", "human evaluation", "text quality assessment", "NLP tasks", "LLM evaluation"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/silver14.pdf",
    "title": "Deterministic Policy Gradient Algorithms",
    "abstract": "In this paper we consider deterministic policy\ngradient algorithms for reinforcement learning\nwith continuous actions. The deterministic pol-\nicy gradient has a particularly appealing form: it\nis the expected gradient of the action-value func-\ntion. This simple form means that the deter-\nministic policy gradient can be estimated much\nmore efficiently than the usual stochastic pol-\nicy gradient. To ensure adequate exploration,\nwe introduce an off-policy actor-critic algorithm\nthat learns a deterministic target policy from an\nexploratory behaviour policy. We demonstrate\nthat deterministic policy gradient algorithms can\nsignificantly outperform their stochastic counter-\nparts in high-dimensional action spaces.",
    "result": {
      "domain": "Reinforcement Learning",
      "problem": "Traditional stochastic policy gradient algorithms for reinforcement learning with continuous actions can be computationally inefficient.",
      "solution": "The paper explores deterministic policy gradient algorithms for reinforcement learning with continuous actions. Deterministic policy gradients have a simple and appealing form: they are the expected gradients of the action-value function, enabling more efficient estimation compared to stochastic policy gradients. To address exploration, an off-policy actor-critic algorithm is introduced, learning a deterministic target policy from an exploratory behavior policy. Experimental results demonstrate that deterministic policy gradient algorithms can significantly outperform stochastic counterparts, particularly in high-dimensional action spaces.",
      "keywords": ["deterministic policy gradient", "reinforcement learning", "continuous actions", "high-dimensional action spaces"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1811.08883v1.pdf",
    "title": "Rethinking ImageNet Pre-training",
    "abstract": "We report competitive results on object detection and in-\nstance segmentation on the COCO dataset using standard\nmodels trained from random initialization. The results\nare no worse than their ImageNet pre-training counterparts\neven when using the hyper-parameters of the baseline sys-\ntem (Mask R-CNN) that were optimized for fine-tuning pre-\ntrained models, with the sole exception of increasing the\nnumber of training iterations SO the randomly initialized\nmodels may converge. Training from random initialization\nis surprisingly robust; our results hold even when: (i) us-\ning only 10% of the training data, (ii) for deeper and wider\nmodels, and (iii) for multiple tasks and metrics. Experi-\nments show that ImageNet pre-training speeds up conver-\ngence early in training, but does not necessarily provide\nregularization or improve final target task accuracy. To\npush the envelope we demonstrate 50.9 AP on COCO ob-\nject detection without using any external data-a result on\npar with the top COCO 2017 competition results that used\nImageNet pre-training. These observations challenge the\nconventional wisdom of ImageNet pre-training for depen-\ndent tasks and we expect these discoveries will encourage\npeople to rethink the current de facto paradigm of 'pre-\ntraining and fine-tuning' in computer vision.",
    "result": {
      "domain": "Computer Vision",
      "problem": "The conventional approach to pre-training deep neural networks on the ImageNet dataset for downstream tasks like object detection and instance segmentation may not always yield significant improvements.",
      "solution": "The paper challenges the conventional wisdom of ImageNet pre-training for dependent tasks in computer vision by reporting competitive results on object detection and instance segmentation on the COCO dataset using standard models trained from random initialization. The results are comparable to or even better than their ImageNet pre-training counterparts, indicating the surprising robustness of training from random initialization. Experimental results demonstrate that ImageNet pre-training speeds up convergence early in training but may not necessarily provide regularization or improve final target task accuracy. The findings suggest a rethinking of the current paradigm of 'pre-training and fine-tuning' in computer vision.",
      "keywords": ["ImageNet pre-training", "random initialization", "object detection", "instance segmentation", "COCO dataset"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/2311.08355v2.pdf",
    "title": "Mustango: Toward Controllable Text-to-Music Generation",
    "abstract": "The quality of the text-to-music models has\nreached new heights due to recent advance-\nments in diffusion models. The controllabil-\nity of various musical aspects, however, has\nbarely been explored. In this paper, we pro-\npose Mustango: a music-domain-knowledge-\ninspired text-to-music system based on dif-\nfusion. Mustango aims to control the gen-\nerated music, not only with general text\ncaptions, but with more rich captions that\ncan include specific instructions related to\nchords, beats, tempo, and key. At the core\nof Mustango is MuNet, a Music-Domain-\nKnowledge-Informed UNet guidance module\nthat steers the generated music to include the\nmusic-specific conditions, which we predict\nfrom the text prompt, as well as the general\ntext embedding, during the reverse diffusion\nprocess. To overcome the limited availability\nof open datasets of music with text captions,\nwe propose a novel data augmentation method\nthat includes altering the harmonic, rhythmic,\nand dynamic aspects of music audio and using\nstate-of-the-art Music Information Retrieval\nmethods to extract the music features which\nwill then be appended to the existing descrip-\ntions in text format. We release the resulting\nMusicBench dataset which contains over 52K\ninstances and includes music-theory -based de-\nscriptions in the caption text. Through exten-\nsive experiments, we show that the quality of\nthe music generated by Mustango is state-of-\nthe-art, and the controllability through music-\nspecific text prompts greatly outperforms other\nmodels such as Musi cGen and AudioLDM2.",
    "result": {
      "domain": "Diffusion",
      "problem": "While recent advancements in diffusion models have led to significant improvements in text-to-music generation, the controllability of various musical aspects in generated music remains largely unexplored.",
      "solution": "The paper introduces Mustango, a text-to-music system inspired by music domain knowledge and based on diffusion models. Mustango aims to control generated music not only with general text captions but also with rich captions containing specific instructions related to chords, beats, tempo, and key. At the core of Mustango is MuNet, a Music-Domain-Knowledge-Informed UNet guidance module that incorporates music-specific conditions predicted from the text prompt and general text embeddings during the reverse diffusion process. To address the limited availability of open datasets with music and text captions, the paper proposes a novel data augmentation method and releases the MusicBench dataset containing over 52K instances with music-theory-based descriptions. Extensive experiments demonstrate that Mustango achieves state-of-the-art music quality, and its controllability through music-specific text prompts significantly outperforms other models like MusicGen and AudioLDM2.",
      "keywords": ["text-to-music generation", "Mustango", "diffusion models", "music controllability", "MuNet"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/2301.12661v1.pdf",
    "title": "Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion\nModels",
    "abstract": "Large-scale multimodal generative modeling has\ncreated milestones in text-to-image and text-to-\nvideo generation. Its application to audio still\nlags behind for two main reasons: the lack of\nlarge-scale datasets with high-quality text-audio\npairs, and the complexity of modeling long con-\ntinuous audio data. In this work, we propose\nMake-An-Audio with a prompt-enhanced diffu-\nsion model that addresses these gaps by 1) in-\ntroducing pseudo prompt enhancement with a\ndistill-then-reprogram approach, it alleviates data\nscarcity with orders of magnitude concept compo-\nsitions by using language-free audios; 2) lever-\naging spectrogram autoencoder to predict the\nself-supervised audio representation instead of\nwaveforms. Together with robust contrastive\nlanguage-audio pretraining (CLAP) representa-\ntions, Make-An-Audio achieves state-of-the-art\nresults in both objective and subjective bench-\nmark evaluation. Moreover, we present its control-\nlability and generalization for X-to-Audio with\n\"No Modality Left Behind\", for the first time\nunlocking the ability to generate high-definition,\nhigh-fidelity audios given a user-defined modality\ninput. Audio samples are available at https :\n/ / Text-to-Audio github io",
    "result": {
      "domain": "Natural Language Processing, Audio Generation",
      "problem": "While large-scale multimodal generative modeling has advanced text-to-image and text-to-video generation, text-to-audio generation still lags behind due to the lack of large-scale datasets with high-quality text-audio pairs and the complexity of modeling long continuous audio data.",
      "solution": "The paper introduces Make-An-Audio, a prompt-enhanced diffusion model aimed at addressing these gaps. It utilizes a distill-then-reprogram approach for pseudo prompt enhancement, leveraging language-free audios to alleviate data scarcity and achieve orders of magnitude concept compositions. Additionally, Make-An-Audio leverages a spectrogram autoencoder to predict self-supervised audio representations instead of waveforms. With robust contrastive language-audio pretraining (CLAP) representations, Make-An-Audio achieves state-of-the-art results in both objective and subjective benchmark evaluations. Furthermore, the paper demonstrates the controllability and generalization of Make-An-Audio for X-to-Audio generation, unlocking the ability to generate high-definition, high-fidelity audios given a user-defined modality input.",
      "keywords": ["text-to-audio generation", "Make-An-Audio", "diffusion models", "audio modeling", "spectrogram autoencoder"]
    }
    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1804.07461v3.pdf",
    "title": "GLUE: A MULTI-TASK BENCHMARK AND ANALYSIS\nPLATFORM FOR NATURAL LANGUAGE UNDERSTAND-\nING",
    "abstract": "For natural language understanding (NLU) technology to be maximally useful, it\nmust be able to process language in a way that is not exclusive to a single task,\ngenre, or dataset. In pursuit of this objective, we introduce the General Language\nUnderstanding Evaluation (GLUE) benchmark, a collection of tools for evaluat-\ning the performance of models across a diverse set of existing NLU tasks. By\nincluding tasks with limited training data, GLUE is designed to favor and encour-\nage models that share general linguistic knowledge across tasks. GLUE also in-\ncludes a hand-crafted diagnostic test suite that enables detailed linguistic analysis\nof models. We evaluate baselines based on current methods for transfer and rep-\nresentation learning and find that multi-task training on all tasks performs better\nthan training a separate model per task. However, the low absolute performance\nof our best model indicates the need for improved general NLU systems.",
    "result": {
      "domain": "Natural Language Processing",
      "problem": "Existing natural language understanding (NLU) technologies often excel at specific tasks, genres, or datasets, but lack the ability to process language in a generalized manner across various tasks.",
      "solution": "The paper introduces the General Language Understanding Evaluation (GLUE) benchmark as a collection of tools for evaluating the performance of models across a diverse set of existing NLU tasks. GLUE aims to promote models that share general linguistic knowledge across tasks, including those with limited training data. Additionally, GLUE includes a diagnostic test suite for detailed linguistic analysis of models. Baseline evaluations based on current methods for transfer and representation learning suggest that multi-task training on all tasks outperforms training separate models per task. However, the low absolute performance of the best model highlights the need for improved general NLU systems.",
      "keywords": ["GLUE benchmark", "natural language understanding", "NLU tasks", "linguistic knowledge sharing", "model evaluation"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/2211.12588v4.pdf",
    "title": "Program of Thoughts Prompting: Disentangling Computa-\ntion from Reasoning for Numerical Reasoning Tasks",
    "abstract": "Recently, there has been significant progress in teaching language models to perform step-by-\nstep reasoning to solve complex numerical reasoning tasks. Chain-of-thoughts prompting\n(CoT) is the state-of-art method for many of these tasks. CoT uses language models to\nproduce text describing reasoning, and computation, and finally the answer to a question.\nHere we propose 'Program of Thoughts' (PoT), which uses language models (mainly Codex)\nto generate text and programming language statements, and finally an answer. In PoT,\nthe computation can be delegated to a program interpreter, which is used to execute the\ngenerated program, thus decoupling complex computation from reasoning and language\nunderstanding. We evaluate PoT on five math word problem datasets and three financial-\nQA datasets in both few-shot and zero-shot settings. We find that PoT has an average\nperformance gain over CoT of around 12% across all datasets. By combining PoT with\nself-consistency decoding, we can achieve extremely strong performance on all the math\ndatasets and financial datasets. All of our data and code will be released.",
    "result": {
      "domain": "Natural Language Processing",
      "problem": "While significant progress has been made in teaching language models to perform step-by-step reasoning for solving complex numerical reasoning tasks, existing methods like Chain-of-Thoughts Prompting (CoT) still couple computation tightly with reasoning and language understanding.",
      "solution": "The paper proposes 'Program of Thoughts' (PoT) as an alternative approach, utilizing language models, mainly Codex, to generate text and programming language statements, followed by an answer. In PoT, computation is delegated to a program interpreter, decoupling complex computation from reasoning and language understanding. Evaluation on five math word problem datasets and three financial-QA datasets in both few-shot and zero-shot settings shows that PoT achieves an average performance gain of around 12% over CoT across all datasets. Additionally, combining PoT with self-consistency decoding yields extremely strong performance on all the evaluated datasets.",
      "keywords": ["Program of Thoughts", "PoT", "numerical reasoning", "Chain-of-thoughts", "language models", "performance gain"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/2309.03409v3.pdf",
    "title": "LARGE LANGUAGE MODELS AS OPTIMIZERS",
    "abstract": "Optimization is ubiquitous. While derivative-based algorithms have been powerful\ntools for various problems, the absence of gradient imposes challenges on many\nreal-world applications. In this work, we propose Optimization by PROmpting\n(OPRO), a simple and effective approach to leverage large language models (LLMs)\nas optimizers, where the optimization task is described in natural language. In\neach optimization step, the LLM generates new solutions from the prompt that\ncontains previously generated solutions with their values, then the new solutions are\nevaluated and added to the prompt for the next optimization step. We first showcase\nOPRO on linear regression and traveling salesman problems, then move on to our\nmain application in prompt optimization, where the goal is to find instructions\nthat maximize the task accuracy. With a variety of LLMs, we demonstrate that\nthe best prompts optimized by OPRO outperform human-designed prompts by\nup to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks. Code at",
    "result": {
      "domain": "Natural Language Processing",
      "problem": "Derivative-based optimization algorithms face challenges in real-world applications where gradients are absent.",
      "solution": "The paper proposes Optimization by PROmpting (OPRO) as a simple and effective approach to leverage large language models (LLMs) as optimizers. In OPRO, the optimization task is described in natural language, and in each optimization step, the LLM generates new solutions from the prompt containing previously generated solutions with their values. These new solutions are then evaluated and added to the prompt for the next optimization step. OPRO is showcased on linear regression and traveling salesman problems, as well as in prompt optimization tasks, where the goal is to find instructions maximizing task accuracy. Experimental results demonstrate that the best prompts optimized by OPRO outperform human-designed prompts by up to 8% on GSM8K and by up to 50% on Big-Bench Hard tasks.",
      "keywords": ["OPRO", "large language models", "optimization", "prompt optimization", "natural language"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1506.02640v5.pdf",
    "title": "You Only Look Once:\nUnified, Real-Time Object Detection",
    "abstract": "We present YOLO, a new approach to object detection.\nPrior work on object detection repurposes classifiers to per-\nform detection. Instead, we frame object detection as a re-\ngression problem to spatially separated bounding boxes and\nassociated class probabilities. A single neural network pre-\ndicts bounding boxes and class probabilities directly from\nfull images in one evaluation. Since the whole detection\npipeline is a single network, it can be optimized end-to-end\ndirectly on detection performance.",
    "result": {
      "domain": "Computer Vision and Machine Learning",
      "problem": "Previous approaches to object detection relied on repurposing classifiers, which could lead to inefficiencies and suboptimal performance.",
      "solution": "Introducing YOLO, a unified approach that frames object detection as a regression problem, directly predicting bounding boxes and class probabilities from full images in one evaluation using a single neural network.",
      "keywords": ["object detection", "YOLO", "neural network", "regression", "bounding boxes"]
      }  
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1507.01526v3.pdf",
    "title": "GRID LONG SHORT-TERM MEMORY",
    "abstract": "This paper introduces Grid Long Short-Term Memory, a network of LSTM cells\narranged in a multidimensional grid that can be applied to vectors, sequences or\nhigher dimensional data such as images. The network differs from existing deep\nLSTM architectures in that the cells are connected between network layers as\nwell as along the spatiotemporal dimensions of the data. The network provides\na unified way of using LSTM for both deep and sequential computation. We ap-\nply the model to algorithmic tasks such as 15-digit integer addition and sequence\nmemorization, where itis able to significantly outperform the standard LSTM. We\nthen give results for two empirical tasks. We find that 2D Grid LSTM achieves\n1.47 bits per character on the Wikipedia character prediction benchmark, which is\nstate-of-the-art among neural approaches. In addition, we use the Grid LSTM to\ndefine a novel two-dimensional translation model, the Reencoder, and show that it\noutperforms a phrase-based reference system on a Chinese-to-English translation\ntask.",
    "result": {
      "summary": "Grid LSTM introduces a novel LSTM architecture for handling vectors, sequences, and higher-dimensional data, excelling in algorithmic and empirical tasks with state-of-the-art results in character prediction and translation.",
      "keywords": ["Grid LSTM", "multidimensional data", "deep learning", "sequence computation", "translation model"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1509.02971v6.pdf",
    "title": "CONTINUOUS CONTROL WITH DEEP REINFORCEMENT\nLEARNING",
    "abstract": "We adapt the ideas underlying the success of Deep Q-Learning to the continuous\naction domain. We present an actor-critic, model-free algorithm based on the de-\nterministic policy gradient that can operate over continuous action spaces. Using\nthe same learning algorithm, network architecture and hyper-parameters, our al-\ngorithm robustly solves more than 20 simulated physics tasks, including classic\nproblems such as cartpole swing-up, dexterous manipulation, legged locomotion\nand car driving. Our algorithm is able to find policies whose performance is com-\npetitive with those found by a planning algorithm with full access to the dynamics\nof the domain and its derivatives. We further demonstrate that for many of the\ntasks the algorithm can learn policies \"end-to-end\": directly from raw pixel in-\nputs.",
    "result": {
      "domain": "Artificial Intelligence and Machine Learning",
      "problem": "Existing deep LSTM architectures lack efficient connections between network layers and along spatiotemporal dimensions of data, limiting their applicability to various tasks.",
      "solution": "Introducing Grid Long Short-Term Memory (LSTM), which arranges LSTM cells in a multidimensional grid, enabling efficient connections between network layers and along spatiotemporal dimensions of data. This architecture provides a unified framework for using LSTM in both deep and sequential computation tasks, outperforming standard LSTM models in tasks such as algorithmic tasks and sequence memorization.",
      "keywords": ["Grid LSTM", "LSTM", "deep learning", "sequence prediction", "algorithmic tasks"]
      }
  }, {
    "pdf_path": "./pdf/AI_VIT_X/NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf",
    "title": "ImageNet Classification with Deep Convolutional\nNeural Networks",
    "abstract": "We trained a large, deep convolutional neural network to classify the 1.2 million\nhigh-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif-\nferent classes. On the test data, we achieved top-1 and top-5 error rates of 37.5%\nand 17.0% which is considerably better than the previous state-of-the-art. The\nneural network, which has 60 million parameters and 650,000 neurons, consists\nof five convolutional layers, some of which are followed by max-pooling layers,\nand three fully-connected layers with a final 1000-way softmax. To make train-\ning faster, we used non-saturating neurons and a very efficient GPU implemen-\ntation of the convolution operation. To reduce overfitting in the fully-connected\nlayers we employed a recently-developed regularization method called \"dropout\"\nthat proved to be very effective. We also entered a variant of this model in the\nILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%,\ncompared to 26.2% achieved by the second-best entry.",
    "result": {
      "domain": "Computer Vision and Deep Learning",
      "problem": "Prior to this study, existing methods for classifying high-resolution images in large datasets like ImageNet were not achieving satisfactory accuracy rates.",
      "solution": "Introducing a large, deep convolutional neural network architecture specifically designed for image classification tasks. The network consists of multiple convolutional layers followed by max-pooling layers and fully-connected layers with a final softmax output. By leveraging non-saturating neurons, efficient GPU implementation, and regularization techniques like dropout, the model achieves significantly better classification accuracy compared to previous state-of-the-art methods.",
      "keywords": ["ImageNet", "convolutional neural networks", "deep learning", "classification", "dropout", "GPU implementation"]
      }
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1705.03551v2.pdf",
    "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset\nfor Reading Comprehension",
    "abstract": "We present TriviaQA, a challenging\nreading comprehension dataset contain-\ning over 650K question-answer-evidence\ntriples. TriviaQA includes 95K question-\nanswer pairs authored by trivia enthusi-\nasts and independently gathered evidence\ndocuments, S1X per question on average,\nthat provide high quality distant super-\nvision for answering the questions. We\nshow that, in comparison to other recently\nintroduced large-scale datasets, TriviaQA\n(1) has relatively complex, compositional\nquestions, (2) has considerable syntactic\nand lexical variability between questions\nand corresponding answer-evidence sen-\ntences, and (3) requires more cross sen-\ntence reasoning to find answers. We also\npresent two baseline algorithms: a feature-\nbased classifier and a state-of-the-art neu-\nral network, that performs well on SQuAD\nreading comprehension. Neither approach\ncomes close to human performance (23%\nand 40% VS. 80%), suggesting that Trivi-\naQA is a challenging testbed that is worth\nsignificant future study. 1",
    "result": {
      "domain": "Natural Language Processing and Machine Learning",
      "problem": "Existing reading comprehension datasets lack the complexity and compositional nature of real-world comprehension tasks.",
      "solution": "Introducing TriviaQA, a large-scale dataset with over 650K question-answer-evidence triples, authored by trivia enthusiasts. The dataset contains questions with considerable syntactic and lexical variability, requiring cross-sentence reasoning to find answers. Baseline algorithms, including a feature-based classifier and a neural network, perform well below human performance on TriviaQA, indicating its challenging nature and the need for further study.",
      "keywords": ["TriviaQA", "reading comprehension", "dataset", "natural language processing", "machine learning"]
      } 
  }, {
    "pdf_path": "./pdf/AI_VIT_X/2105.05233v4.pdf",
    "title": "Diffusion Models Beat GANs on Image Synthesis",
    "abstract": "We show that diffusion models can achieve image sample quality superior to the\ncurrent state-of-the-art generative models. We achieve this on unconditional im-\nage synthesis by finding a better architecture through a series of ablations. For\nconditional image synthesis, we further improve sample quality with classifier guid-\nance: a simple, compute-efficient method for trading off diversity for fidelity using\ngradients from a classifier. We achieve an FID of 2.97 on ImageNet 128 x 128,\n4.59 on ImageNet 256x256, and 7.72 on ImageNet 512x512, and we match\nBigGAN-deep even with as few as 25 forward passes per sample, all while main-\ntaining better coverage of the distribution. Finally, we find that classifier guidance\ncombines well with upsampling diffusion models, further improving FID to 3.94\non ImageNet 256x256 and 3.85 on ImageNet 512x512. We release our code at\nhttps : //github Â· com/ openai / guided-diffusion.",
    "result": {
      "domain": "Computer Vision and Generative Models",
      "problem": "Existing generative models, such as GANs, face challenges in achieving high-quality image synthesis, particularly in terms of sample quality and coverage of the distribution.",
      "solution": "Introducing diffusion models as a superior alternative for image synthesis, achieving higher sample quality compared to state-of-the-art generative models. The study presents improvements in unconditional and conditional image synthesis, including a simple and compute-efficient method called classifier guidance for trading off diversity and fidelity. By leveraging classifier gradients, the proposed models achieve impressive FID scores on ImageNet datasets, surpassing previous approaches even with fewer forward passes per sample.",
      "keywords": ["diffusion models", "GANs", "image synthesis", "classifier guidance", "FID score", "generative models"]
      }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1707.06347v2.pdf",
    "title": "Proximal Policy Optimization Algorithms",
    "abstract": "We propose a new family of policy gradient methods for reinforcement learning, which al-\nternate between sampling data through interaction with the environment, and optimizing a\n\"surrogate\" objective function using stochastic gradient ascent. Whereas standard policy gra-\ndient methods perform one gradient update per data sample, we propose a novel objective\nfunction that enables multiple epochs of minibatch updates. The new methods, which we call\nproximal policy optimization (PPO), have some of the benefits of trust region policy optimiza-\ntion (TRPO), but they are much simpler to implement, more general, and have better sample\ncomplexity (empirically). Our experiments test PPO on a collection of benchmark tasks, includ-\ning simulated robotic locomotion and Atari game playing, and we show that PPO outperforms\nother online policy gradient methods, and overall strikes a favorable balance between sample\ncomplexity, simplicity, and wall-time.",
    "result": {
      "domain": "Reinforcement Learning and Optimization Algorithms",
      "problem": "Existing policy gradient methods for reinforcement learning have limitations in terms of sample complexity and implementation complexity.",
      "solution": "Introducing Proximal Policy Optimization (PPO), a family of policy gradient methods that alternate between sampling data through interaction with the environment and optimizing a surrogate objective function using stochastic gradient ascent. PPO allows for multiple epochs of minibatch updates, combining some benefits of trust region policy optimization (TRPO) with simpler implementation and better sample complexity. Experimental results demonstrate that PPO outperforms other online policy gradient methods across various benchmark tasks.",
      "keywords": ["Proximal Policy Optimization", "policy gradient methods", "reinforcement learning", "optimization algorithms", "sample complexity", "trust region policy optimization", "benchmark tasks"]
      }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1504.08083v2.pdf",
    "title": "Fast R-CNN",
    "abstract": "This paper proposes a Fast Region-based Convolutional\nNetwork method (Fast R-CNN) for object detection. Fast\nR-CNN builds on previous work to efficiently classify ob-\nject proposals using deep convolutional networks. Com-\npared to previous work, Fast R-CNN employs several in-\nnovations to improve training and testing speed while also\nincreasing detection accuracy. Fast R-CNN trains the very\ndeep VGG16 network 9x faster than R-CNN, is 213x faster\nat test-time, and achieves a higher mAP on PASCAL VOC\n2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x\nfaster, tests 10x faster, and is more accurate. Fast R-CNN\nis implemented in Python and C++ (using Caffe) and is\navailable under the open-source MIT License at https Â·Â·",
    "result": {
      "domain": "Computer Vision and Object Detection",
      "problem": "Existing methods for object detection, such as R-CNN, suffer from inefficiencies in both training and testing speed, limiting their practical applicability.",
      "solution": "Introducing Fast Region-based Convolutional Network (Fast R-CNN) for object detection, which efficiently classifies object proposals using deep convolutional networks. Fast R-CNN improves training and testing speed while enhancing detection accuracy through several innovations. It trains the VGG16 network significantly faster than R-CNN, achieves higher mean Average Precision (mAP) on benchmark datasets like PASCAL VOC 2012, and outperforms previous methods such as SPPnet in terms of both speed and accuracy.",
      "keywords": ["Fast R-CNN", "object detection", "region-based convolutional network", "VGG16", "training speed", "testing speed", "mean Average Precision (mAP)"]
      }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1506.01497v3.pdf",
    "title": "Faster R-CNN: Towards Real-Time Object\nDetection with Region Proposal Networks",
    "abstract": "Abstract-State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations.\nAdvances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region\nproposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image\nconvolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional\nnetwork that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to\ngenerate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN\ninto a single network by sharing their convolutional features-using the recently popular terminology of neural networks with\n\"attention\" mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3],\nour detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection\naccuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO\n2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been\nmade publicly available.",
    "result": {
      "domain": "Computer Vision and Real-Time Object Detection",
      "problem": "State-of-the-art object detection networks rely on region proposal algorithms, which can be computationally expensive and slow down the overall detection process.",
      "solution": "Introducing Faster Region-based Convolutional Neural Network (Faster R-CNN) with Region Proposal Networks (RPNs), which share full-image convolutional features with the detection network, enabling nearly cost-free region proposals. The RPN predicts object bounds and objectness scores at each position, trained end-to-end to generate high-quality region proposals. By merging RPN and Fast R-CNN into a single network and leveraging attention mechanisms, Faster R-CNN achieves real-time object detection with high accuracy on benchmark datasets like PASCAL VOC and MS COCO.",
      "keywords": ["Faster R-CNN", "object detection", "region proposal networks", "real-time detection", "convolutional neural networks", "attention mechanisms", "ILSVRC", "COCO dataset"]
      }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1606.05250v3.pdf",
    "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text",
    "abstract": "We present the Stanford Question Answer-\ning Dataset (SQuAD), a new reading compre-\nhension dataset consisting of 100,000+ ques-\ntions posed by crowdworkers on a set of\nWikipedia articles, where the answer to each\nquestion is a segment of text from the cor-\nresponding reading passage. We analyze the\ndataset to understand the types of reason-\ning required to answer the questions, lean-\ning heavily on dependency and constituency\ntrees. We build a strong logistic regression\nmodel, which achieves an F1 score of 51.0%,\na significant improvement over a simple base-\nline (20%). However, human performance\n(86.8%) is much higher, indicating that the\ndataset presents a good challenge problem for\nfuture research. The dataset is freely available\nat https: / / stanford-qa.com.",
    "result": {
      "domain": "Natural Language Processing and Machine Comprehension",
      "problem": "Existing reading comprehension datasets lack sufficient diversity and size, limiting the complexity of questions and reasoning required.",
      "solution": "Introducing the Stanford Question Answer Dataset (SQuAD), a large-scale reading comprehension dataset with over 100,000 questions derived from Wikipedia articles. Each question is accompanied by a segment of text from the corresponding reading passage. The dataset facilitates analysis of the types of reasoning needed to answer questions, leaning heavily on dependency and constituency trees. Despite significant improvements over simple baselines, current models still fall short of human performance, indicating ample room for further research.",
      "keywords": ["SQuAD", "reading comprehension", "dataset", "dependency trees", "constituency trees", "logistic regression", "F1 score"]
      }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1511.06581v3.pdf",
    "title": "Dueling Network Architectures for Deep Reinforcement Learning",
    "abstract": "In recent years there have been many successes\nof using deep representations in reinforcement\nlearning. Still, many of these applications use\nconventional architectures, such as convolutional\nnetworks, LSTMs, or auto-encoders. In this pa-\nper, we present a new neural network architec-\nture for model-free reinforcement learning. Our\ndueling network represents two separate estima-\ntors: one for the state value function and one for\nthe state-dependent action advantage function.\nThe main benefit of this factoring is to general-\nize learning across actions without imposing any\nchange to the underlying reinforcement learning\nalgorithm. Our results show that this architec-\nture leads to better policy evaluation in the pres-\nence of many similar-valued actions. Moreover,\nthe dueling architecture enables our RL agent to\noutperform the state-of-the-art on the Atari 2600\ndomain.",
    "result": {
      "domain": "Reinforcement Learning and Neural Network Architectures",
      "problem": "Existing neural network architectures for reinforcement learning often lack efficient generalization across actions, especially in the presence of many similar-valued actions.",
      "solution": "Introducing the dueling network architecture for deep reinforcement learning, which represents separate estimators for the state value function and the state-dependent action advantage function. This architecture enables better policy evaluation and outperforms existing methods, particularly in environments with many similar-valued actions, as demonstrated on the Atari 2600 domain.",
      "keywords": ["dueling network architecture", "deep reinforcement learning", "state value function", "action advantage function", "policy evaluation", "Atari 2600"]
      }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1511.06391v4.pdf",
    "title": "ORDER MATTERS: SEQUENCE TO SEQUENCE FOR SETS",
    "abstract": "Sequences have become first class citizens in supervised learning thanks to the\nresurgence of recurrent neural networks. Many complex tasks that require map-\nping from or to a sequence of observations can now be formulated with the\nsequence-to-sequence (seq2seq) framework which employs the chain rule to ef-\nficiently represent the joint probability of sequences. In many cases, however,\nvariable sized inputs and/or outputs might not be naturally expressed as sequences.\nFor instance, it is not clear how to input a set of numbers into a model where the\ntask is to sort them; similarly, we do not know how to organize outputs when\nthey correspond to random variables and the task is to model their unknown joint\nprobability. In this paper, we first show using various examples that the order in\nwhich we organize input and/or output data matters significantly when learning an\nunderlying model. We then discuss an extension of the seq2seq framework that\ngoes beyond sequences and handles input sets in a principled way. In addition,\nwe propose a loss which, by searching over possible orders during training, deals\nwith the lack of structure of output sets. We show empirical evidence of our claims\nregarding ordering, and on the modifications to the seq2seq framework on bench-\nmark language modeling and parsing tasks, as well as two artificial tasks - sorting\nnumbers and estimating the joint probability of unknown graphical models.",
    "result": {
      "domain": "Machine Learning and Sequence Modeling",
      "problem": "The sequence-to-sequence (seq2seq) framework is limited in handling tasks where inputs or outputs are not naturally expressed as sequences, such as sorting sets of numbers or modeling joint probabilities of random variables.",
      "solution": "Introducing an extension of the seq2seq framework to handle sets as inputs or outputs in a principled way, acknowledging the significant impact of the order of data organization on learning underlying models. The proposed approach includes a loss function that deals with the lack of structure in output sets by searching over possible orders during training. Empirical evidence supports the effectiveness of the proposed modifications on benchmark language modeling and parsing tasks, as well as tasks like sorting numbers and estimating joint probabilities of unknown graphical models.",
      "keywords": ["sequence-to-sequence", "seq2seq", "sets", "order matters", "machine learning", "sequence modeling", "sorting", "joint probability"]
      }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1506.04834v3.pdf",
    "title": "Tree-Structured Composition in Neural Networks\nwithout Tree-Structured Architectures",
    "abstract": "Tree-structured neural networks encode a particular tree geometry for a sentence\nin the network design. However, these models have at best only slightly out-\nperformed simpler sequence-based models. We hypothesize that neural sequence\nmodels like LSTMs are in fact able to discover and implicitly use recursive com-\npositional structure, at least for tasks with clear cues to that structure in the data.\nWe demonstrate this possibility using an artificial data task for which recursive\ncompositional structure is crucial, and find an LSTM-based sequence model can\nindeed learn to exploit the underlying tree structure. However, its performance\nconsistently lags behind that of tree models, even on large training sets, suggest-\ning that tree-structured models are more effective at exploiting recursive structure.",
    "result": {
      "domain": "Natural Language Processing and Neural Networks",
      "problem": "Tree-structured neural networks, designed to encode sentence structure in a tree geometry, have only marginally outperformed simpler sequence-based models.",
      "solution": "This paper investigates whether neural sequence models like LSTMs can implicitly learn and utilize recursive compositional structure, especially in tasks with clear cues to that structure. Through experiments on an artificial data task, the study demonstrates that LSTM-based sequence models can indeed exploit underlying tree structure, but their performance consistently lags behind that of tree models. This suggests that while sequence models can discover and use recursive structure, tree-structured models are more effective at leveraging it.",
      "keywords": ["neural networks", "tree-structured composition", "LSTMs", "sequence models", "recursive compositional structure", "natural language processing"]
      }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1502.02367v4.pdf",
    "title": "Gated Feedback Recurrent Neural Networks",
    "abstract": "In this work, we propose a novel recurrent neu-\nral network (RNN) architecture. The proposed\nRNN, gated-feedback RNN (GF-RNN), extends\nthe existing approach of stacking multiple recur-\nrent layers by allowing and controlling signals\nflowing from upper recurrent layers to lower lay-\ners using a global gating unit for each pair of\nlayers. The recurrent signals exchanged between\nlayers are gated adaptively based on the previous\nhidden states and the current input. We evalu-\nated the proposed GF-RNN with different types\nof recurrent units, such as tanh, long short-term\nmemory and gated recurrent units, on the tasks\nof character-level language modeling and Python\nprogram evaluation. Our empirical evaluation of\ndifferent RNN units, revealed that in both tasks,\nthe GF-RNN outperforms the conventional ap-\nproaches to build deep stacked RNNs. We sug-\ngest that the improvement arises because the GF-\nRNN can adaptively assign different layers to dif-\nferent timescales and layer-to-layer interactions\n(including the top-down ones which are not usu-\nally present in a stacked RNN) by learning to gate\nthese interactions.",
    "result": {
      "domain": "Neural Networks and Recurrent Neural Networks",
      "problem": "Existing recurrent neural network (RNN) architectures face limitations in efficiently utilizing signals flowing between layers in stacked architectures.",
      "solution": "Introducing the gated-feedback RNN (GF-RNN), a novel RNN architecture that extends the approach of stacking multiple recurrent layers by incorporating global gating units for controlling signals flowing from upper layers to lower layers. The recurrent signals exchanged between layers are adaptively gated based on previous hidden states and current input. Empirical evaluations on tasks such as character-level language modeling and Python program evaluation demonstrate that GF-RNN outperforms conventional approaches in building deep stacked RNNs. The improvement is attributed to GF-RNN's ability to adaptively assign different layers to different timescales and layer-to-layer interactions, including top-down interactions.",
      "keywords": ["gated-feedback RNN", "recurrent neural networks", "RNN architecture", "character-level language modeling", "Python program evaluation"]
      }
  }, {
    "pdf_path": "./pdf/AI_VIT_X/BF00992698.pdf",
    "title": "Q-Learning",
    "abstract": "Abstract. Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian\ndomains. It amounts to an incremental method for dynamic programming which imposes limited computational\ndemands. It works by successively improving its evaluations of the quality of particular actions at particular states. This paper presents and proves in detail a convergence theorem for Q-learning based on that outlined in Watkins\n(1989). We show that Q-learning converges to the optimum action-values with probability 1 so long as all actions\nare repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions\nto the cases of non-discounted, but absorbing, Markov environments, and where many Q values can be changed\neach iteration, rather than just one.",
    "result": {
      "domain": "Reinforcement Learning",
      "problem": "Agents need efficient methods to learn optimal actions in controlled Markovian domains without imposing heavy computational demands.",
      "solution": "Introducing Q-learning, a simple and incremental method for dynamic programming that enables agents to learn optimal actions in controlled Markovian domains. Q-learning iteratively improves evaluations of action quality at specific states, converging to optimum action-values with high probability under certain conditions. The method imposes limited computational demands and is suitable for environments where actions are repeatedly sampled and action-values are discretely represented.",
      "keywords": ["Q-learning", "reinforcement learning", "Markovian domains", "dynamic programming", "convergence theorem"]
      }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1409.4842v1.pdf",
    "title": "Going deeper with convolutions",
    "abstract": "We propose a deep convolutional neural network architecture codenamed Incep-\ntion, which was responsible for setting the new state of the art for classification\nand detection in the ImageNet Large-Scale Visual Recognition Challenge 2014\n(ILSVRC14). The main hallmark of this architecture is the improved utilization\nof the computing resources inside the network. This was achieved by a carefully\ncrafted design that allows for increasing the depth and width of the network while\nkeeping the computational budget constant. To optimize quality, the architectural\ndecisions were based on the Hebbian principle and the intuition of multi-scale\nprocessing. One particular incarnation used in our submission for ILSVRC14 is\ncalled GoogLeNet, a 22 layers deep network, the quality of which is assessed in\nthe context of classification and detection.",
    "result": {
      "domain": "Computer Vision and Deep Learning",
      "problem": "Improving the utilization of computing resources within deep convolutional neural network architectures for classification and detection tasks.",
      "solution": "Introducing Inception, a deep convolutional neural network architecture that set a new state-of-the-art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The architecture achieves improved resource utilization by carefully increasing the depth and width of the network while maintaining a constant computational budget. Architectural decisions are guided by the Hebbian principle and the intuition of multi-scale processing. One notable implementation of Inception, called GoogLeNet, consists of 22 layers and is evaluated for its performance in classification and detection tasks.",
      "keywords": ["Inception", "convolutional neural network", "deep learning", "ILSVRC14", "classification", "detection", "GoogLeNet"]
      }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1711.09020v3.pdf",
    "title": "StarGAN: Unified Generative Adversarial Networks\nfor Multi-Domain Image-to-Image Translation",
    "abstract": "Recent studies have shown remarkable success in image-\nto-image translation for two domains. However, existing\napproaches have limited scalability and robustness in han-\ndling more than two domains, since different models should\nbe built independently for every pair of image domains. To\naddress this limitation, we propose StarGAN, a novel and\nscalable approach that can perform image-to-image trans-\nlations for multiple domains using only a single model.\nSuch a unified model architecture ofStarGAN allows simul-\ntaneous training of multiple datasets with different domains\nwithin a single network. This leads to StarGAN's superior\nquality of translated images compared to existing models as\nwell as the novel capability offlexibly translating an input\nimage to any desired target domain. We empirically demon-\nstrate the effectiveness of our approach on a facial attribute\ntransfer and a facial expression synthesis tasks.",
    "result": {
      "domain": "Computer Vision and Image Processing",
      "problem": "Existing approaches for image-to-image translation are limited in scalability and robustness when handling multiple domains, requiring separate models for each pair of image domains.",
      "solution": "Introducing StarGAN, a novel and scalable approach for image-to-image translation that can handle multiple domains using a single model. StarGAN's unified model architecture enables simultaneous training of multiple datasets with different domains within a single network, resulting in superior quality of translated images compared to existing models. Additionally, StarGAN offers the flexibility to translate an input image to any desired target domain. Empirical evaluations demonstrate the effectiveness of StarGAN on tasks such as facial attribute transfer and facial expression synthesis.",
      "keywords": ["StarGAN", "generative adversarial networks", "image-to-image translation", "multi-domain", "scalability", "facial attribute transfer", "facial expression synthesis"]
      }
    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1503.00075v3.pdf",
    "title": "Improved Semantic Representations From\nTree-Structured Long Short- Term Memory Networks",
    "abstract": "Because of their superior ability to pre-\nserve sequence information over time,\nLong Short-Term Memory (LSTM) net-\nworks, a type of recurrent neural net-\nwork with a more complex computational\nunit, have obtained strong results on a va-\nriety of sequence modeling tasks. The\nonly underlying LSTM structure that has\nbeen explored SO far is a linear chain.\nHowever, natural language exhibits syn-\ntactic properties that would naturally com-\nbine words to phrases. We introduce the\nTree-LSTM, a generalization of LSTMs to\ntree-structured network topologies. Tree-\nLSTMs outperform all existing systems\nand strong LSTM baselines on two tasks:\npredicting the semantic relatedness of two\nsentences (SemEval 2014, Task 1) and\nsentiment classification (Stanford Senti-\nment Treebank).",
    "result": {
      "domain": "Natural Language Processing and Neural Networks",
      "problem": "Existing recurrent neural network architectures, such as LSTM networks, are limited to linear chain structures, which may not effectively capture the hierarchical syntactic properties of natural language.",
      "solution": "Introducing Tree-Structured Long Short-Term Memory (Tree-LSTM) networks, a generalization of LSTM networks to tree-structured network topologies. Tree-LSTMs leverage the hierarchical structure of natural language, outperforming existing systems and strong LSTM baselines on tasks such as predicting the semantic relatedness of sentences and sentiment classification.",
      "keywords": ["Tree-LSTM", "Long Short-Term Memory", "neural networks", "natural language processing", "semantic representations", "tree structure", "sequence modeling", "sentiment classification"]
      }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/glove.pdf",
    "title": "Glo Ve: Global Vectors for Word Representation",
    "abstract": "Recent methods for learning vector space\nrepresentations of words have succeeded\nin capturing fine-grained semantic and\nsyntactic regularities using vector arith-\nmetic, but the origin of these regularities\nhas remained opaque. We analyze and\nmake explicit the model properties needed\nfor such regularities to emerge in word\nvectors. The result is a new global log-\nbilinear regression model that combines\nthe advantages of the two major model\nfamilies in the literature: global matrix\nfactorization and local context window\nmethods. Our model efficiently leverages\nstatistical information by training only on\nthe nonzero elements in a word-word CO-\noccurrence matrix, rather than on the en-\ntire sparse matrix or on individual context\nwindows in a large corpus. The model pro-\nduces a vector space with meaningful sub-\nstructure, as evidenced by its performance\nof 75% on a recent word analogy task. It\nalso outperforms related models on simi-\nlarity tasks and named entity recognition.",
    "result": {
      "domain": "Natural Language Processing and Word Embeddings",
      "problem": "Existing methods for learning word vector representations lack transparency regarding the origin of semantic and syntactic regularities observed in the vectors.",
      "solution": "Introducing GloVe (Global Vectors for Word Representation), a global log-bilinear regression model that combines the advantages of global matrix factorization and local context window methods. GloVe efficiently leverages statistical information by training only on nonzero elements in a word-word co-occurrence matrix, producing a vector space with meaningful substructure. Experimental results demonstrate GloVe's effectiveness, achieving 75% accuracy on a word analogy task and outperforming related models on similarity tasks and named entity recognition.",
      "keywords": ["GloVe", "word embeddings", "word vector representations", "semantic regularities", "syntactic regularities", "log-bilinear regression", "context window methods", "word analogy task", "similarity tasks", "named entity recognition"]
      }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/2302.04166v2.pdf",
    "title": "GPTScore: Evaluate as You Desire",
    "abstract": "Generative Artificial Intelligence (AI) has enabled\nthe development of sophisticated models that are\ncapable of producing high-caliber text, images,\nand other outputs through the utilization of large\npre-trained models. Nevertheless, assessing the\nquality of the generation is an even more ardu-\nous task than the generation itself, and this is-\nsue has not been given adequate consideration\nrecently. This paper proposes a novel evalua-\ntion framework, GPTSCORE, which utilizes the\nemergent abilities (e.g., zero-shot instruction) of\ngenerative pre-trained models to score generated\ntexts. There are 19 pre-trained models explored\nin this paper, ranging in size from 80M (e.g.,\nFLAN-T5-small) to 175B (e.g., GPT3). Exper-\nimental results on four text generation tasks, 22\nevaluation aspects, and corresponding 37 datasets\ndemonstrate that this approach can effectively\nallow us to achieve what one desires to evalu-\nate for texts simply by natural language instruc-\ntions. This nature helps us overcome several\nlong-standing challenges in text evaluation-how\nto achieve customized, multi-faceted evaluation\nwithout the need for annotated samples. We make\nour code publicly available. 1",
    "result": {
      "domain": "Natural Language Processing and Evaluation",
      "problem": "Assessing the quality of text generated by sophisticated models is challenging and often requires extensive manual annotation.",
      "solution": "Introducing GPTScore, a novel evaluation framework that utilizes generative pre-trained models' emergent abilities, such as zero-shot instruction, to score generated texts. GPTScore enables customized, multi-faceted evaluation of text without the need for annotated samples, overcoming long-standing challenges in text evaluation. Experimental results on various text generation tasks and datasets demonstrate the effectiveness of this approach.",
      "keywords": ["GPTScore", "generative artificial intelligence", "text generation", "evaluation framework", "pre-trained models", "zero-shot instruction", "text evaluation", "natural language instructions"]
      }
  }, {
    "pdf_path": "./pdf/AI_VIT_X/2109.06835v1.pdf",
    "title": "The Perils of Using Mechanical Turk\nto Evaluate Open-Ended Text Generation",
    "abstract": "Recent text generation research has increas-\ningly focused on open-ended domains such as\nstory and poetry generation. Because mod-\nels built for such tasks are difficult to evaluate\nautomatically, most researchers in the space\njustify their modeling choices by collecting\ncrowdsourced human judgments of text qual-\nity (e.g., Likert scores of coherence or gram-\nmaticality) from Amazon Mechanical Turk\n(AMT). In this paper, we first conduct a sur-\nvey of 45 open-ended text generation papers\nand find that the vast majority of them fail to\nreport crucial details about their AMT tasks,\nhindering reproducibility. We then run a se-\nries of story evaluation experiments with both\nAMT workers and English teachers and dis-\ncover that even with strict qualification fil-\nters, AMT workers (unlike teachers) fail to\ndistinguish between model-generated text and\nhuman-generated references. We show that\nAMT worker judgments improve when they\nare shown model-generated output alongside\nhuman-generated references, which enables\nthe workers to better calibrate their ratings.\nFinally, interviews with the English teachers\nprovide deeper insights into the challenges of\nthe evaluation process, particularly when rat-\ning model-generated text.",
    "result": {
      "domain": "Natural Language Processing and Evaluation",
      "problem": "Evaluating text generation models, particularly in open-ended domains like story and poetry generation, is challenging and often relies on crowdsourced human judgments. However, the reliability of such evaluations, especially when using platforms like Amazon Mechanical Turk (AMT), is questioned.",
      "solution": "Highlighting the perils of using Amazon Mechanical Turk (AMT) to evaluate open-ended text generation, this paper conducts a survey of open-ended text generation research and identifies shortcomings in reporting crucial details about AMT tasks, hindering reproducibility. Experimental results show that even with strict qualification filters, AMT workers struggle to distinguish between model-generated text and human-generated references. However, providing model-generated output alongside human references improves AMT worker judgments by enabling better calibration of ratings. Interviews with English teachers offer deeper insights into the challenges of evaluating model-generated text.",
      "keywords": ["text generation", "evaluation", "Amazon Mechanical Turk", "AMT", "crowdsourced judgments", "reproducibility", "story generation", "poetry generation", "human judgments", "calibration"]
      }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/2204.08292v1.pdf",
    "title": "StepGame: A New Benchmark for\nRobust Multi-Hop Spatial Reasoning in Texts",
    "abstract": "Inferring spatial relations in natural language is a crucial abil-\nity an intelligent system should possess. The bAbI dataset\ntries to capture tasks relevant to this domain (task 17 and 19).\nHowever, these tasks have several limitations. Most impor-\ntantly, they are limited to fixed expressions, they are limited\nin the number of reasoning steps required to solve them, and\nthey fail to test the robustness of models to input that contains\nirrelevant or redundant information. In this paper, we present\na new Question-Answering dataset called StepGame for ro-\nbust multi-hop spatial reasoning in texts. Our experiments\ndemonstrate that state-of-the-art models on the bAbI dataset\nstruggle on the StepGame dataset. Moreover, we propose a\nTensor-Product based Memory-Augmented Neural Network\n(TP-MANN) specialized for spatial reasoning tasks. Experi-\nmental results on both datasets show that our model outper-\nforms all the baselines with superior generalization and ro-\nbustness performance.",
    "result": {
      "domain": "Natural Language Processing and Spatial Reasoning",
      "problem": "Existing datasets for spatial reasoning in natural language have limitations, such as fixed expressions, a limited number of reasoning steps, and a lack of testing for robustness to irrelevant or redundant information.",
      "solution": "Introducing StepGame, a new Question-Answering dataset designed for robust multi-hop spatial reasoning in texts. StepGame addresses the limitations of existing datasets and provides challenges for state-of-the-art models. Additionally, this paper proposes a Tensor-Product based Memory-Augmented Neural Network (TP-MANN) specialized for spatial reasoning tasks. Experimental results demonstrate that the proposed model outperforms all baselines on both the StepGame dataset and existing benchmarks, showing superior generalization and robustness performance.",
      "keywords": ["StepGame", "spatial reasoning", "natural language", "Question-Answering dataset", "multi-hop reasoning", "Tensor-Product based Memory-Augmented Neural Network", "robustness"]
      }
    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/nature14236.pdf",
    "title": "Human-level control through deep reinforcement\nlearning",
    "abstract": "The theory ofreinforcement learning provides a normative account1,\ndeeply rooted in psychological2 and neuroscientific3 perspectives on\nanimal behaviour, of how agents may optimize their control of an\nenvironment. To use reinforcement learning successfully in situations\napproaching real-world complexity, however, agents are confronted\nwith a difficult task: they must derive efficient representations ofthe\nenvironment from high-dimensional sensory inputs, and use these\nto generalize past experience to new situations. Remarkably, humans\nand other animals seem to solve this problem through a harmonious\ncombination ofreinforcement learning and hierarchical sensory pro-\ncessing systems4,5, the former evidenced by a wealth of neural data\nrevealing notable parallels between the phasic signals emitted by dopa-\nminergic neurons and temporal difference reinforcement learning\nalgorithms3. While reinforcement learning agents have achieved some\nsuccesses in a variety of domains6-8, their applicability has previously\nbeen limited to domains in which useful features can be handcrafted,\nor to domains with fully observed, low-dimensional state spaces.\nHere we use recent advances in training deep neural networks9-11 to\ndevelop a novel artificial agent, termed a deep Q-network, that can\nlearn successful policies directly from high-dimensional sensory inputs\nusing end-to-end reinforcement learning. We tested this agent on\nthe challenging domain of classic Atari 2600 games12. We demon-\nstrate that the deep Q-network agent, receiving only the pixels and\nthe game score as inputs, was able to surpass the performance of all\nprevious algorithms and achieve a level comparable to that of a pro-\nfessional human games tester across a set of 49 games, using the same\nalgorithm, network architecture and hyperparameters. This work\nbridges the divide between high-dimensional sensory inputs and\nactions, resulting in the first artificial agent that is capable oflearn-\ning to excel at a diverse array of challenging tasks.",
    "result": {
      "domain": "Reinforcement Learning and Artificial Intelligence",
      "problem": "Applying reinforcement learning to real-world scenarios with high-dimensional sensory inputs poses a significant challenge, as agents must efficiently represent the environment and generalize past experiences.",
      "solution": "This paper introduces a novel artificial agent, termed a deep Q-network, which leverages recent advances in training deep neural networks to learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. The agent is tested on classic Atari 2600 games and demonstrates superior performance compared to previous algorithms, achieving a level comparable to that of a professional human games tester across a diverse set of 49 games.",
      "keywords": ["deep reinforcement learning", "deep Q-network", "Atari 2600 games", "high-dimensional sensory inputs", "end-to-end learning", "artificial intelligence"]
      }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1409.0473v7.pdf",
    "title": "NEURAL MACHINE TRANSLATION\nBY JOINTLY LEARNING TO ALIGN AND TRANSLATE",
    "abstract": "Neural machine translation is a recently proposed approach to machine transla-\ntion. Unlike the traditional statistical machine translation, the neural machine\ntranslation aims at building a single neural network that can be jointly tuned to\nmaximize the translation performance. The models proposed recently for neu-\nral machine translation often belong to a family of encoder-decoders and encode\na source sentence into a fixed-length vector from which a decoder generates a\ntranslation. In this paper, we conjecture that the use of a fixed-length vector is a\nbottleneck in improving the performance of this basic encoder-decoder architec-\nture, and propose to extend this by allowing a model to automatically (soft-)search\nfor parts of a source sentence that are relevant to predicting a target word, without\nhaving to form these parts as a hard segment explicitly. With this new approach,\nwe achieve a translation performance comparable to the existing state-of-the-art\nphrase-based system on the task of English-to-French translation. Furthermore,\nqualitative analysis reveals that the (soft-)alignments found by the model agree\nwell with our intuition.",
    "result": {
      "domain": "Natural Language Processing and Machine Translation",
      "problem": "Traditional statistical machine translation systems face limitations, prompting the development of neural machine translation (NMT). However, NMT models based on encoder-decoder architectures with fixed-length vector representations may suffer from performance bottlenecks.",
      "solution": "Introducing a novel approach to neural machine translation that extends the basic encoder-decoder architecture by allowing the model to automatically search for relevant parts of a source sentence during translation without explicitly forming hard segments. This approach achieves translation performance comparable to state-of-the-art phrase-based systems for English-to-French translation.",
      "keywords": ["neural machine translation", "encoder-decoder architecture", "alignment", "translation performance", "soft-search", "machine translation"]
      }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1810.04805v2.pdf",
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding",
    "abstract": "We introduce a new language representa-\ntion model called BERT, which stands for\nBidirectional Encoder Representations from\nTransformers. Unlike recent language repre-\nsentation models (Peters et al., 2018a; Rad-\nford et al., 2018), BERT is designed to pre-\ntrain deep bidirectional representations from\nunlabeled text by jointly conditioning on both\nleft and right context in all layers. As a re-\nsult, the pre-trained BERT model can be fine-\ntuned with just one additional output layer\nto create state-of-the-art models for a wide\nrange of tasks, such as question answering and\nlanguage inference, without substantial task-\nspecific architecture modifications.",
    "result": {
      "domain": "Natural Language Processing and Language Understanding",
      "problem": "Developing effective language representation models that capture bidirectional context from unlabeled text is essential for various natural language processing tasks.",
      "solution": "Introducing BERT, a language representation model that pre-trains deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. BERT can be fine-tuned with just one additional output layer to create state-of-the-art models for various tasks, including question answering and language inference, without significant task-specific architecture modifications.",
      "keywords": ["BERT", "language representation model", "deep bidirectional transformers", "pre-training", "unlabeled text", "fine-tuning", "question answering", "language inference"]
      }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/2303.16634v3.pdf",
    "title": "G-EVAL: NLG Evaluation using GPT-4 with Better Human Alignment",
    "abstract": "The quality of texts generated by natural lan-\nguage generation (NLG) systems is hard to\nmeasure automatically. Conventional reference-\nbased metrics, such as BLEU and ROUGE,\nhave been shown to have relatively low cor-\nrelation with human judgments, especially for\ntasks that require creativity and diversity. Re-\ncent studies suggest using large language mod-\nels (LLMs) as reference-free metrics for NLG\nevaluation, which have the benefit of being ap-\nplicable to new tasks that lack human refer-\nences. However, these LLM-based evaluators\nstill have lower human correspondence than\nmedium-size neural evaluators. In this work,\nwe present G-EVAL, a framework of using\nlarge language models with chain-of-thoughts\n(CoT) and a form-filling paradigm, to assess the\nquality of NLG outputs. We experiment with\ntwo generation tasks, text summarization and\ndialogue generation. We show that G-EVAL\nwith GPT-4 as the backbone model achieves a\nSpearman correlation of 0.514 with human on\nsummarization task, outperforming all previous\nmethods by a large margin. We also propose\nanalysis on the behavior of LLM-based eval-\nuators, and highlight the potential concern of\nLLM-based evaluators having a bias towards\nthe LLM-generated texts. 1",
    "result": {
      "domain": "Natural Language Generation and Evaluation",
      "problem": "Measuring the quality of texts generated by natural language generation (NLG) systems is challenging, especially for tasks requiring creativity and diversity. Conventional reference-based metrics have low correlation with human judgments, and large language models (LLMs) used as reference-free metrics still have lower human correspondence compared to medium-size neural evaluators.",
      "solution": "Introducing G-EVAL, a framework for NLG evaluation using large language models with chain-of-thoughts (CoT) and a form-filling paradigm. G-EVAL, with GPT-4 as the backbone model, achieves a high Spearman correlation with human judgments on summarization tasks, outperforming previous methods by a significant margin.",
      "keywords": ["G-EVAL", "natural language generation", "NLG evaluation", "large language models", "GPT-4", "Spearman correlation", "evaluation metrics"]
      }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/2010.02502v4.pdf",
    "title": "DENOISING DIFFUSION IMPLICIT MODELS",
    "abstract": "Denoising diffusion probabilistic models (DDPMs) have achieved high qual-\nity image generation without adversarial training, yet they require simulating a\nMarkov chain for many steps in order to produce a sample. To accelerate sam-\npling, we present denoising diffusion implicit models (DDIMs), a more efficient\nclass of iterative implicit probabilistic models with the same training procedure as\nDDPMs. In DDPMs, the generative process is defined as the reverse of a particular\nMarkovian diffusion process. We generalize DDPMs via a class of non-Markovian\ndiffusion processes that lead to the same training objective. These non-Markovian\nprocesses can correspond to generative processes that are deterministic, giving rise\nto implicit models that produce high quality samples much faster. We empirically\ndemonstrate that DDIMs can produce high quality samples 10x to 50x faster in\nterms of wall-clock time compared to DDPMs, allow us to trade off computation\nfor sample quality, perform semantically meaningful image interpolation directly\nin the latent space, and reconstruct observations with very low error.",
    "result": {
      "domain": "Generative Modeling and Image Generation",
      "problem": "Denoising diffusion probabilistic models (DDPMs) achieve high-quality image generation but require simulating a Markov chain for many steps to produce a sample, leading to slow sampling times.",
      "solution": "Introducing denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. DDIMs generalize DDPMs via non-Markovian diffusion processes, enabling faster sample generation without compromising quality.",
      "keywords": ["DDIMs", "DDPMs", "generative modeling", "image generation", "probabilistic models", "implicit models", "diffusion processes"]
      }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1310.4546v1.pdf",
    "title": "Distributed Representations of Words and Phrases\nand their Compositionality",
    "abstract": "The recently introduced continuous Skip-gram model is an efficient method for\nlearning high-quality distributed vector representations that capture a large num-\nber of precise syntactic and semantic word relationships. In this paper we present\nseveral extensions that improve both the quality of the vectors and the training\nspeed. By subsampling of the frequent words we obtain significant speedup and\nalso learn more regular word representations. We also describe a simple alterna-\ntive to the hierarchical softmax called negative sampling.\nAn inherent limitation of word representations is their indifference to word order\nand their inability to represent idiomatic phrases. For example, the meanings of\n\"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\" Â· Motivated\nby this example, we present a simple method for finding phrases in text, and show",
    "result": {
      "domain": "Natural Language Processing (NLP)",
      "problem": "Word representations lack the ability to capture word order and represent idiomatic phrases.",
      "solution": "Introducing extensions to the continuous Skip-gram model, such as subsampling of frequent words and negative sampling, to improve the quality of distributed vector representations. Additionally, proposing a simple method for finding phrases in text to address the limitation of word representations.",
      "keywords": ["continuous Skip-gram model", "word representations", "distributed representations", "phrase compositionality", "NLP"]
      }  
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1312.6114v11.pdf",
    "title": "Auto-Encoding Variational Bayes",
    "abstract": "How can we perform efficient inference and learning in directed probabilistic\nmodels, in the presence of continuous latent variables with intractable posterior\ndistributions, and large datasets? We introduce a stochastic variational inference\nand learning algorithm that scales to large datasets and, under some mild differ-\nentiability conditions, even works in the intractable case. Our contributions are\ntwo-fold. First, we show that a reparameterization of the variational lower bound\nyields a lower bound estimator that can be straightforwardly optimized using stan-\ndard stochastic gradient methods. Second, we show that for i.i.d. datasets with\ncontinuous latent variables per datapoint, posterior inference can be made espe-\ncially efficient by fitting an approximate inference model (also called a recogni-\ntion model) to the intractable posterior using the proposed lower bound estimator.\nTheoretical advantages are reflected in experimental results.",
    "result": {
      "domain": "Machine Learning",
      "problem": "Performing efficient inference and learning in directed probabilistic models with continuous latent variables and intractable posterior distributions, especially with large datasets.",
      "solution": "Introducing a stochastic variational inference and learning algorithm that scales to large datasets and works even in the presence of intractable posterior distributions. This algorithm involves reparameterization of the variational lower bound and fitting an approximate inference model to the intractable posterior.",
      "keywords": ["stochastic variational inference", "reparameterization", "intractable posterior", "continuous latent variables", "large datasets", "machine learning"]
      }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/2006.11239v2.pdf",
    "title": "Denoising Diffusion Probabilistic Models",
    "abstract": "We present high quality image synthesis results using diffusion probabilistic models,\na class of latent variable models inspired by considerations from nonequilibrium\nthermodynamics. Our best results are obtained by training on a weighted variational\nbound designed according to a novel connection between diffusion probabilistic\nmodels and denoising score matching with Langevin dynamics, and our models nat-\nurally admit a progressive lossy decompression scheme that can be interpreted as a\ngeneralization of autoregressive decoding. On the unconditional CIFAR10 dataset,\nwe obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On\n256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our imple-\nmentation is available at https : / / github Â· com/hoj onathanho/diffusion.",
    "result": {
      "domain": "Machine Learning",
      "problem": "Synthesizing high-quality images using latent variable models.",
      "solution": "Introducing diffusion probabilistic models, a class of latent variable models inspired by nonequilibrium thermodynamics. These models utilize a weighted variational bound and a connection with denoising score matching with Langevin dynamics. Additionally, they support a progressive lossy decompression scheme, resembling autoregressive decoding.",
      "keywords": ["diffusion probabilistic models", "latent variable models", "image synthesis", "variational bound", "Langevin dynamics", "progressive decoding", "machine learning"]
      }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/nature16961.pdf",
    "title": "Mastering the game of Go with deep\nneural networks and tree search",
    "abstract": "The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its\nenormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach\nto computer Go that uses 'value networks' to evaluate board positions and 'policy networks' to select moves. These deep\nneural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement\nlearning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-\nof-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a\nnew search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm,\nour program AlphaGo achieved a 99. 8% winning rate against other Go programs, and defeated the human European Go\nchampion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the\nfull -sized game of Go, a feat previously thought to be at least a decade away.",
    "result": {
      "domain": "Game AI, Machine Learning",
      "problem": "The game of Go poses challenges for AI due to its vast search space and the difficulty of evaluating board positions and moves.",
      "solution": "Introducing a new approach that uses deep neural networks (value networks and policy networks) trained through a combination of supervised learning from human expert games and reinforcement learning from self-play. Additionally, a novel search algorithm combining Monte Carlo simulation with value and policy networks is introduced.",
      "keywords": ["Go", "deep neural networks", "tree search", "value networks", "policy networks", "supervised learning", "reinforcement learning", "Monte Carlo simulation", "AlphaGo"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1406.2661v1.pdf",
    "title": "Generative Adversarial Nets",
    "abstract": "We propose a new framework for estimating generative models via an adversar-\nial process, in which we simultaneously train two models: a generative model G\nthat captures the data distribution, and a discriminative model D that estimates\nthe probability that a sample came from the training data rather than G. The train-\ning procedure for G is to maximize the probability of D making a mistake. This\nframework corresponds to a minimax two-player game. In the space of arbitrary\nfunctions G and D, a unique solution exists, with G recovering the training data\ndistribution and D equal to 1/2 everywhere. In the case where G and D are defined\nby multilayer perceptrons, the entire system can be trained with backpropagation.\nThere is no need for any Markov chains or unrolled approximate inference net-\nworks during either training or generation of samples. Experiments demonstrate\nthe potential of the framework through qualitative and quantitative evaluation of\nthe generated samples.",
    "result": {
      "domain": "Machine Learning, Generative Models",
      "problem": "Traditional generative models face challenges in accurately capturing complex data distributions.",
      "solution": "Introducing a novel framework called Generative Adversarial Nets (GANs), where two models, a generative model (G) and a discriminative model (D), are trained simultaneously in a minimax two-player game. The training objective for G is to maximize the probability of D making a mistake. This framework allows G to capture the data distribution without the need for Markov chains or unrolled approximate inference networks.",
      "keywords": ["Generative Adversarial Nets", "GANs", "generative model", "discriminative model", "minimax game", "backpropagation", "multilayer perceptrons"]
    }        
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1406.1078v3.pdf",
    "title": "Learning Phrase Representations using RNN Encoder-Decoder\nfor Statistical Machine Translation",
    "abstract": "In this paper, we propose a novel neu-\nral network model called RNN Encoder-\nDecoder that consists of two recurrent\nneural networks (RNN). One RNN en-\ncodes a sequence of symbols into a fixed-\nlength vector representation, and the other\ndecodes the representation into another se-\nquence of symbols. The encoder and de-\ncoder of the proposed model are jointly\ntrained to maximize the conditional prob-\nability of a target sequence given a source\nsequence. The performance of a statisti-\ncal machine translation system is empiri-\ncally found to improve by using the con-\nditional probabilities of phrase pairs com-\nputed by the RNN Encoder-Decoder as an\nadditional feature in the existing log-linear\nmodel. Qualitatively, we show that the\nproposed model learns a semantically and\nsyntactically meaningful representation of\nlinguistic phrases.",
    "result": {
      "domain": "Natural Language Processing, Machine Translation",
      "problem": "Traditional statistical machine translation systems lack effective methods for learning phrase representations and capturing semantic and syntactic features of linguistic phrases.",
      "solution": "Introducing a novel neural network model called RNN Encoder-Decoder, consisting of two recurrent neural networks (RNNs). One RNN encodes a sequence into a fixed-length vector representation, while the other decodes the representation into another sequence. These components are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder are used as additional features in the existing log-linear model, leading to improved translation performance. Qualitatively, the proposed model learns semantically and syntactically meaningful representations of linguistic phrases.",
      "keywords": ["RNN Encoder-Decoder", "neural network model", "machine translation", "statistical machine translation", "phrase representations", "conditional probability", "semantic representation", "syntactic representation"]
    }        
  }, {
    "pdf_path": "./pdf/AI_VIT_X/P02-1040.pdf",
    "title": "BLEU: a Method for Automatic Evaluation of Machine Translation",
    "abstract": "Human evaluations of machine translation\nare extensive but expensive. Human eval-\nuations can take months to finish and in-\nvolve human labor that can not be reused.\nWe propose a method of automatic ma-\nchine translation evaluation that is quick,\ninexpensive, and language-independent,\nthat correlates highly with human evalu-\nation, and that has little marginal cost per\nrun. We present this method as an auto-\nmated understudy to skilled human judges\nwhich substitutes for them when there is\nneed for quick or frequent evaluations. 1",
    "result": {
      "domain": "Natural Language Processing, Machine Translation",
      "problem": "Human evaluations of machine translation are time-consuming, expensive, and require human labor that cannot be easily reused. Conducting extensive human evaluations can take months to finish.",
      "solution": "Introducing a method of automatic machine translation evaluation called BLEU (Bilingual Evaluation Understudy), which is quick, inexpensive, and language-independent. BLEU correlates highly with human evaluation and has little marginal cost per run. It serves as an automated substitute for skilled human judges, providing quick or frequent evaluations when needed.",
      "keywords": ["BLEU", "automatic evaluation", "machine translation", "human evaluation", "language-independent", "correlation", "cost-effective"]
    }        
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1411.4555v2.pdf",
    "title": "Show and Tell: A Neural Image Caption Generator",
    "abstract": "Automatically describing the content of an image is a\nfundamental problem in artificial intelligence that connects\ncomputer vision and natural language processing. In this\npaper, we present a generative model based on a deep re-\ncurrent architecture that combines recent advances in com-\nputer vision and machine translation and that can be used\nto generate natural sentences describing an image. The\nmodel is trained to maximize the likelihood of the target de-\nscription sentence given the training image. Experiments\non several datasets show the accuracy of the model and the\nfluency of the language it learns solely from image descrip-\ntions. Our model is often quite accurate, which we verify\nboth qualitatively and quantitatively. For instance, while\nthe current state-of-the-art BLEU-1 score (the higher the\nbetter) on the Pascal dataset is 25, our approach yields 59,\nto be compared to human performance around 69. We also\nshow BLEU-1 score improvements on Flickr30k, from 56 to\n66, and on SBU, from 19 to 28. Lastly, on the newly released\nCOCO dataset, we achieve a BLEU-4 of27.7, which is the\ncurrent state-of-the-art.",
    "result": {
      "domain": "Computer Vision, Natural Language Processing",
      "problem": "Automatically describing the content of an image requires connecting computer vision with natural language processing. Existing methods may not accurately generate natural sentences describing images.",
      "solution": "Introducing a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation. The model is trained to maximize the likelihood of the target description sentence given the training image.",
      "keywords": ["image captioning", "generative model", "deep recurrent architecture", "computer vision", "natural language processing", "BLEU score", "accuracy", "fluency"]
    }        
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1409.3215v3.pdf",
    "title": "Sequence to Sequence Learning\nwith Neural Networks",
    "abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excel-\nlent performance on difficult learning tasks. Although DNNs work well whenever\nlarge labeled training sets are available, they cannot be used to map sequences to\nsequences. In this paper, we present a general end-to-end approach to sequence\nlearning that makes minimal assumptions on the sequence structure. Our method\nuses a multilayered Long Short-Term Memory (LSTM) to map the input sequence\nto a vector of a fixed dimensionality, and then another deep LSTM to decode the\ntarget sequence from the vector. Our main result is that on an English to French\ntranslation task from the WMT' 14 dataset, the translations produced by the LSTM\nachieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU\nscore was penalized on out-of-vocabulary words. Additionally, the LSTM did not\nhave difficulty on long sentences. For comparison, a phrase-based SMT system\nachieves a BLEU score of 33.3 on the same dataset. When we used the LSTM\nto rerank the 1000 hypotheses produced by the aforementioned SMT system, its\nBLEU score increases to 36.5, which is close to the previous best result on this\ntask. The LSTM also learned sensible phrase and sentence representations that\nare sensitive to word order and are relatively invariant to the active and the pas-\nsive voice. Finally, we found that reversing the order of the words in all source\nsentences (but not target sentences) improved the LSTM's performance markedly,\nbecause doing SO introduced many short term dependencies between the source\nand the target sentence which made the optimization problem easier.",
    "result": {
      "domain": "Sequence-to-sequence learning, Neural Networks, Natural Language Processing",
      "problem": "Deep Neural Networks (DNNs) cannot be used effectively to map sequences to sequences.",
      "solution": "Introducing an end-to-end approach to sequence learning using multilayered Long Short-Term Memory (LSTM) networks to map input sequences to fixed-dimensional vectors and decode target sequences from these vectors.",
      "keywords": ["sequence-to-sequence learning", "neural networks", "Long Short-Term Memory (LSTM)", "translation task", "BLEU score", "phrase-based SMT system"]
    }      
  }, {
    "pdf_path": "./pdf/AI_VIT_X/2112.10752v2.pdf",
    "title": "High-Resolution Image Synthesis with Latent Diffusion Models",
    "abstract": "By decomposing the image formation process into a se-\nquential application of denoising autoencoders, diffusion\nmodels (DMs) achieve state-of-the-art synthesis results on\nimage data and beyond. Additionally, their formulation al-\nlows for a guiding mechanism to control the image gen-\neration process without retraining. However, since these\nmodels typically operate directly in pixel space, optimiza-\ntion of powerful DMs often consumes hundreds of GPU\ndays and inference is expensive due to sequential evalu-\nations. To enable DM training on limited computational\nresources while retaining their quality and flexibility, we\napply them in the latent space of powerful pretrained au-\ntoencoders. In contrast to previous work, training diffusion\nmodels on such a representation allows for the first time\nto reach a near-optimal point between complexity reduc-\ntion and detail preservation, greatly boosting visual fidelity.\nBy introducing cross-attention layers into the model archi-\ntecture, we turn diffusion models into powerful and flexi-\nble generators for general conditioning inputs such as text\nor bounding boxes and high-resolution synthesis becomes\npossible in a convolutional manner. Our latent diffusion\nmodels (LDMs) achieve new state-of-the-art scores for im-\nage inpainting and class-conditional image synthesis and\nhighly competitive performance on various tasks, includ-\ning text-to-image synthesis, unconditional image generation\nand super-resolution, while significantly reducing computa-\ntional requirements compared to pixel-based DMs.",
    "result": {
      "domain": "Image synthesis, Latent diffusion models, Autoencoders",
      "problem": "Training powerful diffusion models directly in pixel space requires extensive computational resources and inference is expensive.",
      "solution": "Utilizing latent space of pretrained autoencoders for training diffusion models to reduce computational requirements while retaining quality and flexibility. Introducing cross-attention layers to enable conditioning inputs such as text or bounding boxes and achieve high-resolution synthesis in a convolutional manner.",
      "keywords": ["image synthesis", "diffusion models", "latent space", "autoencoders", "cross-attention layers", "high-resolution synthesis"]
    }        
  }, {
    "pdf_path": "./pdf/AI_VIT_X/2104.05832v1.pdf",
    "title": "SPARTQA: A Textual Question Answering Benchmark\nfor Spatial Reasoning",
    "abstract": "This paper proposes a question-answering\n(QA) benchmark for spatial reasoning on nat-\nural language text which contains more real-\nistic spatial phenomena not covered by prior\nwork and is challenging for state-of-the-art\nlanguage models (LM). We propose a distant\nsupervision method to improve on this task.\nSpecifically, we design grammar and reason-\ning rules to automatically generate a spatial de-\nscription of visual scenes and corresponding\nQA pairs. Experiments show that further pre-\ntraining LMs on these automatically generated\ndata significantly improves LMs' capability on\nspatial understanding, which in turn helps to\nbetter solve two external datasets, bAbI, and\nboolQ. We hope that this work can foster inves-\ntigations into more sophisticated models for\nspatial reasoning over text.",
    "result": {
      "domain": "Textual question answering, Spatial reasoning, Benchmarking",
      "problem": "Existing benchmarks for spatial reasoning in textual question answering lack coverage of realistic spatial phenomena and pose challenges for state-of-the-art language models.",
      "solution": "Proposing a new benchmark, SPARTQA, which includes more realistic spatial phenomena and is challenging for current language models. Utilizing distant supervision to improve the benchmark by automatically generating spatial descriptions of visual scenes and corresponding question-answer pairs.",
      "keywords": ["textual question answering", "spatial reasoning", "benchmark", "language models", "distant supervision"]
    }        
  }, {
    "pdf_path": "./pdf/AI_VIT_X/2210.09261v1.pdf",
    "title": "Challenging BIG-Bench tasks and\nwhether chain-of-thought can solve them",
    "abstract": "BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that focuses on tasks believed to be beyond\nthe capabilities of current language models. Language models have already made good progress on this\nbenchmark, with the best model in the BIG-Bench paper outperforming average reported human-rater results\non 65% of the BIG-Bench tasks via few-shot prompting. But on what tasks do language models fall short of\naverage human-rater performance, and are those tasks actually unsolvable by current language models?",
    "result": {
      "domain": "Natural Language Processing, Evaluation, Language Models",
      "problem": "Identifying tasks in the BIG-Bench evaluation suite where language models fail to achieve average human-rater performance and determining whether these tasks are truly unsolvable by current language models.",
      "solution": "Investigating the performance of language models on challenging tasks from the BIG-Bench evaluation suite and assessing the potential of chain-of-thought approach in solving these tasks.",
      "keywords": ["BIG-Bench", "evaluation suite", "language models", "unsolvable tasks", "chain-of-thought", "performance assessment"]
    }      
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1301.3781v3.pdf",
    "title": "Efficient Estimation of Word Representations in\nVector Space",
    "abstract": "We propose two novel model architectures for computing continuous vector repre-\nsentations of words from very large data sets. The quality of these representations\nis measured in a word similarity task, and the results are compared to the previ-\nously best performing techniques based on different types of neural networks. We\nobserve large improvements in accuracy at much lower computational cost, i.e. it\ntakes less than a day to learn high quality word vectors from a 1.6 billion words\ndata set. Furthermore, we show that these vectors provide state-of-the-art perfor-\nmance on our test set for measuring syntactic and semantic word similarities.",
    "result": {
      "domain": "Natural Language Processing, Word Embeddings",
      "problem": "Efficiently computing continuous vector representations of words from very large datasets.",
      "solution": "Introducing two novel model architectures that achieve high-quality word representations with significant improvements in accuracy compared to previous techniques, while requiring much lower computational cost.",
      "keywords": ["word representations", "vector space", "efficient estimation", "word similarity", "neural networks", "syntactic similarity", "semantic similarity"]
    }      
  }, {
    "pdf_path": "./pdf/AI_VIT_X/2304.13731v2.pdf",
    "title": "Text-to-Audio Generation using Instruction-Tuned\nLLM and Latent Diffusion Model",
    "abstract": "The immense scale of the recent large language models (LLM) allows many in-\nteresting properties, such as, instruction- and chain-of-thought-based fine-tuning,\nthat has significantly improved zero- and few-shot performance in many natu-\nral language processing (NLP) tasks. Inspired by such successes, we adopt such\nan instruction-tuned LLM FLAN-T5 as the text encoder for text-to-audio (TTA)\ngeneration-a task where the goal is to generate an audio from its textual de-\nscription. The prior works on TTA either pre-trained a joint text-audio encoder\nor used a non-instruction-tuned model, such as, T5. Consequently, our latent dif-\nfusion model (LDM)-based approach (TANGO) outperforms the state-of-the-art\nAudioLDM on most metrics and stays comparable on the rest on AudioCaps test\nset, despite training the LDM on a 63 times smaller dataset and keeping the text\nencoder frozen. This improvement might also be attributed to the adoption of au-\ndio pressure level-based sound mixing for the training set augmentation, whereas\nthe prior methods take a random mix.",
    "result": {
      "domain": "Natural Language Processing, Text-to-Audio Generation",
      "problem": "Improving text-to-audio generation using large language models (LLM) and latent diffusion models (LDM).",
      "solution": "Utilizing an instruction-tuned LLM FLAN-T5 as the text encoder and a latent diffusion model (LDM) in the TANGO approach to outperform previous methods on the AudioCaps test set, despite training the LDM on a smaller dataset and keeping the text encoder frozen.",
      "keywords": ["text-to-audio generation", "large language models", "LLM", "latent diffusion model", "instruction tuning", "fine-tuning", "FLAN-T5", "TANGO approach"]
    }        
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1609.01704v7.pdf",
    "title": "HIERARCHICAL MULTISCALE\nRECURRENT NEURAL NETWORKS",
    "abstract": "Learning both hierarchical and temporal representation has been among the long-\nstanding challenges of recurrent neural networks. Multiscale recurrent neural\nnetworks have been considered as a promising approach to resolve this issue, yet\nthere has been a lack of empirical evidence showing that this type of models can\nactually capture the temporal dependencies by discovering the latent hierarchical\nstructure of the sequence. In this paper, we propose a novel multiscale approach,\ncalled the hierarchical multiscale recurrent neural network, that can capture the\nlatent hierarchical structure in the sequence by encoding the temporal dependencies\nwith different timescales using a novel update mechanism. We show some evidence\nthat the proposed model can discover underlying hierarchical structure in the\nsequences without using explicit boundary information. We evaluate our proposed\nmodel on character-level language modelling and handwriting sequence generation.",
    "result": {
      "domain": "Recurrent Neural Networks, Sequence Modeling",
      "problem": "Difficulty in learning hierarchical and temporal representations using recurrent neural networks.",
      "solution": "Introduction of a novel approach, hierarchical multiscale recurrent neural networks, capable of capturing the latent hierarchical structure in sequences by encoding temporal dependencies with different timescales using a novel update mechanism.",
      "keywords": ["hierarchical multiscale recurrent neural networks", "sequence modeling", "temporal dependencies", "hierarchical structure", "character-level language modeling", "handwriting sequence generation"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1509.06461v3.pdf",
    "title": "Deep Reinforcement Learning with Double Q-learning",
    "abstract": "The popular Q-learning algorithm is known to overestimate\naction values under certain conditions. It was not previously\nknown whether, in practice, such overestimations are com-\nmon, whether they harm performance, and whether they can\ngenerally be prevented. In this paper, we answer all these\nquestions affirmatively. In particular, we first show that the\nrecent DQN algorithm, which combines Q-learning with a\ndeep neural network, suffers from substantial overestimations\nin some games in the Atari 2600 domain. We then show that\nthe idea behind the Double Q-learning algorithm, which was\nintroduced in a tabular setting, can be generalized to work\nwith large-scale function approximation. We propose a spe-\ncific adaptation to the DQN algorithm and show that the re-\nsulting algorithm not only reduces the observed overestima-\ntions, as hypothesized, but that this also leads to much better\nperformance on several games.",
    "result": {
      "domain": "Reinforcement Learning, Deep Learning",
      "problem": "Q-learning algorithm known to overestimate action values in certain conditions, leading to potential performance issues.",
      "solution": "Introduction of Double Q-learning algorithm, a modification of the DQN algorithm, which reduces overestimations and improves performance in Atari 2600 games.",
      "keywords": ["Q-learning", "Double Q-learning", "DQN algorithm", "Atari 2600 games", "deep reinforcement learning"]
    }        
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1312.5602v1.pdf",
    "title": "Playing Atari with Deep Reinforcement Learning",
    "abstract": "We present the first deep learning model to successfully learn control policies di-\nrectly from high-dimensional sensory input using reinforcement learning. The\nmodel is a convolutional neural network, trained with a variant of Q-learning,\nwhose input is raw pixels and whose output is a value function estimating future\nrewards. We apply our method to seven Atari 2600 games from the Arcade Learn-\ning Environment, with no adjustment of the architecture or learning algorithm. We\nfind that it outperforms all previous approaches on six of the games and surpasses\na human expert on three of them.",
    "result": {
      "domain": "Reinforcement Learning, Deep Learning, Artificial Intelligence",
      "problem": "Learning control policies directly from high-dimensional sensory input using reinforcement learning was challenging.",
      "solution": "Introduction of a convolutional neural network trained with Q-learning variant, which takes raw pixels as input and estimates future rewards, successfully outperforms previous approaches on seven Atari 2600 games.",
      "keywords": ["Deep Reinforcement Learning", "Convolutional Neural Network", "Q-learning", "Atari 2600 games", "Arcade Learning Environment"]
    }        
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1909.09586v1.pdf",
    "title": "- Understanding LSTM -\na tutorial into Long Short- Term Memory\nRecurrent Neural Networks",
    "abstract": "Long Short-Term Memory Recurrent Neural Networks (LSTM-RNN)\nare one of the most powerful dynamic classifiers publicly known. The net-\nwork itself and the related learning algorithms are reasonably well docu-\nmented to get an idea how it works. This paper will shed more light into\nunderstanding how LSTM-RNNs evolved and why they work impressively\nwell, focusing on the early, ground-breaking publications. We significantly\nimproved documentation and fixed a number of errors and inconsistencies\nthat accumulated in previous publications. To support understanding we\nas well revised and unified the notation used.",
    "result": {
      "domain": "Recurrent Neural Networks, Machine Learning",
      "problem": "Understanding the evolution and functioning of Long Short-Term Memory Recurrent Neural Networks (LSTM-RNNs) was not adequately documented.",
      "solution": "This paper provides a detailed tutorial on LSTM-RNNs, focusing on their evolution, functioning, and early groundbreaking publications. It improves documentation, corrects errors, and unifies notation to support better understanding.",
      "keywords": ["Long Short-Term Memory", "LSTM", "Recurrent Neural Networks", "Dynamic classifiers", "Machine Learning"]
    }        
  }, {
    "pdf_path": "./pdf/AI_VIT_X/2303.15056v2.pdf",
    "title": "CHATGPT OUTPERFORMS CROWD- WORKERS\nFOR TEXT-ANNOTATION TASKS",
    "abstract": "Many NLP applications require manual text annotations for a variety of tasks, notably\nto train classifiers or evaluate the performance of unsupervised models. Depending\non the size and degree of complexity, the tasks may be conducted by crowd-workers\non platforms such as MTurk as well as trained annotators, such as research assistants.\nUsing four samples of tweets and news articles (n = 6,183), we show that ChatGPT\noutperforms crowd-workers for several annotation tasks, including relevance, stance,\ntopics, and frame detection. Across the four datasets, the zero-shot accuracy of\nChatGPT exceeds that of crowd-workers by about 25 percentage points on average,\nwhile ChatGPT's intercoder agreement exceeds that of both crowd-workers and\ntrained annotators for all tasks. Moreover, the per-annotation cost of ChatGPT is\nless than $0.003-about thirty times cheaper than MTurk. These results demonstrate\nthe potential of large language models to drastically increase the efficiency of text\nclassification.",
    "result": {
      "domain": "Natural Language Processing, Text Annotation",
      "problem": "Manual text annotation tasks, crucial for NLP applications, are typically conducted by crowd-workers or trained annotators, which can be costly and time-consuming.",
      "solution": "This paper demonstrates that ChatGPT outperforms crowd-workers for text annotation tasks such as relevance, stance, topics, and frame detection. ChatGPT achieves higher accuracy and intercoder agreement compared to both crowd-workers and trained annotators, while being significantly more cost-effective.",
      "keywords": ["ChatGPT", "Text Annotation", "Natural Language Processing", "Crowdsourcing", "Efficiency"]
    }        
  }, {
    "pdf_path": "./pdf/AI_VIT_X/2211.10435v2.pdf",
    "title": "PAL: Program-aided Language Models",
    "abstract": "Large language models (LLMs) have recently\ndemonstrated an impressive ability to perform\narithmetic and symbolic reasoning tasks, when\nprovided with a few examples at test time (\"few-\nshot prompting\"). Much of this success can be\nattributed to prompting methods such as \"chain-\nof-thought\", which employ LLMs for both under-\nstanding the problem description by decomposing\nit into steps, as well as solving each step of the\nproblem. While LLMs seem to be adept at this\nsort of step-by-step decomposition, LLMs often\nmake logical and arithmetic mistakes in the solu-\ntion part, even when the problem is decomposed\ncorrectly. In this paper, we present Program-\nAided Language models (PAL): a novel approach\nthat uses the LLM to read natural language prob-\nlems and generate programs as the intermediate\nreasoning steps, but offloads the solution step to a\nruntime such as a Python interpreter. With PAL,\ndecomposing the natural language problem into\nrunnable steps remains the only learning task for\nthe LLM, while solving is delegated to the inter-\npreter. We demonstrate this synergy between a\nneural LLM and a symbolic interpreter across 13\nmathematical, symbolic, and algorithmic reason-\ning tasks from BIG-Bench Hard and other bench-\nmarks. In all these natural language reasoning\ntasks, generating code using an LLM and rea-\nsoning using a Python interpreter leads to more\naccurate results than much larger models. For ex-\nample, PAL using CODEX achieves state-of-the-\nart few-shot accuracy on the GSM8K benchmark\nof math word problems, surpassing PaLM-540B\nwhich uses chain-of-thought by absolute 15% top-\n1. Our code and data are publicly available at\nhttp : / / reasonwithpal Â· com Â·",
    "result": {
      "domain": "Natural Language Processing, Symbolic Reasoning, Program Synthesis",
      "problem": "Large language models (LLMs) struggle with logical and arithmetic mistakes when solving problems, even when they can decompose the problem correctly. This limits their effectiveness in tasks requiring symbolic reasoning.",
      "solution": "This paper introduces Program-Aided Language models (PAL), which leverage LLMs to generate intermediate programs from natural language problems and delegate the solution step to a runtime such as a Python interpreter. By separating the reasoning and solving steps, PAL improves accuracy in mathematical, symbolic, and algorithmic reasoning tasks compared to traditional LLM-based approaches.",
      "keywords": ["Language Models", "Symbolic Reasoning", "Program Synthesis", "Natural Language Processing", "Few-shot Learning"]
    }     
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1409.1556v6.pdf",
    "title": "VERY DEEP CONVOLUTIONAL NETWORKS\nFOR LARGE-SCALE IMAGE RECOGNITION",
    "abstract": "In this work we investigate the effect of the convolutional network depth on its\naccuracy in the large-scale image recognition setting. Our main contribution is\na thorough evaluation of networks of increasing depth using an architecture with\nvery small (3 x 3) convolution filters, which shows that a significant improvement\non the prior-art configurations can be achieved by pushing the depth to 16-19\nweight layers. These findings were the basis of our ImageNet Challenge 2014\nsubmission, where our team secured the first and the second places in the localisa-\ntion and classification tracks respectively. We also show that our representations\ngeneralise well to other datasets, where they achieve state-of-the-art results. We\nhave made our two best-performing ConvNet models publicly available to facili-\ntate further research on the use of deep visual representations in computer vision.",
    "result": {
      "domain": "Computer Vision, Image Recognition",
      "problem": "Effect of convolutional network depth on accuracy in large-scale image recognition",
      "solution": "Thorough evaluation of networks with increasing depth using small (3 x 3) convolution filters, achieving significant improvement in accuracy by increasing depth to 16-19 weight layers",
      "keywords": ["Very Deep Convolutional Networks", "Convolutional Neural Networks", "Image Recognition", "Network Depth", "ImageNet Challenge", "State-of-the-art", "3x3 Convolution Filters"]
    }    
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1511.06018v2.pdf",
    "title": "SEGMENTAL RECURRENT NEURAL NETWORKS",
    "abstract": "We introduce segmental recurrent neural networks (SRNNs) which define,\ngiven an input sequence, a joint probability distribution over segmentations of\nthe input and labelings of the segments. Representations of the input segments\n(i.e., contiguous subsequences of the input) are computed by encoding their con-\nstituent tokens using bidirectional recurrent neural nets, and these \"segment em-\nbeddings\" are used to define compatibility scores with output labels. These lo-\ncal compatibility scores are integrated using a global semi-Markov conditional\nrandom field. Both fully supervised training-in which segment boundaries and\nlabels are observed -as well as partially supervised training-in which segment\nboundaries are latent-are straightforward. Experiments on handwriting recog-\nnition and joint Chinese word segmentation/POS tagging show that, compared to\nmodels that do not explicitly represent segments such as BIO tagging schemes and\nconnectionist temporal classification (CTC), SRNNs obtain substantially higher\naccuracies.",
    "result": {
      "domain": "Machine Learning, Natural Language Processing, Handwriting Recognition",
      "problem": "Traditional models lack the ability to define joint probability distribution over segmentations of input sequences and labelings of segments, leading to lower accuracies in tasks like handwriting recognition and joint Chinese word segmentation/POS tagging.",
      "solution": "Introduction of segmental recurrent neural networks (SRNNs) which compute representations of input segments using bidirectional recurrent neural nets and define a joint probability distribution over segmentations of input sequences and labelings of segments, integrating local compatibility scores using a global semi-Markov conditional random field.",
      "keywords": ["Segmental Recurrent Neural Networks", "SRNNs", "Segmentation", "Labeling", "Handwriting Recognition", "POS Tagging", "Conditional Random Field"]
    }        
  }, {
    "pdf_path": "./pdf/AI_VIT_X/1511.06434v2.pdf",
    "title": "UNSUPERVISED REPRESENTATION LEARNING\nWITH DEEP CONVOLUTIONAL\nGENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "In recent years, supervised learning with convolutional networks (CNNs) has\nseen huge adoption in computer vision applications. Comparatively, unsupervised\nlearning with CNNs has received less attention. In this work we hope to help\nbridge the gap between the success of CNNs for supervised learning and unsuper-\nvised learning. We introduce a class of CNNs called deep convolutional generative\nadversarial networks (DCGANs), that have certain architectural constraints, and\ndemonstrate that they are a strong candidate for unsupervised learning. Training\non various image datasets, we show convincing evidence that our deep convolu-\ntional adversarial pair learns a hierarchy of representations from object parts to\nscenes in both the generator and discriminator. Additionally, we use the learned\nfeatures for novel tasks - demonstrating their applicability as general image repre-\nsentations.",
    "result": {
      "domain": "Machine Learning, Computer Vision",
      "problem": "Unsupervised learning with convolutional networks (CNNs) has received less attention compared to supervised learning, leading to a gap in leveraging CNNs for various computer vision tasks.",
      "solution": "Introduction of deep convolutional generative adversarial networks (DCGANs), a class of CNNs with specific architectural constraints, as a strong candidate for unsupervised learning. Training on various image datasets demonstrates that DCGANs learn a hierarchy of representations from object parts to scenes in both the generator and discriminator, providing general image representations applicable for novel tasks.",
      "keywords": ["Unsupervised Representation Learning", "Deep Convolutional Generative Adversarial Networks", "DCGANs", "Convolutional Networks", "Computer Vision", "Image Representation"]
    }        
  }
]